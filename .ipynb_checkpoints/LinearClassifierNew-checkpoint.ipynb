{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import print_function\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 文件读取\n",
    "def unpickle(file):\n",
    "\twith open(file,'rb') as fo:\n",
    "\t\tdict = pickle.load(fo)\n",
    "\treturn dict\n",
    "\n",
    "def load_file(file):\n",
    "\tdictTrain = unpickle(file + \"data_batch_1\")\n",
    "\tdataTrain = dictTrain['data']\n",
    "\tlabelTrain = dictTrain['labels']\n",
    "\n",
    "\tfor i in range(2,6):\n",
    "\t\tdictTrain = unpickle(file + \"data_batch_\" + str(i))\n",
    "\t\tdataTrain = np.vstack([dataTrain,dictTrain['data']])\n",
    "\t\tlabelTrain = np.hstack([labelTrain,dictTrain['labels']])\n",
    "\n",
    "\tdictTest = unpickle(file + \"test_batch\")\n",
    "\tdataTest = dictTest['data']\n",
    "\tlabelTest = dictTest['labels']\n",
    "\tlabelTest = np.array(labelTest)\n",
    "\n",
    "\treturn dataTrain, labelTrain, dataTest, labelTest\n",
    "\n",
    "\n",
    "#softmax loss 函数\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "\t'''\n",
    "\t\tW:权重矩阵\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t'''\n",
    "\t#初始化数据\n",
    "\tloss = 0.0\n",
    "\tdW = np.zeros_like(W)\n",
    "\tnum_train = X.shape[0]\t#样本数\n",
    "\tnum_class = W.shape[1]\t#样本类别数\n",
    "\n",
    "\tfor i in xrange(num_train):\n",
    "\t\tscore = X[i].dot(W)\n",
    "\t\tscore -= np.max(score)\t#提高样本稳定性\n",
    "\n",
    "\t\tcorrect_score = score[y[i]]\n",
    "\t\texp_sum = np.sum(np.exp(score))\n",
    "\t\tloss += np.log(exp_sum) - correct_score\n",
    "\n",
    "\t\tfor j in xrange(num_class):\n",
    "\t\t\tif (j == y[i]):\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i] - X[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i]\n",
    "\n",
    "\n",
    "\tloss /= num_train\n",
    "\tloss += 0.5 * reg * np.sum(W*W)\n",
    "\n",
    "\tdW /= num_train\n",
    "\tdW += reg * W\n",
    "\n",
    "\treturn loss, dW\n",
    "\n",
    "#线性分类器\n",
    "class LinearClassifier(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.W = None\n",
    "\n",
    "\tdef train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\tstep_size:学习步进速度\n",
    "\t\treg:正则化强度\n",
    "\t\tnum_iters:迭代次数\n",
    "\t\tbatch_size:每次迭代图片样本数\n",
    "\t\tverbose:是否打印信息\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss_history:每次训练loss值\n",
    "\t\t'''\n",
    "\t\tnum_train, dim = X.shape\n",
    "\t\tnum_classes = np.max(y) + 1\n",
    "\t\n",
    "\t\tif self.W is None:\n",
    "\t\t\tself.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "\t\tloss_history = []\n",
    "\n",
    "\t\tfor it in xrange(num_iters):\n",
    "\t\t\t#从样本中不重复随机采batch_size个样本\n",
    "\t\t\tsample_index = np.random.choice(num_train, batch_size, replace=False)\n",
    "\n",
    "\t\t\tX_batch = X[sample_index, :]\n",
    "\t\t\ty_batch = y[sample_index]\n",
    "\n",
    "\t\t\tloss, grad = self.loss(X_batch, y_batch, reg)\n",
    "\t\t\tloss_history.append(loss)\n",
    "\n",
    "\t\t\tself.W += -step_size * grad\n",
    "\n",
    "\t\t\tif (verbose and it %10 == 0):\n",
    "\t\t\t\tprint('iteration %d / %d, samples: %d, loss: %f' % (it, num_iters, batch_size, loss))\n",
    "\n",
    "\t\treturn loss_history\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\ty_pred:标签预测值\n",
    "\t\t'''\n",
    "\t\ty_pred = np.zeros(X.shape[1])\n",
    "\n",
    "\t\tscore = X.dot(self.W)\n",
    "\t\ty_pred = np.argmax(score, axis = 1)\n",
    "\n",
    "\t\treturn y_pred\n",
    "\n",
    "\n",
    "\tdef loss(self, X_batch, y_batch, reg):\n",
    "\t\t'''\n",
    "\t\tX_batch:图片训练集(矩阵)\n",
    "\t\ty_batch:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t\t'''\n",
    "\t\treturn softmax_loss_naive(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ...\n",
      "iteration 0 / 8000, samples: 200, loss: 4.737583\n",
      "iteration 10 / 8000, samples: 200, loss: 353.905978\n",
      "iteration 20 / 8000, samples: 200, loss: 372.944436\n",
      "iteration 30 / 8000, samples: 200, loss: 183.003109\n",
      "iteration 40 / 8000, samples: 200, loss: 297.340992\n",
      "iteration 50 / 8000, samples: 200, loss: 486.711883\n",
      "iteration 60 / 8000, samples: 200, loss: 342.255541\n",
      "iteration 70 / 8000, samples: 200, loss: 272.145013\n",
      "iteration 80 / 8000, samples: 200, loss: 339.412697\n",
      "iteration 90 / 8000, samples: 200, loss: 261.308409\n",
      "iteration 100 / 8000, samples: 200, loss: 214.170663\n",
      "iteration 110 / 8000, samples: 200, loss: 212.553801\n",
      "iteration 120 / 8000, samples: 200, loss: 231.230216\n",
      "iteration 130 / 8000, samples: 200, loss: 292.230670\n",
      "iteration 140 / 8000, samples: 200, loss: 378.518551\n",
      "iteration 150 / 8000, samples: 200, loss: 263.355660\n",
      "iteration 160 / 8000, samples: 200, loss: 210.490195\n",
      "iteration 170 / 8000, samples: 200, loss: 276.716468\n",
      "iteration 180 / 8000, samples: 200, loss: 236.129317\n",
      "iteration 190 / 8000, samples: 200, loss: 225.656267\n",
      "iteration 200 / 8000, samples: 200, loss: 405.387710\n",
      "iteration 210 / 8000, samples: 200, loss: 360.561798\n",
      "iteration 220 / 8000, samples: 200, loss: 563.292038\n",
      "iteration 230 / 8000, samples: 200, loss: 292.010036\n",
      "iteration 240 / 8000, samples: 200, loss: 438.189291\n",
      "iteration 250 / 8000, samples: 200, loss: 210.390474\n",
      "iteration 260 / 8000, samples: 200, loss: 258.328775\n",
      "iteration 270 / 8000, samples: 200, loss: 208.492533\n",
      "iteration 280 / 8000, samples: 200, loss: 268.932668\n",
      "iteration 290 / 8000, samples: 200, loss: 392.790105\n",
      "iteration 300 / 8000, samples: 200, loss: 279.447667\n",
      "iteration 310 / 8000, samples: 200, loss: 200.182389\n",
      "iteration 320 / 8000, samples: 200, loss: 343.981152\n",
      "iteration 330 / 8000, samples: 200, loss: 168.942368\n",
      "iteration 340 / 8000, samples: 200, loss: 220.295288\n",
      "iteration 350 / 8000, samples: 200, loss: 229.337411\n",
      "iteration 360 / 8000, samples: 200, loss: 356.707408\n",
      "iteration 370 / 8000, samples: 200, loss: 249.320444\n",
      "iteration 380 / 8000, samples: 200, loss: 271.536590\n",
      "iteration 390 / 8000, samples: 200, loss: 285.889763\n",
      "iteration 400 / 8000, samples: 200, loss: 349.963718\n",
      "iteration 410 / 8000, samples: 200, loss: 321.223825\n",
      "iteration 420 / 8000, samples: 200, loss: 221.928809\n",
      "iteration 430 / 8000, samples: 200, loss: 182.181504\n",
      "iteration 440 / 8000, samples: 200, loss: 231.457346\n",
      "iteration 450 / 8000, samples: 200, loss: 209.217509\n",
      "iteration 460 / 8000, samples: 200, loss: 234.100852\n",
      "iteration 470 / 8000, samples: 200, loss: 380.336250\n",
      "iteration 480 / 8000, samples: 200, loss: 432.003717\n",
      "iteration 490 / 8000, samples: 200, loss: 202.136900\n",
      "iteration 500 / 8000, samples: 200, loss: 189.866300\n",
      "iteration 510 / 8000, samples: 200, loss: 214.960856\n",
      "iteration 520 / 8000, samples: 200, loss: 304.663111\n",
      "iteration 530 / 8000, samples: 200, loss: 369.945104\n",
      "iteration 540 / 8000, samples: 200, loss: 378.533787\n",
      "iteration 550 / 8000, samples: 200, loss: 410.855894\n",
      "iteration 560 / 8000, samples: 200, loss: 258.592489\n",
      "iteration 570 / 8000, samples: 200, loss: 336.700774\n",
      "iteration 580 / 8000, samples: 200, loss: 315.750224\n",
      "iteration 590 / 8000, samples: 200, loss: 250.302514\n",
      "iteration 600 / 8000, samples: 200, loss: 210.040176\n",
      "iteration 610 / 8000, samples: 200, loss: 281.182207\n",
      "iteration 620 / 8000, samples: 200, loss: 301.426345\n",
      "iteration 630 / 8000, samples: 200, loss: 280.013088\n",
      "iteration 640 / 8000, samples: 200, loss: 342.189753\n",
      "iteration 650 / 8000, samples: 200, loss: 275.777101\n",
      "iteration 660 / 8000, samples: 200, loss: 272.447789\n",
      "iteration 670 / 8000, samples: 200, loss: 384.426785\n",
      "iteration 680 / 8000, samples: 200, loss: 273.075002\n",
      "iteration 690 / 8000, samples: 200, loss: 263.149100\n",
      "iteration 700 / 8000, samples: 200, loss: 257.340924\n",
      "iteration 710 / 8000, samples: 200, loss: 360.698784\n",
      "iteration 720 / 8000, samples: 200, loss: 248.930072\n",
      "iteration 730 / 8000, samples: 200, loss: 432.827644\n",
      "iteration 740 / 8000, samples: 200, loss: 226.270289\n",
      "iteration 750 / 8000, samples: 200, loss: 236.688628\n",
      "iteration 760 / 8000, samples: 200, loss: 272.294921\n",
      "iteration 770 / 8000, samples: 200, loss: 346.944308\n",
      "iteration 780 / 8000, samples: 200, loss: 221.698345\n",
      "iteration 790 / 8000, samples: 200, loss: 348.692898\n",
      "iteration 800 / 8000, samples: 200, loss: 337.774356\n",
      "iteration 810 / 8000, samples: 200, loss: 286.947725\n",
      "iteration 820 / 8000, samples: 200, loss: 279.306725\n",
      "iteration 830 / 8000, samples: 200, loss: 220.655160\n",
      "iteration 840 / 8000, samples: 200, loss: 299.695839\n",
      "iteration 850 / 8000, samples: 200, loss: 311.132294\n",
      "iteration 860 / 8000, samples: 200, loss: 312.602957\n",
      "iteration 870 / 8000, samples: 200, loss: 199.208394\n",
      "iteration 880 / 8000, samples: 200, loss: 359.391570\n",
      "iteration 890 / 8000, samples: 200, loss: 230.031620\n",
      "iteration 900 / 8000, samples: 200, loss: 223.640750\n",
      "iteration 910 / 8000, samples: 200, loss: 250.048416\n",
      "iteration 920 / 8000, samples: 200, loss: 245.899878\n",
      "iteration 930 / 8000, samples: 200, loss: 219.721949\n",
      "iteration 940 / 8000, samples: 200, loss: 180.315027\n",
      "iteration 950 / 8000, samples: 200, loss: 282.310266\n",
      "iteration 960 / 8000, samples: 200, loss: 305.399263\n",
      "iteration 970 / 8000, samples: 200, loss: 196.785356\n",
      "iteration 980 / 8000, samples: 200, loss: 178.992938\n",
      "iteration 990 / 8000, samples: 200, loss: 283.848671\n",
      "iteration 1000 / 8000, samples: 200, loss: 292.316278\n",
      "iteration 1010 / 8000, samples: 200, loss: 362.055904\n",
      "iteration 1020 / 8000, samples: 200, loss: 222.008705\n",
      "iteration 1030 / 8000, samples: 200, loss: 206.178813\n",
      "iteration 1040 / 8000, samples: 200, loss: 230.082999\n",
      "iteration 1050 / 8000, samples: 200, loss: 236.964694\n",
      "iteration 1060 / 8000, samples: 200, loss: 175.237518\n",
      "iteration 1070 / 8000, samples: 200, loss: 179.962359\n",
      "iteration 1080 / 8000, samples: 200, loss: 218.282026\n",
      "iteration 1090 / 8000, samples: 200, loss: 245.447651\n",
      "iteration 1100 / 8000, samples: 200, loss: 277.638523\n",
      "iteration 1110 / 8000, samples: 200, loss: 229.481855\n",
      "iteration 1120 / 8000, samples: 200, loss: 365.154384\n",
      "iteration 1130 / 8000, samples: 200, loss: 329.700852\n",
      "iteration 1140 / 8000, samples: 200, loss: 230.590276\n",
      "iteration 1150 / 8000, samples: 200, loss: 191.449052\n",
      "iteration 1160 / 8000, samples: 200, loss: 226.431624\n",
      "iteration 1170 / 8000, samples: 200, loss: 228.765946\n",
      "iteration 1180 / 8000, samples: 200, loss: 234.526473\n",
      "iteration 1190 / 8000, samples: 200, loss: 379.183327\n",
      "iteration 1200 / 8000, samples: 200, loss: 217.446742\n",
      "iteration 1210 / 8000, samples: 200, loss: 307.272327\n",
      "iteration 1220 / 8000, samples: 200, loss: 275.775606\n",
      "iteration 1230 / 8000, samples: 200, loss: 329.772410\n",
      "iteration 1240 / 8000, samples: 200, loss: 245.166642\n",
      "iteration 1250 / 8000, samples: 200, loss: 307.672009\n",
      "iteration 1260 / 8000, samples: 200, loss: 265.540950\n",
      "iteration 1270 / 8000, samples: 200, loss: 228.864766\n",
      "iteration 1280 / 8000, samples: 200, loss: 347.801467\n",
      "iteration 1290 / 8000, samples: 200, loss: 304.175886\n",
      "iteration 1300 / 8000, samples: 200, loss: 215.959036\n",
      "iteration 1310 / 8000, samples: 200, loss: 277.420983\n",
      "iteration 1320 / 8000, samples: 200, loss: 345.488799\n",
      "iteration 1330 / 8000, samples: 200, loss: 308.562830\n",
      "iteration 1340 / 8000, samples: 200, loss: 223.259548\n",
      "iteration 1350 / 8000, samples: 200, loss: 324.315588\n",
      "iteration 1360 / 8000, samples: 200, loss: 374.760216\n",
      "iteration 1370 / 8000, samples: 200, loss: 255.314512\n",
      "iteration 1380 / 8000, samples: 200, loss: 401.821759\n",
      "iteration 1390 / 8000, samples: 200, loss: 270.735631\n",
      "iteration 1400 / 8000, samples: 200, loss: 208.503632\n",
      "iteration 1410 / 8000, samples: 200, loss: 280.048229\n",
      "iteration 1420 / 8000, samples: 200, loss: 274.967644\n",
      "iteration 1430 / 8000, samples: 200, loss: 169.965263\n",
      "iteration 1440 / 8000, samples: 200, loss: 383.019686\n",
      "iteration 1450 / 8000, samples: 200, loss: 214.062077\n",
      "iteration 1460 / 8000, samples: 200, loss: 317.172021\n",
      "iteration 1470 / 8000, samples: 200, loss: 300.647966\n",
      "iteration 1480 / 8000, samples: 200, loss: 257.659096\n",
      "iteration 1490 / 8000, samples: 200, loss: 298.119431\n",
      "iteration 1500 / 8000, samples: 200, loss: 216.452187\n",
      "iteration 1510 / 8000, samples: 200, loss: 213.180586\n",
      "iteration 1520 / 8000, samples: 200, loss: 331.318955\n",
      "iteration 1530 / 8000, samples: 200, loss: 207.115696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1540 / 8000, samples: 200, loss: 199.185458\n",
      "iteration 1550 / 8000, samples: 200, loss: 242.116274\n",
      "iteration 1560 / 8000, samples: 200, loss: 184.452397\n",
      "iteration 1570 / 8000, samples: 200, loss: 360.323773\n",
      "iteration 1580 / 8000, samples: 200, loss: 284.433488\n",
      "iteration 1590 / 8000, samples: 200, loss: 181.448076\n",
      "iteration 1600 / 8000, samples: 200, loss: 418.207975\n",
      "iteration 1610 / 8000, samples: 200, loss: 369.143268\n",
      "iteration 1620 / 8000, samples: 200, loss: 166.482257\n",
      "iteration 1630 / 8000, samples: 200, loss: 306.622721\n",
      "iteration 1640 / 8000, samples: 200, loss: 223.978517\n",
      "iteration 1650 / 8000, samples: 200, loss: 342.429301\n",
      "iteration 1660 / 8000, samples: 200, loss: 209.058079\n",
      "iteration 1670 / 8000, samples: 200, loss: 299.608758\n",
      "iteration 1680 / 8000, samples: 200, loss: 328.356742\n",
      "iteration 1690 / 8000, samples: 200, loss: 271.163238\n",
      "iteration 1700 / 8000, samples: 200, loss: 268.332010\n",
      "iteration 1710 / 8000, samples: 200, loss: 232.603485\n",
      "iteration 1720 / 8000, samples: 200, loss: 314.774209\n",
      "iteration 1730 / 8000, samples: 200, loss: 195.818384\n",
      "iteration 1740 / 8000, samples: 200, loss: 366.045548\n",
      "iteration 1750 / 8000, samples: 200, loss: 234.202496\n",
      "iteration 1760 / 8000, samples: 200, loss: 235.670345\n",
      "iteration 1770 / 8000, samples: 200, loss: 228.387810\n",
      "iteration 1780 / 8000, samples: 200, loss: 326.972155\n",
      "iteration 1790 / 8000, samples: 200, loss: 278.127856\n",
      "iteration 1800 / 8000, samples: 200, loss: 436.343349\n",
      "iteration 1810 / 8000, samples: 200, loss: 241.017275\n",
      "iteration 1820 / 8000, samples: 200, loss: 327.512639\n",
      "iteration 1830 / 8000, samples: 200, loss: 197.095345\n",
      "iteration 1840 / 8000, samples: 200, loss: 304.175995\n",
      "iteration 1850 / 8000, samples: 200, loss: 284.783457\n",
      "iteration 1860 / 8000, samples: 200, loss: 169.927272\n",
      "iteration 1870 / 8000, samples: 200, loss: 236.718734\n",
      "iteration 1880 / 8000, samples: 200, loss: 244.163250\n",
      "iteration 1890 / 8000, samples: 200, loss: 446.665093\n",
      "iteration 1900 / 8000, samples: 200, loss: 185.835904\n",
      "iteration 1910 / 8000, samples: 200, loss: 276.094420\n",
      "iteration 1920 / 8000, samples: 200, loss: 307.602268\n",
      "iteration 1930 / 8000, samples: 200, loss: 200.417461\n",
      "iteration 1940 / 8000, samples: 200, loss: 180.184844\n",
      "iteration 1950 / 8000, samples: 200, loss: 354.999579\n",
      "iteration 1960 / 8000, samples: 200, loss: 299.807882\n",
      "iteration 1970 / 8000, samples: 200, loss: 178.857386\n",
      "iteration 1980 / 8000, samples: 200, loss: 218.595296\n",
      "iteration 1990 / 8000, samples: 200, loss: 203.224308\n",
      "iteration 2000 / 8000, samples: 200, loss: 245.899046\n",
      "iteration 2010 / 8000, samples: 200, loss: 293.837674\n",
      "iteration 2020 / 8000, samples: 200, loss: 274.162284\n",
      "iteration 2030 / 8000, samples: 200, loss: 294.244326\n",
      "iteration 2040 / 8000, samples: 200, loss: 434.830100\n",
      "iteration 2050 / 8000, samples: 200, loss: 287.961997\n",
      "iteration 2060 / 8000, samples: 200, loss: 312.532684\n",
      "iteration 2070 / 8000, samples: 200, loss: 198.501452\n",
      "iteration 2080 / 8000, samples: 200, loss: 207.150414\n",
      "iteration 2090 / 8000, samples: 200, loss: 265.583240\n",
      "iteration 2100 / 8000, samples: 200, loss: 394.903401\n",
      "iteration 2110 / 8000, samples: 200, loss: 205.372781\n",
      "iteration 2120 / 8000, samples: 200, loss: 232.679132\n",
      "iteration 2130 / 8000, samples: 200, loss: 310.702125\n",
      "iteration 2140 / 8000, samples: 200, loss: 232.010219\n",
      "iteration 2150 / 8000, samples: 200, loss: 187.363543\n",
      "iteration 2160 / 8000, samples: 200, loss: 221.464277\n",
      "iteration 2170 / 8000, samples: 200, loss: 195.552874\n",
      "iteration 2180 / 8000, samples: 200, loss: 192.704191\n",
      "iteration 2190 / 8000, samples: 200, loss: 323.928477\n",
      "iteration 2200 / 8000, samples: 200, loss: 234.923337\n",
      "iteration 2210 / 8000, samples: 200, loss: 242.802438\n",
      "iteration 2220 / 8000, samples: 200, loss: 291.073952\n",
      "iteration 2230 / 8000, samples: 200, loss: 245.049807\n",
      "iteration 2240 / 8000, samples: 200, loss: 236.286879\n",
      "iteration 2250 / 8000, samples: 200, loss: 194.012399\n",
      "iteration 2260 / 8000, samples: 200, loss: 268.812882\n",
      "iteration 2270 / 8000, samples: 200, loss: 275.591584\n",
      "iteration 2280 / 8000, samples: 200, loss: 258.416094\n",
      "iteration 2290 / 8000, samples: 200, loss: 335.889545\n",
      "iteration 2300 / 8000, samples: 200, loss: 270.201349\n",
      "iteration 2310 / 8000, samples: 200, loss: 218.264223\n",
      "iteration 2320 / 8000, samples: 200, loss: 403.255526\n",
      "iteration 2330 / 8000, samples: 200, loss: 438.125694\n",
      "iteration 2340 / 8000, samples: 200, loss: 396.807051\n",
      "iteration 2350 / 8000, samples: 200, loss: 141.405628\n",
      "iteration 2360 / 8000, samples: 200, loss: 211.548298\n",
      "iteration 2370 / 8000, samples: 200, loss: 356.950813\n",
      "iteration 2380 / 8000, samples: 200, loss: 376.480448\n",
      "iteration 2390 / 8000, samples: 200, loss: 272.159309\n",
      "iteration 2400 / 8000, samples: 200, loss: 206.418601\n",
      "iteration 2410 / 8000, samples: 200, loss: 410.498488\n",
      "iteration 2420 / 8000, samples: 200, loss: 166.913808\n",
      "iteration 2430 / 8000, samples: 200, loss: 214.729367\n",
      "iteration 2440 / 8000, samples: 200, loss: 209.965194\n",
      "iteration 2450 / 8000, samples: 200, loss: 260.768372\n",
      "iteration 2460 / 8000, samples: 200, loss: 290.833163\n",
      "iteration 2470 / 8000, samples: 200, loss: 254.920965\n",
      "iteration 2480 / 8000, samples: 200, loss: 256.659619\n",
      "iteration 2490 / 8000, samples: 200, loss: 219.981870\n",
      "iteration 2500 / 8000, samples: 200, loss: 214.527556\n",
      "iteration 2510 / 8000, samples: 200, loss: 339.615396\n",
      "iteration 2520 / 8000, samples: 200, loss: 262.087217\n",
      "iteration 2530 / 8000, samples: 200, loss: 197.429893\n",
      "iteration 2540 / 8000, samples: 200, loss: 340.264508\n",
      "iteration 2550 / 8000, samples: 200, loss: 304.783419\n",
      "iteration 2560 / 8000, samples: 200, loss: 321.919979\n",
      "iteration 2570 / 8000, samples: 200, loss: 282.315942\n",
      "iteration 2580 / 8000, samples: 200, loss: 129.390209\n",
      "iteration 2590 / 8000, samples: 200, loss: 205.318954\n",
      "iteration 2600 / 8000, samples: 200, loss: 266.210915\n",
      "iteration 2610 / 8000, samples: 200, loss: 269.781548\n",
      "iteration 2620 / 8000, samples: 200, loss: 221.248276\n",
      "iteration 2630 / 8000, samples: 200, loss: 206.839223\n",
      "iteration 2640 / 8000, samples: 200, loss: 282.011865\n",
      "iteration 2650 / 8000, samples: 200, loss: 256.072181\n",
      "iteration 2660 / 8000, samples: 200, loss: 252.696246\n",
      "iteration 2670 / 8000, samples: 200, loss: 300.204038\n",
      "iteration 2680 / 8000, samples: 200, loss: 206.427402\n",
      "iteration 2690 / 8000, samples: 200, loss: 166.908282\n",
      "iteration 2700 / 8000, samples: 200, loss: 265.608170\n",
      "iteration 2710 / 8000, samples: 200, loss: 320.807074\n",
      "iteration 2720 / 8000, samples: 200, loss: 233.392185\n",
      "iteration 2730 / 8000, samples: 200, loss: 264.442238\n",
      "iteration 2740 / 8000, samples: 200, loss: 273.092216\n",
      "iteration 2750 / 8000, samples: 200, loss: 267.017698\n",
      "iteration 2760 / 8000, samples: 200, loss: 300.403907\n",
      "iteration 2770 / 8000, samples: 200, loss: 220.308235\n",
      "iteration 2780 / 8000, samples: 200, loss: 207.380646\n",
      "iteration 2790 / 8000, samples: 200, loss: 261.279886\n",
      "iteration 2800 / 8000, samples: 200, loss: 225.099816\n",
      "iteration 2810 / 8000, samples: 200, loss: 247.462552\n",
      "iteration 2820 / 8000, samples: 200, loss: 320.315482\n",
      "iteration 2830 / 8000, samples: 200, loss: 287.210081\n",
      "iteration 2840 / 8000, samples: 200, loss: 335.489570\n",
      "iteration 2850 / 8000, samples: 200, loss: 350.100796\n",
      "iteration 2860 / 8000, samples: 200, loss: 459.790875\n",
      "iteration 2870 / 8000, samples: 200, loss: 255.513009\n",
      "iteration 2880 / 8000, samples: 200, loss: 282.307794\n",
      "iteration 2890 / 8000, samples: 200, loss: 265.975071\n",
      "iteration 2900 / 8000, samples: 200, loss: 278.547556\n",
      "iteration 2910 / 8000, samples: 200, loss: 213.301092\n",
      "iteration 2920 / 8000, samples: 200, loss: 316.724604\n",
      "iteration 2930 / 8000, samples: 200, loss: 247.602455\n",
      "iteration 2940 / 8000, samples: 200, loss: 209.686950\n",
      "iteration 2950 / 8000, samples: 200, loss: 226.435669\n",
      "iteration 2960 / 8000, samples: 200, loss: 254.569070\n",
      "iteration 2970 / 8000, samples: 200, loss: 285.028328\n",
      "iteration 2980 / 8000, samples: 200, loss: 263.160123\n",
      "iteration 2990 / 8000, samples: 200, loss: 214.070019\n",
      "iteration 3000 / 8000, samples: 200, loss: 377.285358\n",
      "iteration 3010 / 8000, samples: 200, loss: 173.459984\n",
      "iteration 3020 / 8000, samples: 200, loss: 215.538016\n",
      "iteration 3030 / 8000, samples: 200, loss: 175.765583\n",
      "iteration 3040 / 8000, samples: 200, loss: 337.812195\n",
      "iteration 3050 / 8000, samples: 200, loss: 160.466460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3060 / 8000, samples: 200, loss: 390.366881\n",
      "iteration 3070 / 8000, samples: 200, loss: 175.947010\n",
      "iteration 3080 / 8000, samples: 200, loss: 219.629454\n",
      "iteration 3090 / 8000, samples: 200, loss: 298.160919\n",
      "iteration 3100 / 8000, samples: 200, loss: 304.756865\n",
      "iteration 3110 / 8000, samples: 200, loss: 168.841817\n",
      "iteration 3120 / 8000, samples: 200, loss: 274.524731\n",
      "iteration 3130 / 8000, samples: 200, loss: 356.986065\n",
      "iteration 3140 / 8000, samples: 200, loss: 237.421911\n",
      "iteration 3150 / 8000, samples: 200, loss: 176.928257\n",
      "iteration 3160 / 8000, samples: 200, loss: 231.355156\n",
      "iteration 3170 / 8000, samples: 200, loss: 275.179925\n",
      "iteration 3180 / 8000, samples: 200, loss: 152.505518\n",
      "iteration 3190 / 8000, samples: 200, loss: 146.110653\n",
      "iteration 3200 / 8000, samples: 200, loss: 239.767035\n",
      "iteration 3210 / 8000, samples: 200, loss: 173.480985\n",
      "iteration 3220 / 8000, samples: 200, loss: 395.146213\n",
      "iteration 3230 / 8000, samples: 200, loss: 172.110151\n",
      "iteration 3240 / 8000, samples: 200, loss: 199.352029\n",
      "iteration 3250 / 8000, samples: 200, loss: 200.161451\n",
      "iteration 3260 / 8000, samples: 200, loss: 187.440618\n",
      "iteration 3270 / 8000, samples: 200, loss: 187.633053\n",
      "iteration 3280 / 8000, samples: 200, loss: 425.254161\n",
      "iteration 3290 / 8000, samples: 200, loss: 227.572240\n",
      "iteration 3300 / 8000, samples: 200, loss: 409.727503\n",
      "iteration 3310 / 8000, samples: 200, loss: 308.726063\n",
      "iteration 3320 / 8000, samples: 200, loss: 363.043767\n",
      "iteration 3330 / 8000, samples: 200, loss: 531.496880\n",
      "iteration 3340 / 8000, samples: 200, loss: 231.417233\n",
      "iteration 3350 / 8000, samples: 200, loss: 281.278781\n",
      "iteration 3360 / 8000, samples: 200, loss: 225.787258\n",
      "iteration 3370 / 8000, samples: 200, loss: 189.311726\n",
      "iteration 3380 / 8000, samples: 200, loss: 306.753636\n",
      "iteration 3390 / 8000, samples: 200, loss: 193.847637\n",
      "iteration 3400 / 8000, samples: 200, loss: 407.552763\n",
      "iteration 3410 / 8000, samples: 200, loss: 203.604540\n",
      "iteration 3420 / 8000, samples: 200, loss: 244.884613\n",
      "iteration 3430 / 8000, samples: 200, loss: 309.750990\n",
      "iteration 3440 / 8000, samples: 200, loss: 249.162251\n",
      "iteration 3450 / 8000, samples: 200, loss: 205.931087\n",
      "iteration 3460 / 8000, samples: 200, loss: 173.690011\n",
      "iteration 3470 / 8000, samples: 200, loss: 233.427743\n",
      "iteration 3480 / 8000, samples: 200, loss: 203.365311\n",
      "iteration 3490 / 8000, samples: 200, loss: 291.440342\n",
      "iteration 3500 / 8000, samples: 200, loss: 261.668501\n",
      "iteration 3510 / 8000, samples: 200, loss: 279.304042\n",
      "iteration 3520 / 8000, samples: 200, loss: 240.484058\n",
      "iteration 3530 / 8000, samples: 200, loss: 221.708464\n",
      "iteration 3540 / 8000, samples: 200, loss: 203.272871\n",
      "iteration 3550 / 8000, samples: 200, loss: 256.026672\n",
      "iteration 3560 / 8000, samples: 200, loss: 165.475724\n",
      "iteration 3570 / 8000, samples: 200, loss: 194.282101\n",
      "iteration 3580 / 8000, samples: 200, loss: 113.560329\n",
      "iteration 3590 / 8000, samples: 200, loss: 372.611580\n",
      "iteration 3600 / 8000, samples: 200, loss: 327.020687\n",
      "iteration 3610 / 8000, samples: 200, loss: 173.130649\n",
      "iteration 3620 / 8000, samples: 200, loss: 247.988431\n",
      "iteration 3630 / 8000, samples: 200, loss: 254.331499\n",
      "iteration 3640 / 8000, samples: 200, loss: 235.098082\n",
      "iteration 3650 / 8000, samples: 200, loss: 209.111431\n",
      "iteration 3660 / 8000, samples: 200, loss: 240.099310\n",
      "iteration 3670 / 8000, samples: 200, loss: 219.290016\n",
      "iteration 3680 / 8000, samples: 200, loss: 317.409393\n",
      "iteration 3690 / 8000, samples: 200, loss: 187.165319\n",
      "iteration 3700 / 8000, samples: 200, loss: 160.221861\n",
      "iteration 3710 / 8000, samples: 200, loss: 393.073185\n",
      "iteration 3720 / 8000, samples: 200, loss: 414.235989\n",
      "iteration 3730 / 8000, samples: 200, loss: 169.177426\n",
      "iteration 3740 / 8000, samples: 200, loss: 160.500524\n",
      "iteration 3750 / 8000, samples: 200, loss: 373.107272\n",
      "iteration 3760 / 8000, samples: 200, loss: 160.900956\n",
      "iteration 3770 / 8000, samples: 200, loss: 300.028631\n",
      "iteration 3780 / 8000, samples: 200, loss: 281.637853\n",
      "iteration 3790 / 8000, samples: 200, loss: 416.470410\n",
      "iteration 3800 / 8000, samples: 200, loss: 249.288943\n",
      "iteration 3810 / 8000, samples: 200, loss: 290.522883\n",
      "iteration 3820 / 8000, samples: 200, loss: 303.419337\n",
      "iteration 3830 / 8000, samples: 200, loss: 340.169632\n",
      "iteration 3840 / 8000, samples: 200, loss: 172.617543\n",
      "iteration 3850 / 8000, samples: 200, loss: 241.770465\n",
      "iteration 3860 / 8000, samples: 200, loss: 263.916547\n",
      "iteration 3870 / 8000, samples: 200, loss: 465.303739\n",
      "iteration 3880 / 8000, samples: 200, loss: 294.432570\n",
      "iteration 3890 / 8000, samples: 200, loss: 177.980221\n",
      "iteration 3900 / 8000, samples: 200, loss: 207.781032\n",
      "iteration 3910 / 8000, samples: 200, loss: 201.053517\n",
      "iteration 3920 / 8000, samples: 200, loss: 224.727941\n",
      "iteration 3930 / 8000, samples: 200, loss: 252.210819\n",
      "iteration 3940 / 8000, samples: 200, loss: 231.477628\n",
      "iteration 3950 / 8000, samples: 200, loss: 317.653396\n",
      "iteration 3960 / 8000, samples: 200, loss: 280.177266\n",
      "iteration 3970 / 8000, samples: 200, loss: 255.012875\n",
      "iteration 3980 / 8000, samples: 200, loss: 157.420870\n",
      "iteration 3990 / 8000, samples: 200, loss: 210.094074\n",
      "iteration 4000 / 8000, samples: 200, loss: 178.199203\n",
      "iteration 4010 / 8000, samples: 200, loss: 254.907633\n",
      "iteration 4020 / 8000, samples: 200, loss: 288.087963\n",
      "iteration 4030 / 8000, samples: 200, loss: 292.974292\n",
      "iteration 4040 / 8000, samples: 200, loss: 196.305338\n",
      "iteration 4050 / 8000, samples: 200, loss: 256.017669\n",
      "iteration 4060 / 8000, samples: 200, loss: 414.593718\n",
      "iteration 4070 / 8000, samples: 200, loss: 133.018326\n",
      "iteration 4080 / 8000, samples: 200, loss: 227.536786\n",
      "iteration 4090 / 8000, samples: 200, loss: 250.174773\n",
      "iteration 4100 / 8000, samples: 200, loss: 163.977029\n",
      "iteration 4110 / 8000, samples: 200, loss: 251.816978\n",
      "iteration 4120 / 8000, samples: 200, loss: 223.617982\n",
      "iteration 4130 / 8000, samples: 200, loss: 242.707207\n",
      "iteration 4140 / 8000, samples: 200, loss: 200.640339\n",
      "iteration 4150 / 8000, samples: 200, loss: 256.547711\n",
      "iteration 4160 / 8000, samples: 200, loss: 203.998282\n",
      "iteration 4170 / 8000, samples: 200, loss: 279.644569\n",
      "iteration 4180 / 8000, samples: 200, loss: 175.753418\n",
      "iteration 4190 / 8000, samples: 200, loss: 247.856337\n",
      "iteration 4200 / 8000, samples: 200, loss: 202.545613\n",
      "iteration 4210 / 8000, samples: 200, loss: 225.948710\n",
      "iteration 4220 / 8000, samples: 200, loss: 248.873498\n",
      "iteration 4230 / 8000, samples: 200, loss: 225.076158\n",
      "iteration 4240 / 8000, samples: 200, loss: 206.767786\n",
      "iteration 4250 / 8000, samples: 200, loss: 160.435710\n",
      "iteration 4260 / 8000, samples: 200, loss: 346.049648\n",
      "iteration 4270 / 8000, samples: 200, loss: 146.861015\n",
      "iteration 4280 / 8000, samples: 200, loss: 281.031808\n",
      "iteration 4290 / 8000, samples: 200, loss: 198.408393\n",
      "iteration 4300 / 8000, samples: 200, loss: 319.011592\n",
      "iteration 4310 / 8000, samples: 200, loss: 345.059591\n",
      "iteration 4320 / 8000, samples: 200, loss: 229.019500\n",
      "iteration 4330 / 8000, samples: 200, loss: 168.912762\n",
      "iteration 4340 / 8000, samples: 200, loss: 439.660769\n",
      "iteration 4350 / 8000, samples: 200, loss: 297.886863\n",
      "iteration 4360 / 8000, samples: 200, loss: 382.167731\n",
      "iteration 4370 / 8000, samples: 200, loss: 247.161517\n",
      "iteration 4380 / 8000, samples: 200, loss: 326.404795\n",
      "iteration 4390 / 8000, samples: 200, loss: 261.835680\n",
      "iteration 4400 / 8000, samples: 200, loss: 227.068367\n",
      "iteration 4410 / 8000, samples: 200, loss: 282.660384\n",
      "iteration 4420 / 8000, samples: 200, loss: 185.460883\n",
      "iteration 4430 / 8000, samples: 200, loss: 407.779232\n",
      "iteration 4440 / 8000, samples: 200, loss: 255.715485\n",
      "iteration 4450 / 8000, samples: 200, loss: 241.102358\n",
      "iteration 4460 / 8000, samples: 200, loss: 297.558578\n",
      "iteration 4470 / 8000, samples: 200, loss: 307.965402\n",
      "iteration 4480 / 8000, samples: 200, loss: 240.635480\n",
      "iteration 4490 / 8000, samples: 200, loss: 343.175176\n",
      "iteration 4500 / 8000, samples: 200, loss: 288.934973\n",
      "iteration 4510 / 8000, samples: 200, loss: 222.855142\n",
      "iteration 4520 / 8000, samples: 200, loss: 256.732925\n",
      "iteration 4530 / 8000, samples: 200, loss: 239.003606\n",
      "iteration 4540 / 8000, samples: 200, loss: 272.634730\n",
      "iteration 4550 / 8000, samples: 200, loss: 269.173759\n",
      "iteration 4560 / 8000, samples: 200, loss: 246.221809\n",
      "iteration 4570 / 8000, samples: 200, loss: 313.962224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4580 / 8000, samples: 200, loss: 210.661025\n",
      "iteration 4590 / 8000, samples: 200, loss: 269.777376\n",
      "iteration 4600 / 8000, samples: 200, loss: 246.648597\n",
      "iteration 4610 / 8000, samples: 200, loss: 162.810305\n",
      "iteration 4620 / 8000, samples: 200, loss: 383.979946\n",
      "iteration 4630 / 8000, samples: 200, loss: 231.042792\n",
      "iteration 4640 / 8000, samples: 200, loss: 222.599862\n",
      "iteration 4650 / 8000, samples: 200, loss: 299.250253\n",
      "iteration 4660 / 8000, samples: 200, loss: 316.760939\n",
      "iteration 4670 / 8000, samples: 200, loss: 168.924984\n",
      "iteration 4680 / 8000, samples: 200, loss: 303.176738\n",
      "iteration 4690 / 8000, samples: 200, loss: 276.702548\n",
      "iteration 4700 / 8000, samples: 200, loss: 273.324776\n",
      "iteration 4710 / 8000, samples: 200, loss: 262.881222\n",
      "iteration 4720 / 8000, samples: 200, loss: 238.998440\n",
      "iteration 4730 / 8000, samples: 200, loss: 194.189410\n",
      "iteration 4740 / 8000, samples: 200, loss: 287.347801\n",
      "iteration 4750 / 8000, samples: 200, loss: 190.921499\n",
      "iteration 4760 / 8000, samples: 200, loss: 163.562061\n",
      "iteration 4770 / 8000, samples: 200, loss: 343.434371\n",
      "iteration 4780 / 8000, samples: 200, loss: 201.395092\n",
      "iteration 4790 / 8000, samples: 200, loss: 299.723482\n",
      "iteration 4800 / 8000, samples: 200, loss: 238.342405\n",
      "iteration 4810 / 8000, samples: 200, loss: 167.948694\n",
      "iteration 4820 / 8000, samples: 200, loss: 235.998942\n",
      "iteration 4830 / 8000, samples: 200, loss: 228.259814\n",
      "iteration 4840 / 8000, samples: 200, loss: 239.244778\n",
      "iteration 4850 / 8000, samples: 200, loss: 271.657780\n",
      "iteration 4860 / 8000, samples: 200, loss: 307.290553\n",
      "iteration 4870 / 8000, samples: 200, loss: 249.378863\n",
      "iteration 4880 / 8000, samples: 200, loss: 145.770876\n",
      "iteration 4890 / 8000, samples: 200, loss: 263.630807\n",
      "iteration 4900 / 8000, samples: 200, loss: 272.906516\n",
      "iteration 4910 / 8000, samples: 200, loss: 437.012820\n",
      "iteration 4920 / 8000, samples: 200, loss: 288.791403\n",
      "iteration 4930 / 8000, samples: 200, loss: 310.184665\n",
      "iteration 4940 / 8000, samples: 200, loss: 357.583037\n",
      "iteration 4950 / 8000, samples: 200, loss: 217.634672\n",
      "iteration 4960 / 8000, samples: 200, loss: 329.370993\n",
      "iteration 4970 / 8000, samples: 200, loss: 264.048625\n",
      "iteration 4980 / 8000, samples: 200, loss: 256.819444\n",
      "iteration 4990 / 8000, samples: 200, loss: 289.870870\n",
      "iteration 5000 / 8000, samples: 200, loss: 259.868841\n",
      "iteration 5010 / 8000, samples: 200, loss: 205.990211\n",
      "iteration 5020 / 8000, samples: 200, loss: 202.461257\n",
      "iteration 5030 / 8000, samples: 200, loss: 259.990395\n",
      "iteration 5040 / 8000, samples: 200, loss: 192.928202\n",
      "iteration 5050 / 8000, samples: 200, loss: 324.207132\n",
      "iteration 5060 / 8000, samples: 200, loss: 243.127451\n",
      "iteration 5070 / 8000, samples: 200, loss: 329.204364\n",
      "iteration 5080 / 8000, samples: 200, loss: 290.729045\n",
      "iteration 5090 / 8000, samples: 200, loss: 251.044382\n",
      "iteration 5100 / 8000, samples: 200, loss: 341.633510\n",
      "iteration 5110 / 8000, samples: 200, loss: 329.478414\n",
      "iteration 5120 / 8000, samples: 200, loss: 264.523631\n",
      "iteration 5130 / 8000, samples: 200, loss: 236.228856\n",
      "iteration 5140 / 8000, samples: 200, loss: 148.466928\n",
      "iteration 5150 / 8000, samples: 200, loss: 257.352284\n",
      "iteration 5160 / 8000, samples: 200, loss: 256.479029\n",
      "iteration 5170 / 8000, samples: 200, loss: 333.610064\n",
      "iteration 5180 / 8000, samples: 200, loss: 198.737187\n",
      "iteration 5190 / 8000, samples: 200, loss: 244.390020\n",
      "iteration 5200 / 8000, samples: 200, loss: 167.822826\n",
      "iteration 5210 / 8000, samples: 200, loss: 276.665199\n",
      "iteration 5220 / 8000, samples: 200, loss: 182.474909\n",
      "iteration 5230 / 8000, samples: 200, loss: 235.592615\n",
      "iteration 5240 / 8000, samples: 200, loss: 361.022690\n",
      "iteration 5250 / 8000, samples: 200, loss: 160.598401\n",
      "iteration 5260 / 8000, samples: 200, loss: 230.460746\n",
      "iteration 5270 / 8000, samples: 200, loss: 237.927837\n",
      "iteration 5280 / 8000, samples: 200, loss: 289.157560\n",
      "iteration 5290 / 8000, samples: 200, loss: 195.729814\n",
      "iteration 5300 / 8000, samples: 200, loss: 401.015488\n",
      "iteration 5310 / 8000, samples: 200, loss: 195.895780\n",
      "iteration 5320 / 8000, samples: 200, loss: 194.748744\n",
      "iteration 5330 / 8000, samples: 200, loss: 261.942059\n",
      "iteration 5340 / 8000, samples: 200, loss: 392.868429\n",
      "iteration 5350 / 8000, samples: 200, loss: 223.285124\n",
      "iteration 5360 / 8000, samples: 200, loss: 290.935932\n",
      "iteration 5370 / 8000, samples: 200, loss: 329.959727\n",
      "iteration 5380 / 8000, samples: 200, loss: 186.121312\n",
      "iteration 5390 / 8000, samples: 200, loss: 215.096690\n",
      "iteration 5400 / 8000, samples: 200, loss: 436.572190\n",
      "iteration 5410 / 8000, samples: 200, loss: 325.596824\n",
      "iteration 5420 / 8000, samples: 200, loss: 175.286334\n",
      "iteration 5430 / 8000, samples: 200, loss: 393.501137\n",
      "iteration 5440 / 8000, samples: 200, loss: 195.076062\n",
      "iteration 5450 / 8000, samples: 200, loss: 226.470224\n",
      "iteration 5460 / 8000, samples: 200, loss: 287.763874\n",
      "iteration 5470 / 8000, samples: 200, loss: 166.447957\n",
      "iteration 5480 / 8000, samples: 200, loss: 240.190369\n",
      "iteration 5490 / 8000, samples: 200, loss: 279.640791\n",
      "iteration 5500 / 8000, samples: 200, loss: 307.069854\n",
      "iteration 5510 / 8000, samples: 200, loss: 207.902206\n",
      "iteration 5520 / 8000, samples: 200, loss: 261.490387\n",
      "iteration 5530 / 8000, samples: 200, loss: 253.532609\n",
      "iteration 5540 / 8000, samples: 200, loss: 227.288116\n",
      "iteration 5550 / 8000, samples: 200, loss: 356.501973\n",
      "iteration 5560 / 8000, samples: 200, loss: 394.671280\n",
      "iteration 5570 / 8000, samples: 200, loss: 161.798472\n",
      "iteration 5580 / 8000, samples: 200, loss: 434.815758\n",
      "iteration 5590 / 8000, samples: 200, loss: 352.160556\n",
      "iteration 5600 / 8000, samples: 200, loss: 293.833188\n",
      "iteration 5610 / 8000, samples: 200, loss: 357.506532\n",
      "iteration 5620 / 8000, samples: 200, loss: 221.072064\n",
      "iteration 5630 / 8000, samples: 200, loss: 225.876085\n",
      "iteration 5640 / 8000, samples: 200, loss: 221.703469\n",
      "iteration 5650 / 8000, samples: 200, loss: 321.522190\n",
      "iteration 5660 / 8000, samples: 200, loss: 285.886007\n",
      "iteration 5670 / 8000, samples: 200, loss: 318.807805\n",
      "iteration 5680 / 8000, samples: 200, loss: 295.463985\n",
      "iteration 5690 / 8000, samples: 200, loss: 257.718682\n",
      "iteration 5700 / 8000, samples: 200, loss: 218.065653\n",
      "iteration 5710 / 8000, samples: 200, loss: 190.268724\n",
      "iteration 5720 / 8000, samples: 200, loss: 177.597778\n",
      "iteration 5730 / 8000, samples: 200, loss: 215.817221\n",
      "iteration 5740 / 8000, samples: 200, loss: 266.208424\n",
      "iteration 5750 / 8000, samples: 200, loss: 330.431841\n",
      "iteration 5760 / 8000, samples: 200, loss: 201.565459\n",
      "iteration 5770 / 8000, samples: 200, loss: 293.368916\n",
      "iteration 5780 / 8000, samples: 200, loss: 215.343932\n",
      "iteration 5790 / 8000, samples: 200, loss: 329.738404\n",
      "iteration 5800 / 8000, samples: 200, loss: 233.597641\n",
      "iteration 5810 / 8000, samples: 200, loss: 149.057554\n",
      "iteration 5820 / 8000, samples: 200, loss: 274.498817\n",
      "iteration 5830 / 8000, samples: 200, loss: 149.608489\n",
      "iteration 5840 / 8000, samples: 200, loss: 322.265792\n",
      "iteration 5850 / 8000, samples: 200, loss: 225.730863\n",
      "iteration 5860 / 8000, samples: 200, loss: 195.897885\n",
      "iteration 5870 / 8000, samples: 200, loss: 296.158307\n",
      "iteration 5880 / 8000, samples: 200, loss: 213.947772\n",
      "iteration 5890 / 8000, samples: 200, loss: 305.122518\n",
      "iteration 5900 / 8000, samples: 200, loss: 348.514449\n",
      "iteration 5910 / 8000, samples: 200, loss: 179.292560\n",
      "iteration 5920 / 8000, samples: 200, loss: 361.682272\n",
      "iteration 5930 / 8000, samples: 200, loss: 359.310522\n",
      "iteration 5940 / 8000, samples: 200, loss: 285.577899\n",
      "iteration 5950 / 8000, samples: 200, loss: 281.587590\n",
      "iteration 5960 / 8000, samples: 200, loss: 194.958315\n",
      "iteration 5970 / 8000, samples: 200, loss: 228.848430\n",
      "iteration 5980 / 8000, samples: 200, loss: 242.592914\n",
      "iteration 5990 / 8000, samples: 200, loss: 281.146793\n",
      "iteration 6000 / 8000, samples: 200, loss: 242.231506\n",
      "iteration 6010 / 8000, samples: 200, loss: 200.859587\n",
      "iteration 6020 / 8000, samples: 200, loss: 355.341378\n",
      "iteration 6030 / 8000, samples: 200, loss: 216.301037\n",
      "iteration 6040 / 8000, samples: 200, loss: 227.136727\n",
      "iteration 6050 / 8000, samples: 200, loss: 206.351152\n",
      "iteration 6060 / 8000, samples: 200, loss: 205.181706\n",
      "iteration 6070 / 8000, samples: 200, loss: 181.357120\n",
      "iteration 6080 / 8000, samples: 200, loss: 173.588392\n",
      "iteration 6090 / 8000, samples: 200, loss: 310.699105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6100 / 8000, samples: 200, loss: 253.017252\n",
      "iteration 6110 / 8000, samples: 200, loss: 217.860359\n",
      "iteration 6120 / 8000, samples: 200, loss: 284.984417\n",
      "iteration 6130 / 8000, samples: 200, loss: 328.384810\n",
      "iteration 6140 / 8000, samples: 200, loss: 194.688035\n",
      "iteration 6150 / 8000, samples: 200, loss: 154.295326\n",
      "iteration 6160 / 8000, samples: 200, loss: 378.579194\n",
      "iteration 6170 / 8000, samples: 200, loss: 274.567289\n",
      "iteration 6180 / 8000, samples: 200, loss: 294.358726\n",
      "iteration 6190 / 8000, samples: 200, loss: 212.170501\n",
      "iteration 6200 / 8000, samples: 200, loss: 332.700334\n",
      "iteration 6210 / 8000, samples: 200, loss: 288.239731\n",
      "iteration 6220 / 8000, samples: 200, loss: 328.729324\n",
      "iteration 6230 / 8000, samples: 200, loss: 463.629973\n",
      "iteration 6240 / 8000, samples: 200, loss: 334.620923\n",
      "iteration 6250 / 8000, samples: 200, loss: 142.253729\n",
      "iteration 6260 / 8000, samples: 200, loss: 238.978906\n",
      "iteration 6270 / 8000, samples: 200, loss: 413.649964\n",
      "iteration 6280 / 8000, samples: 200, loss: 241.505424\n",
      "iteration 6290 / 8000, samples: 200, loss: 321.982374\n",
      "iteration 6300 / 8000, samples: 200, loss: 297.647240\n",
      "iteration 6310 / 8000, samples: 200, loss: 215.178543\n",
      "iteration 6320 / 8000, samples: 200, loss: 334.863147\n",
      "iteration 6330 / 8000, samples: 200, loss: 198.001332\n",
      "iteration 6340 / 8000, samples: 200, loss: 290.264828\n",
      "iteration 6350 / 8000, samples: 200, loss: 174.622143\n",
      "iteration 6360 / 8000, samples: 200, loss: 328.999921\n",
      "iteration 6370 / 8000, samples: 200, loss: 257.628660\n",
      "iteration 6380 / 8000, samples: 200, loss: 195.767215\n",
      "iteration 6390 / 8000, samples: 200, loss: 215.714604\n",
      "iteration 6400 / 8000, samples: 200, loss: 303.827901\n",
      "iteration 6410 / 8000, samples: 200, loss: 132.718450\n",
      "iteration 6420 / 8000, samples: 200, loss: 211.552395\n",
      "iteration 6430 / 8000, samples: 200, loss: 288.781904\n",
      "iteration 6440 / 8000, samples: 200, loss: 308.366622\n",
      "iteration 6450 / 8000, samples: 200, loss: 191.689370\n",
      "iteration 6460 / 8000, samples: 200, loss: 154.186676\n",
      "iteration 6470 / 8000, samples: 200, loss: 252.650944\n",
      "iteration 6480 / 8000, samples: 200, loss: 242.927336\n",
      "iteration 6490 / 8000, samples: 200, loss: 215.308637\n",
      "iteration 6500 / 8000, samples: 200, loss: 201.826613\n",
      "iteration 6510 / 8000, samples: 200, loss: 180.197693\n",
      "iteration 6520 / 8000, samples: 200, loss: 260.697566\n",
      "iteration 6530 / 8000, samples: 200, loss: 292.650933\n",
      "iteration 6540 / 8000, samples: 200, loss: 366.752952\n",
      "iteration 6550 / 8000, samples: 200, loss: 231.096013\n",
      "iteration 6560 / 8000, samples: 200, loss: 399.146298\n",
      "iteration 6570 / 8000, samples: 200, loss: 225.987429\n",
      "iteration 6580 / 8000, samples: 200, loss: 240.308124\n",
      "iteration 6590 / 8000, samples: 200, loss: 316.875900\n",
      "iteration 6600 / 8000, samples: 200, loss: 270.846030\n",
      "iteration 6610 / 8000, samples: 200, loss: 237.038515\n",
      "iteration 6620 / 8000, samples: 200, loss: 224.151444\n",
      "iteration 6630 / 8000, samples: 200, loss: 423.734280\n",
      "iteration 6640 / 8000, samples: 200, loss: 265.135598\n",
      "iteration 6650 / 8000, samples: 200, loss: 272.913134\n",
      "iteration 6660 / 8000, samples: 200, loss: 382.599494\n",
      "iteration 6670 / 8000, samples: 200, loss: 229.464480\n",
      "iteration 6680 / 8000, samples: 200, loss: 165.963946\n",
      "iteration 6690 / 8000, samples: 200, loss: 284.903114\n",
      "iteration 6700 / 8000, samples: 200, loss: 201.486158\n",
      "iteration 6710 / 8000, samples: 200, loss: 325.610578\n",
      "iteration 6720 / 8000, samples: 200, loss: 155.612838\n",
      "iteration 6730 / 8000, samples: 200, loss: 211.104288\n",
      "iteration 6740 / 8000, samples: 200, loss: 305.470244\n",
      "iteration 6750 / 8000, samples: 200, loss: 308.480984\n",
      "iteration 6760 / 8000, samples: 200, loss: 393.656315\n",
      "iteration 6770 / 8000, samples: 200, loss: 210.996982\n",
      "iteration 6780 / 8000, samples: 200, loss: 201.496448\n",
      "iteration 6790 / 8000, samples: 200, loss: 177.421551\n",
      "iteration 6800 / 8000, samples: 200, loss: 197.477788\n",
      "iteration 6810 / 8000, samples: 200, loss: 359.006247\n",
      "iteration 6820 / 8000, samples: 200, loss: 238.343248\n",
      "iteration 6830 / 8000, samples: 200, loss: 354.938007\n",
      "iteration 6840 / 8000, samples: 200, loss: 178.974387\n",
      "iteration 6850 / 8000, samples: 200, loss: 274.947817\n",
      "iteration 6860 / 8000, samples: 200, loss: 282.823711\n",
      "iteration 6870 / 8000, samples: 200, loss: 357.363288\n",
      "iteration 6880 / 8000, samples: 200, loss: 284.985481\n",
      "iteration 6890 / 8000, samples: 200, loss: 162.861744\n",
      "iteration 6900 / 8000, samples: 200, loss: 235.856042\n",
      "iteration 6910 / 8000, samples: 200, loss: 293.961866\n",
      "iteration 6920 / 8000, samples: 200, loss: 247.336008\n",
      "iteration 6930 / 8000, samples: 200, loss: 209.895407\n",
      "iteration 6940 / 8000, samples: 200, loss: 226.667662\n",
      "iteration 6950 / 8000, samples: 200, loss: 194.682328\n",
      "iteration 6960 / 8000, samples: 200, loss: 332.705693\n",
      "iteration 6970 / 8000, samples: 200, loss: 168.754679\n",
      "iteration 6980 / 8000, samples: 200, loss: 312.651630\n",
      "iteration 6990 / 8000, samples: 200, loss: 213.283415\n",
      "iteration 7000 / 8000, samples: 200, loss: 313.463209\n",
      "iteration 7010 / 8000, samples: 200, loss: 244.123743\n",
      "iteration 7020 / 8000, samples: 200, loss: 234.068560\n",
      "iteration 7030 / 8000, samples: 200, loss: 213.752451\n",
      "iteration 7040 / 8000, samples: 200, loss: 286.119408\n",
      "iteration 7050 / 8000, samples: 200, loss: 333.819991\n",
      "iteration 7060 / 8000, samples: 200, loss: 118.715775\n",
      "iteration 7070 / 8000, samples: 200, loss: 247.443744\n",
      "iteration 7080 / 8000, samples: 200, loss: 219.707504\n",
      "iteration 7090 / 8000, samples: 200, loss: 279.895946\n",
      "iteration 7100 / 8000, samples: 200, loss: 224.930792\n",
      "iteration 7110 / 8000, samples: 200, loss: 309.608733\n",
      "iteration 7120 / 8000, samples: 200, loss: 279.636557\n",
      "iteration 7130 / 8000, samples: 200, loss: 421.375001\n",
      "iteration 7140 / 8000, samples: 200, loss: 185.075234\n",
      "iteration 7150 / 8000, samples: 200, loss: 268.795600\n",
      "iteration 7160 / 8000, samples: 200, loss: 294.593944\n",
      "iteration 7170 / 8000, samples: 200, loss: 188.474356\n",
      "iteration 7180 / 8000, samples: 200, loss: 210.754045\n",
      "iteration 7190 / 8000, samples: 200, loss: 236.666173\n",
      "iteration 7200 / 8000, samples: 200, loss: 200.775746\n",
      "iteration 7210 / 8000, samples: 200, loss: 225.537852\n",
      "iteration 7220 / 8000, samples: 200, loss: 275.969101\n",
      "iteration 7230 / 8000, samples: 200, loss: 184.951539\n",
      "iteration 7240 / 8000, samples: 200, loss: 318.544545\n",
      "iteration 7250 / 8000, samples: 200, loss: 157.237195\n",
      "iteration 7260 / 8000, samples: 200, loss: 246.471507\n",
      "iteration 7270 / 8000, samples: 200, loss: 203.454177\n",
      "iteration 7280 / 8000, samples: 200, loss: 217.964320\n",
      "iteration 7290 / 8000, samples: 200, loss: 306.583829\n",
      "iteration 7300 / 8000, samples: 200, loss: 367.092687\n",
      "iteration 7310 / 8000, samples: 200, loss: 289.090176\n",
      "iteration 7320 / 8000, samples: 200, loss: 200.624599\n",
      "iteration 7330 / 8000, samples: 200, loss: 219.360141\n",
      "iteration 7340 / 8000, samples: 200, loss: 205.732495\n",
      "iteration 7350 / 8000, samples: 200, loss: 255.978178\n",
      "iteration 7360 / 8000, samples: 200, loss: 180.349268\n",
      "iteration 7370 / 8000, samples: 200, loss: 308.828932\n",
      "iteration 7380 / 8000, samples: 200, loss: 365.117003\n",
      "iteration 7390 / 8000, samples: 200, loss: 281.450294\n",
      "iteration 7400 / 8000, samples: 200, loss: 184.512572\n",
      "iteration 7410 / 8000, samples: 200, loss: 163.136050\n",
      "iteration 7420 / 8000, samples: 200, loss: 276.189195\n",
      "iteration 7430 / 8000, samples: 200, loss: 226.215482\n",
      "iteration 7440 / 8000, samples: 200, loss: 172.423907\n",
      "iteration 7450 / 8000, samples: 200, loss: 212.545690\n",
      "iteration 7460 / 8000, samples: 200, loss: 217.604580\n",
      "iteration 7470 / 8000, samples: 200, loss: 243.808982\n",
      "iteration 7480 / 8000, samples: 200, loss: 302.270015\n",
      "iteration 7490 / 8000, samples: 200, loss: 264.466784\n",
      "iteration 7500 / 8000, samples: 200, loss: 213.786091\n",
      "iteration 7510 / 8000, samples: 200, loss: 344.347897\n",
      "iteration 7520 / 8000, samples: 200, loss: 313.448296\n",
      "iteration 7530 / 8000, samples: 200, loss: 205.650973\n",
      "iteration 7540 / 8000, samples: 200, loss: 204.801395\n",
      "iteration 7550 / 8000, samples: 200, loss: 192.656856\n",
      "iteration 7560 / 8000, samples: 200, loss: 317.016482\n",
      "iteration 7570 / 8000, samples: 200, loss: 304.918041\n",
      "iteration 7580 / 8000, samples: 200, loss: 321.912615\n",
      "iteration 7590 / 8000, samples: 200, loss: 454.655316\n",
      "iteration 7600 / 8000, samples: 200, loss: 231.214293\n",
      "iteration 7610 / 8000, samples: 200, loss: 145.995099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7620 / 8000, samples: 200, loss: 215.372328\n",
      "iteration 7630 / 8000, samples: 200, loss: 170.849039\n",
      "iteration 7640 / 8000, samples: 200, loss: 294.178895\n",
      "iteration 7650 / 8000, samples: 200, loss: 208.380927\n",
      "iteration 7660 / 8000, samples: 200, loss: 219.754992\n",
      "iteration 7670 / 8000, samples: 200, loss: 299.295333\n",
      "iteration 7680 / 8000, samples: 200, loss: 311.112318\n",
      "iteration 7690 / 8000, samples: 200, loss: 363.348691\n",
      "iteration 7700 / 8000, samples: 200, loss: 368.478742\n",
      "iteration 7710 / 8000, samples: 200, loss: 277.858001\n",
      "iteration 7720 / 8000, samples: 200, loss: 199.831006\n",
      "iteration 7730 / 8000, samples: 200, loss: 234.781872\n",
      "iteration 7740 / 8000, samples: 200, loss: 332.478460\n",
      "iteration 7750 / 8000, samples: 200, loss: 253.925128\n",
      "iteration 7760 / 8000, samples: 200, loss: 301.538011\n",
      "iteration 7770 / 8000, samples: 200, loss: 296.588428\n",
      "iteration 7780 / 8000, samples: 200, loss: 224.757219\n",
      "iteration 7790 / 8000, samples: 200, loss: 299.623274\n",
      "iteration 7800 / 8000, samples: 200, loss: 349.888793\n",
      "iteration 7810 / 8000, samples: 200, loss: 201.370028\n",
      "iteration 7820 / 8000, samples: 200, loss: 214.380327\n",
      "iteration 7830 / 8000, samples: 200, loss: 220.741657\n",
      "iteration 7840 / 8000, samples: 200, loss: 194.912402\n",
      "iteration 7850 / 8000, samples: 200, loss: 276.478569\n",
      "iteration 7860 / 8000, samples: 200, loss: 183.595634\n",
      "iteration 7870 / 8000, samples: 200, loss: 261.406168\n",
      "iteration 7880 / 8000, samples: 200, loss: 350.990905\n",
      "iteration 7890 / 8000, samples: 200, loss: 392.898608\n",
      "iteration 7900 / 8000, samples: 200, loss: 396.069333\n",
      "iteration 7910 / 8000, samples: 200, loss: 230.430273\n",
      "iteration 7920 / 8000, samples: 200, loss: 179.003600\n",
      "iteration 7930 / 8000, samples: 200, loss: 190.138692\n",
      "iteration 7940 / 8000, samples: 200, loss: 358.627615\n",
      "iteration 7950 / 8000, samples: 200, loss: 203.214570\n",
      "iteration 7960 / 8000, samples: 200, loss: 215.970056\n",
      "iteration 7970 / 8000, samples: 200, loss: 184.180019\n",
      "iteration 7980 / 8000, samples: 200, loss: 287.168668\n",
      "iteration 7990 / 8000, samples: 200, loss: 343.923914\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "file_path = './'\n",
    "\n",
    "dataTrain, labelTrain, dataTest, labelTest = load_file(file_path)\n",
    "dataTrain = dataTrain - np.mean(dataTrain, axis=0)\n",
    "LC = LinearClassifier()\n",
    "\n",
    "print('start training ...')\n",
    "#train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True)\n",
    "#在dataTrain中不重复随机抽取batch_size个样本，迭代训练num_iters次\n",
    "loss_all = LC.train(dataTrain, labelTrain, num_iters = 8000, batch_size = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss is 213.846863\n",
      "start predicting ...\n",
      "the accuracy rate is 20.000000 \n"
     ]
    }
   ],
   "source": [
    "print('last loss is %f' %(loss_all[-1]))\n",
    "#开始预测\n",
    "print('start predicting ...')\n",
    "y_pred = LC.predict(dataTest)\n",
    "\n",
    "hit = 0\n",
    "for i in xrange(y_pred.size):\n",
    "\tif (y_pred[i] == labelTest[i]):\n",
    "\t\thit += 1\n",
    "\n",
    "print('the accuracy rate is %f ' % (hit/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
