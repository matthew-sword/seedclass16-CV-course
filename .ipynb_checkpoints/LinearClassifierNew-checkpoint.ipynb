{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import print_function\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 文件读取\n",
    "def unpickle(file):\n",
    "\twith open(file,'rb') as fo:\n",
    "\t\tdict = pickle.load(fo)\n",
    "\treturn dict\n",
    "\n",
    "def load_file(file):\n",
    "\tdictTrain = unpickle(file + \"data_batch_1\")\n",
    "\tdataTrain = dictTrain['data']\n",
    "\tlabelTrain = dictTrain['labels']\n",
    "\n",
    "\tfor i in range(2,6):\n",
    "\t\tdictTrain = unpickle(file + \"data_batch_\" + str(i))\n",
    "\t\tdataTrain = np.vstack([dataTrain,dictTrain['data']])\n",
    "\t\tlabelTrain = np.hstack([labelTrain,dictTrain['labels']])\n",
    "\n",
    "\tdictTest = unpickle(file + \"test_batch\")\n",
    "\tdataTest = dictTest['data']\n",
    "\tlabelTest = dictTest['labels']\n",
    "\tlabelTest = np.array(labelTest)\n",
    "\n",
    "\treturn dataTrain, labelTrain, dataTest, labelTest\n",
    "\n",
    "\n",
    "#softmax loss 函数\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "\t'''\n",
    "\t\tW:权重矩阵\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t'''\n",
    "\t#初始化数据\n",
    "\tloss = 0.0\n",
    "\tdW = np.zeros_like(W)\n",
    "\tnum_train = X.shape[0]\t#样本数\n",
    "\tnum_class = W.shape[1]\t#样本类别数\n",
    "\n",
    "\tfor i in xrange(num_train):\n",
    "\t\tscore = X[i].dot(W)\n",
    "\t\tscore -= np.max(score)\t#提高样本稳定性\n",
    "\n",
    "\t\tcorrect_score = score[y[i]]\n",
    "\t\texp_sum = np.sum(np.exp(score))\n",
    "\t\tloss += np.log(exp_sum) - correct_score\n",
    "\n",
    "\t\tfor j in xrange(num_class):\n",
    "\t\t\tif (j == y[i]):\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i] - X[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i]\n",
    "\n",
    "\n",
    "\tloss /= num_train\n",
    "\tloss += 0.5 * reg * np.sum(W*W)\n",
    "\n",
    "\tdW /= num_train\n",
    "\tdW += reg * W\n",
    "\n",
    "\treturn loss, dW\n",
    "\n",
    "#线性分类器\n",
    "class LinearClassifier(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.W = None\n",
    "\n",
    "\tdef train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\tstep_size:学习步进速度\n",
    "\t\treg:正则化强度\n",
    "\t\tnum_iters:迭代次数\n",
    "\t\tbatch_size:每次迭代图片样本数\n",
    "\t\tverbose:是否打印信息\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss_history:每次训练loss值\n",
    "\t\t'''\n",
    "\t\tnum_train, dim = X.shape\n",
    "\t\tnum_classes = np.max(y) + 1\n",
    "\t\n",
    "\t\tif self.W is None:\n",
    "\t\t\tself.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "\t\tloss_history = []\n",
    "\n",
    "\t\tfor it in xrange(num_iters):\n",
    "\t\t\t#从样本中不重复随机采batch_size个样本\n",
    "\t\t\tsample_index = np.random.choice(num_train, batch_size, replace=False)\n",
    "\n",
    "\t\t\tX_batch = X[sample_index, :]\n",
    "\t\t\ty_batch = y[sample_index]\n",
    "\n",
    "\t\t\tloss, grad = self.loss(X_batch, y_batch, reg)\n",
    "\t\t\tloss_history.append(loss)\n",
    "\n",
    "\t\t\tself.W += -step_size * grad\n",
    "\n",
    "\t\t\tif (verbose and it %10 == 0):\n",
    "\t\t\t\tprint('iteration %d / %d, samples: %d, loss: %f' % (it, num_iters, batch_size, loss))\n",
    "\n",
    "\t\treturn loss_history\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\ty_pred:标签预测值\n",
    "\t\t'''\n",
    "\t\ty_pred = np.zeros(X.shape[1])\n",
    "\n",
    "\t\tscore = X.dot(self.W)\n",
    "\t\ty_pred = np.argmax(score, axis = 1)\n",
    "\n",
    "\t\treturn y_pred\n",
    "\n",
    "\n",
    "\tdef loss(self, X_batch, y_batch, reg):\n",
    "\t\t'''\n",
    "\t\tX_batch:图片训练集(矩阵)\n",
    "\t\ty_batch:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t\t'''\n",
    "\t\treturn softmax_loss_naive(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from PIL import Image\n",
    "#开始训练\n",
    "file_path = './'\n",
    "\n",
    "dataTrain, labelTrain, dataTest, labelTest = load_file(file_path)\n",
    "\n",
    "# dataTr = np.zeros((dataTrFirst.shape[0],32*32))\n",
    "# dataTs = np.zeros((dataTsFirst.shape[0],32*32))\n",
    "\n",
    "\n",
    "# for i in range(dataTrFirst.shape[0] -45000):\n",
    "#     img = dataTrFirst[i].reshape((32,32,3))\n",
    "#     img = Image.fromarray(img)\n",
    "#     res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "#     dataTr[i] = res.reshape((1,32*32))\n",
    "# print(\"训练集加载完成\")\n",
    "\n",
    "# for i in range(dataTsFirst.shape[0] -1):\n",
    "#     img = dataTsFirst[i].reshape((32,32,3))\n",
    "#     img = Image.fromarray(img)\n",
    "#     res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "#     dataTs[i] = res.reshape((1,32*32))\n",
    "# print(\"测试集加载完成\")\n",
    "\n",
    "dataTrain = dataTrain - np.mean(dataTrain, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ...\n",
      "iteration 0 / 8000, samples: 200, loss: 5.268518\n",
      "iteration 10 / 8000, samples: 200, loss: 291.940793\n",
      "iteration 20 / 8000, samples: 200, loss: 470.167491\n",
      "iteration 30 / 8000, samples: 200, loss: 396.503014\n",
      "iteration 40 / 8000, samples: 200, loss: 275.653119\n",
      "iteration 50 / 8000, samples: 200, loss: 236.857250\n",
      "iteration 60 / 8000, samples: 200, loss: 428.768571\n",
      "iteration 70 / 8000, samples: 200, loss: 183.326149\n",
      "iteration 80 / 8000, samples: 200, loss: 358.236100\n",
      "iteration 90 / 8000, samples: 200, loss: 382.733480\n",
      "iteration 100 / 8000, samples: 200, loss: 296.860364\n",
      "iteration 110 / 8000, samples: 200, loss: 342.799669\n",
      "iteration 120 / 8000, samples: 200, loss: 208.957833\n",
      "iteration 130 / 8000, samples: 200, loss: 291.712020\n",
      "iteration 140 / 8000, samples: 200, loss: 210.666215\n",
      "iteration 150 / 8000, samples: 200, loss: 348.312329\n",
      "iteration 160 / 8000, samples: 200, loss: 308.057992\n",
      "iteration 170 / 8000, samples: 200, loss: 210.753489\n",
      "iteration 180 / 8000, samples: 200, loss: 263.919887\n",
      "iteration 190 / 8000, samples: 200, loss: 274.271095\n",
      "iteration 200 / 8000, samples: 200, loss: 221.299277\n",
      "iteration 210 / 8000, samples: 200, loss: 150.081320\n",
      "iteration 220 / 8000, samples: 200, loss: 309.060043\n",
      "iteration 230 / 8000, samples: 200, loss: 315.663864\n",
      "iteration 240 / 8000, samples: 200, loss: 348.232948\n",
      "iteration 250 / 8000, samples: 200, loss: 290.296792\n",
      "iteration 260 / 8000, samples: 200, loss: 406.811201\n",
      "iteration 270 / 8000, samples: 200, loss: 416.192522\n",
      "iteration 280 / 8000, samples: 200, loss: 299.340323\n",
      "iteration 290 / 8000, samples: 200, loss: 282.761614\n",
      "iteration 300 / 8000, samples: 200, loss: 213.751966\n",
      "iteration 310 / 8000, samples: 200, loss: 356.955063\n",
      "iteration 320 / 8000, samples: 200, loss: 208.460204\n",
      "iteration 330 / 8000, samples: 200, loss: 319.292269\n",
      "iteration 340 / 8000, samples: 200, loss: 183.365317\n",
      "iteration 350 / 8000, samples: 200, loss: 203.062555\n",
      "iteration 360 / 8000, samples: 200, loss: 206.498174\n",
      "iteration 370 / 8000, samples: 200, loss: 262.720530\n",
      "iteration 380 / 8000, samples: 200, loss: 231.619494\n",
      "iteration 390 / 8000, samples: 200, loss: 300.337811\n",
      "iteration 400 / 8000, samples: 200, loss: 344.661464\n",
      "iteration 410 / 8000, samples: 200, loss: 206.430454\n",
      "iteration 420 / 8000, samples: 200, loss: 298.861691\n",
      "iteration 430 / 8000, samples: 200, loss: 252.368619\n",
      "iteration 440 / 8000, samples: 200, loss: 302.039930\n",
      "iteration 450 / 8000, samples: 200, loss: 307.825542\n",
      "iteration 460 / 8000, samples: 200, loss: 362.980741\n",
      "iteration 470 / 8000, samples: 200, loss: 219.731803\n",
      "iteration 480 / 8000, samples: 200, loss: 299.971361\n",
      "iteration 490 / 8000, samples: 200, loss: 223.658341\n",
      "iteration 500 / 8000, samples: 200, loss: 244.905919\n",
      "iteration 510 / 8000, samples: 200, loss: 351.958076\n",
      "iteration 520 / 8000, samples: 200, loss: 410.277457\n",
      "iteration 530 / 8000, samples: 200, loss: 269.008184\n",
      "iteration 540 / 8000, samples: 200, loss: 262.678161\n",
      "iteration 550 / 8000, samples: 200, loss: 315.665105\n",
      "iteration 560 / 8000, samples: 200, loss: 308.426081\n",
      "iteration 570 / 8000, samples: 200, loss: 440.849699\n",
      "iteration 580 / 8000, samples: 200, loss: 235.175880\n",
      "iteration 590 / 8000, samples: 200, loss: 222.929919\n",
      "iteration 600 / 8000, samples: 200, loss: 243.654307\n",
      "iteration 610 / 8000, samples: 200, loss: 356.580513\n",
      "iteration 620 / 8000, samples: 200, loss: 202.574142\n",
      "iteration 630 / 8000, samples: 200, loss: 345.704041\n",
      "iteration 640 / 8000, samples: 200, loss: 234.169737\n",
      "iteration 650 / 8000, samples: 200, loss: 277.986809\n",
      "iteration 660 / 8000, samples: 200, loss: 146.259650\n",
      "iteration 670 / 8000, samples: 200, loss: 289.693520\n",
      "iteration 680 / 8000, samples: 200, loss: 280.530961\n",
      "iteration 690 / 8000, samples: 200, loss: 306.747653\n",
      "iteration 700 / 8000, samples: 200, loss: 350.711828\n",
      "iteration 710 / 8000, samples: 200, loss: 250.850721\n",
      "iteration 720 / 8000, samples: 200, loss: 252.059488\n",
      "iteration 730 / 8000, samples: 200, loss: 352.756510\n",
      "iteration 740 / 8000, samples: 200, loss: 275.811803\n",
      "iteration 750 / 8000, samples: 200, loss: 310.183442\n",
      "iteration 760 / 8000, samples: 200, loss: 259.450870\n",
      "iteration 770 / 8000, samples: 200, loss: 259.947070\n",
      "iteration 780 / 8000, samples: 200, loss: 338.306138\n",
      "iteration 790 / 8000, samples: 200, loss: 312.633166\n",
      "iteration 800 / 8000, samples: 200, loss: 197.452361\n",
      "iteration 810 / 8000, samples: 200, loss: 171.151837\n",
      "iteration 820 / 8000, samples: 200, loss: 300.880736\n",
      "iteration 830 / 8000, samples: 200, loss: 282.728486\n",
      "iteration 840 / 8000, samples: 200, loss: 338.726825\n",
      "iteration 850 / 8000, samples: 200, loss: 314.931942\n",
      "iteration 860 / 8000, samples: 200, loss: 229.982900\n",
      "iteration 870 / 8000, samples: 200, loss: 207.608476\n",
      "iteration 880 / 8000, samples: 200, loss: 318.806136\n",
      "iteration 890 / 8000, samples: 200, loss: 348.539601\n",
      "iteration 900 / 8000, samples: 200, loss: 243.895867\n",
      "iteration 910 / 8000, samples: 200, loss: 248.750955\n",
      "iteration 920 / 8000, samples: 200, loss: 195.463976\n",
      "iteration 930 / 8000, samples: 200, loss: 237.848537\n",
      "iteration 940 / 8000, samples: 200, loss: 323.788307\n",
      "iteration 950 / 8000, samples: 200, loss: 321.620952\n",
      "iteration 960 / 8000, samples: 200, loss: 206.522282\n",
      "iteration 970 / 8000, samples: 200, loss: 372.259942\n",
      "iteration 980 / 8000, samples: 200, loss: 281.473886\n",
      "iteration 990 / 8000, samples: 200, loss: 379.269684\n",
      "iteration 1000 / 8000, samples: 200, loss: 418.170424\n",
      "iteration 1010 / 8000, samples: 200, loss: 278.000887\n",
      "iteration 1020 / 8000, samples: 200, loss: 256.798502\n",
      "iteration 1030 / 8000, samples: 200, loss: 158.244081\n",
      "iteration 1040 / 8000, samples: 200, loss: 236.786180\n",
      "iteration 1050 / 8000, samples: 200, loss: 410.151154\n",
      "iteration 1060 / 8000, samples: 200, loss: 311.800487\n",
      "iteration 1070 / 8000, samples: 200, loss: 373.843619\n",
      "iteration 1080 / 8000, samples: 200, loss: 280.393263\n",
      "iteration 1090 / 8000, samples: 200, loss: 180.837478\n",
      "iteration 1100 / 8000, samples: 200, loss: 265.586969\n",
      "iteration 1110 / 8000, samples: 200, loss: 286.849950\n",
      "iteration 1120 / 8000, samples: 200, loss: 411.470776\n",
      "iteration 1130 / 8000, samples: 200, loss: 331.213623\n",
      "iteration 1140 / 8000, samples: 200, loss: 228.997806\n",
      "iteration 1150 / 8000, samples: 200, loss: 230.203736\n",
      "iteration 1160 / 8000, samples: 200, loss: 227.910790\n",
      "iteration 1170 / 8000, samples: 200, loss: 295.320719\n",
      "iteration 1180 / 8000, samples: 200, loss: 370.383517\n",
      "iteration 1190 / 8000, samples: 200, loss: 255.626306\n",
      "iteration 1200 / 8000, samples: 200, loss: 317.257042\n",
      "iteration 1210 / 8000, samples: 200, loss: 342.381240\n",
      "iteration 1220 / 8000, samples: 200, loss: 242.080848\n",
      "iteration 1230 / 8000, samples: 200, loss: 220.785584\n",
      "iteration 1240 / 8000, samples: 200, loss: 318.817464\n",
      "iteration 1250 / 8000, samples: 200, loss: 302.925805\n",
      "iteration 1260 / 8000, samples: 200, loss: 230.103611\n",
      "iteration 1270 / 8000, samples: 200, loss: 202.782003\n",
      "iteration 1280 / 8000, samples: 200, loss: 323.485008\n",
      "iteration 1290 / 8000, samples: 200, loss: 403.052187\n",
      "iteration 1300 / 8000, samples: 200, loss: 280.535066\n",
      "iteration 1310 / 8000, samples: 200, loss: 201.373413\n",
      "iteration 1320 / 8000, samples: 200, loss: 298.008031\n",
      "iteration 1330 / 8000, samples: 200, loss: 285.536088\n",
      "iteration 1340 / 8000, samples: 200, loss: 319.268389\n",
      "iteration 1350 / 8000, samples: 200, loss: 250.505247\n",
      "iteration 1360 / 8000, samples: 200, loss: 189.251432\n",
      "iteration 1370 / 8000, samples: 200, loss: 239.997547\n",
      "iteration 1380 / 8000, samples: 200, loss: 252.914772\n",
      "iteration 1390 / 8000, samples: 200, loss: 269.682550\n",
      "iteration 1400 / 8000, samples: 200, loss: 214.574088\n",
      "iteration 1410 / 8000, samples: 200, loss: 358.726204\n",
      "iteration 1420 / 8000, samples: 200, loss: 243.873581\n",
      "iteration 1430 / 8000, samples: 200, loss: 230.069341\n",
      "iteration 1440 / 8000, samples: 200, loss: 309.727897\n",
      "iteration 1450 / 8000, samples: 200, loss: 238.270405\n",
      "iteration 1460 / 8000, samples: 200, loss: 262.826057\n",
      "iteration 1470 / 8000, samples: 200, loss: 284.192290\n",
      "iteration 1480 / 8000, samples: 200, loss: 183.441772\n",
      "iteration 1490 / 8000, samples: 200, loss: 202.977345\n",
      "iteration 1500 / 8000, samples: 200, loss: 350.771253\n",
      "iteration 1510 / 8000, samples: 200, loss: 230.445898\n",
      "iteration 1520 / 8000, samples: 200, loss: 234.936608\n",
      "iteration 1530 / 8000, samples: 200, loss: 272.097963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1540 / 8000, samples: 200, loss: 275.927853\n",
      "iteration 1550 / 8000, samples: 200, loss: 261.145642\n",
      "iteration 1560 / 8000, samples: 200, loss: 237.485043\n",
      "iteration 1570 / 8000, samples: 200, loss: 313.258484\n",
      "iteration 1580 / 8000, samples: 200, loss: 219.799681\n",
      "iteration 1590 / 8000, samples: 200, loss: 307.217927\n",
      "iteration 1600 / 8000, samples: 200, loss: 391.734598\n",
      "iteration 1610 / 8000, samples: 200, loss: 215.384876\n",
      "iteration 1620 / 8000, samples: 200, loss: 313.565272\n",
      "iteration 1630 / 8000, samples: 200, loss: 208.838502\n",
      "iteration 1640 / 8000, samples: 200, loss: 315.218747\n",
      "iteration 1650 / 8000, samples: 200, loss: 257.363664\n",
      "iteration 1660 / 8000, samples: 200, loss: 295.598060\n",
      "iteration 1670 / 8000, samples: 200, loss: 305.566637\n",
      "iteration 1680 / 8000, samples: 200, loss: 285.046993\n",
      "iteration 1690 / 8000, samples: 200, loss: 298.658021\n",
      "iteration 1700 / 8000, samples: 200, loss: 262.245421\n",
      "iteration 1710 / 8000, samples: 200, loss: 312.057060\n",
      "iteration 1720 / 8000, samples: 200, loss: 255.884502\n",
      "iteration 1730 / 8000, samples: 200, loss: 254.877878\n",
      "iteration 1740 / 8000, samples: 200, loss: 319.867958\n",
      "iteration 1750 / 8000, samples: 200, loss: 216.220392\n",
      "iteration 1760 / 8000, samples: 200, loss: 238.567146\n",
      "iteration 1770 / 8000, samples: 200, loss: 221.099638\n",
      "iteration 1780 / 8000, samples: 200, loss: 276.277757\n",
      "iteration 1790 / 8000, samples: 200, loss: 184.891473\n",
      "iteration 1800 / 8000, samples: 200, loss: 252.471088\n",
      "iteration 1810 / 8000, samples: 200, loss: 266.659353\n",
      "iteration 1820 / 8000, samples: 200, loss: 332.835290\n",
      "iteration 1830 / 8000, samples: 200, loss: 200.219789\n",
      "iteration 1840 / 8000, samples: 200, loss: 230.221921\n",
      "iteration 1850 / 8000, samples: 200, loss: 201.112538\n",
      "iteration 1860 / 8000, samples: 200, loss: 322.353196\n",
      "iteration 1870 / 8000, samples: 200, loss: 286.176168\n",
      "iteration 1880 / 8000, samples: 200, loss: 244.273941\n",
      "iteration 1890 / 8000, samples: 200, loss: 444.860181\n",
      "iteration 1900 / 8000, samples: 200, loss: 204.488750\n",
      "iteration 1910 / 8000, samples: 200, loss: 325.606439\n",
      "iteration 1920 / 8000, samples: 200, loss: 253.457371\n",
      "iteration 1930 / 8000, samples: 200, loss: 324.384124\n",
      "iteration 1940 / 8000, samples: 200, loss: 257.864872\n",
      "iteration 1950 / 8000, samples: 200, loss: 218.651562\n",
      "iteration 1960 / 8000, samples: 200, loss: 283.335459\n",
      "iteration 1970 / 8000, samples: 200, loss: 305.867490\n",
      "iteration 1980 / 8000, samples: 200, loss: 367.796308\n",
      "iteration 1990 / 8000, samples: 200, loss: 302.555202\n",
      "iteration 2000 / 8000, samples: 200, loss: 347.496898\n",
      "iteration 2010 / 8000, samples: 200, loss: 213.815681\n",
      "iteration 2020 / 8000, samples: 200, loss: 345.755556\n",
      "iteration 2030 / 8000, samples: 200, loss: 195.834512\n",
      "iteration 2040 / 8000, samples: 200, loss: 329.209225\n",
      "iteration 2050 / 8000, samples: 200, loss: 321.207190\n",
      "iteration 2060 / 8000, samples: 200, loss: 355.285101\n",
      "iteration 2070 / 8000, samples: 200, loss: 260.911648\n",
      "iteration 2080 / 8000, samples: 200, loss: 248.756533\n",
      "iteration 2090 / 8000, samples: 200, loss: 413.676464\n",
      "iteration 2100 / 8000, samples: 200, loss: 279.370199\n",
      "iteration 2110 / 8000, samples: 200, loss: 235.381543\n",
      "iteration 2120 / 8000, samples: 200, loss: 324.074472\n",
      "iteration 2130 / 8000, samples: 200, loss: 244.858678\n",
      "iteration 2140 / 8000, samples: 200, loss: 180.654701\n",
      "iteration 2150 / 8000, samples: 200, loss: 194.752190\n",
      "iteration 2160 / 8000, samples: 200, loss: 327.159775\n",
      "iteration 2170 / 8000, samples: 200, loss: 272.515927\n",
      "iteration 2180 / 8000, samples: 200, loss: 357.265682\n",
      "iteration 2190 / 8000, samples: 200, loss: 294.999459\n",
      "iteration 2200 / 8000, samples: 200, loss: 241.296196\n",
      "iteration 2210 / 8000, samples: 200, loss: 183.337266\n",
      "iteration 2220 / 8000, samples: 200, loss: 288.500127\n",
      "iteration 2230 / 8000, samples: 200, loss: 245.882982\n",
      "iteration 2240 / 8000, samples: 200, loss: 263.605350\n",
      "iteration 2250 / 8000, samples: 200, loss: 254.009124\n",
      "iteration 2260 / 8000, samples: 200, loss: 209.598472\n",
      "iteration 2270 / 8000, samples: 200, loss: 427.869103\n",
      "iteration 2280 / 8000, samples: 200, loss: 178.047396\n",
      "iteration 2290 / 8000, samples: 200, loss: 238.128442\n",
      "iteration 2300 / 8000, samples: 200, loss: 262.162117\n",
      "iteration 2310 / 8000, samples: 200, loss: 259.949854\n",
      "iteration 2320 / 8000, samples: 200, loss: 231.894111\n",
      "iteration 2330 / 8000, samples: 200, loss: 341.322102\n",
      "iteration 2340 / 8000, samples: 200, loss: 215.966774\n",
      "iteration 2350 / 8000, samples: 200, loss: 217.992937\n",
      "iteration 2360 / 8000, samples: 200, loss: 211.991490\n",
      "iteration 2370 / 8000, samples: 200, loss: 210.857657\n",
      "iteration 2380 / 8000, samples: 200, loss: 327.180487\n",
      "iteration 2390 / 8000, samples: 200, loss: 252.187969\n",
      "iteration 2400 / 8000, samples: 200, loss: 331.952809\n",
      "iteration 2410 / 8000, samples: 200, loss: 289.908484\n",
      "iteration 2420 / 8000, samples: 200, loss: 221.788414\n",
      "iteration 2430 / 8000, samples: 200, loss: 320.481576\n",
      "iteration 2440 / 8000, samples: 200, loss: 249.057233\n",
      "iteration 2450 / 8000, samples: 200, loss: 266.175638\n",
      "iteration 2460 / 8000, samples: 200, loss: 229.383094\n",
      "iteration 2470 / 8000, samples: 200, loss: 388.476361\n",
      "iteration 2480 / 8000, samples: 200, loss: 252.883133\n",
      "iteration 2490 / 8000, samples: 200, loss: 450.183426\n",
      "iteration 2500 / 8000, samples: 200, loss: 228.004273\n",
      "iteration 2510 / 8000, samples: 200, loss: 294.290753\n",
      "iteration 2520 / 8000, samples: 200, loss: 189.339432\n",
      "iteration 2530 / 8000, samples: 200, loss: 260.915460\n",
      "iteration 2540 / 8000, samples: 200, loss: 326.189866\n",
      "iteration 2550 / 8000, samples: 200, loss: 127.478914\n",
      "iteration 2560 / 8000, samples: 200, loss: 518.810884\n",
      "iteration 2570 / 8000, samples: 200, loss: 279.616665\n",
      "iteration 2580 / 8000, samples: 200, loss: 245.846067\n",
      "iteration 2590 / 8000, samples: 200, loss: 177.876778\n",
      "iteration 2600 / 8000, samples: 200, loss: 171.098758\n",
      "iteration 2610 / 8000, samples: 200, loss: 190.953339\n",
      "iteration 2620 / 8000, samples: 200, loss: 191.702128\n",
      "iteration 2630 / 8000, samples: 200, loss: 256.795831\n",
      "iteration 2640 / 8000, samples: 200, loss: 295.928666\n",
      "iteration 2650 / 8000, samples: 200, loss: 223.514331\n",
      "iteration 2660 / 8000, samples: 200, loss: 342.246468\n",
      "iteration 2670 / 8000, samples: 200, loss: 260.656457\n",
      "iteration 2680 / 8000, samples: 200, loss: 233.521051\n",
      "iteration 2690 / 8000, samples: 200, loss: 215.559929\n",
      "iteration 2700 / 8000, samples: 200, loss: 307.035224\n",
      "iteration 2710 / 8000, samples: 200, loss: 273.481522\n",
      "iteration 2720 / 8000, samples: 200, loss: 303.981400\n",
      "iteration 2730 / 8000, samples: 200, loss: 208.449824\n",
      "iteration 2740 / 8000, samples: 200, loss: 237.176590\n",
      "iteration 2750 / 8000, samples: 200, loss: 313.276230\n",
      "iteration 2760 / 8000, samples: 200, loss: 279.275885\n",
      "iteration 2770 / 8000, samples: 200, loss: 314.090278\n",
      "iteration 2780 / 8000, samples: 200, loss: 252.471474\n",
      "iteration 2790 / 8000, samples: 200, loss: 192.540737\n",
      "iteration 2800 / 8000, samples: 200, loss: 255.114961\n",
      "iteration 2810 / 8000, samples: 200, loss: 381.165216\n",
      "iteration 2820 / 8000, samples: 200, loss: 235.118728\n",
      "iteration 2830 / 8000, samples: 200, loss: 246.197991\n",
      "iteration 2840 / 8000, samples: 200, loss: 189.770134\n",
      "iteration 2850 / 8000, samples: 200, loss: 357.983426\n",
      "iteration 2860 / 8000, samples: 200, loss: 151.247034\n",
      "iteration 2870 / 8000, samples: 200, loss: 267.783970\n",
      "iteration 2880 / 8000, samples: 200, loss: 226.970428\n",
      "iteration 2890 / 8000, samples: 200, loss: 280.744193\n",
      "iteration 2900 / 8000, samples: 200, loss: 228.557077\n",
      "iteration 2910 / 8000, samples: 200, loss: 237.378397\n",
      "iteration 2920 / 8000, samples: 200, loss: 231.879817\n",
      "iteration 2930 / 8000, samples: 200, loss: 291.262696\n",
      "iteration 2940 / 8000, samples: 200, loss: 294.563956\n",
      "iteration 2950 / 8000, samples: 200, loss: 229.887826\n",
      "iteration 2960 / 8000, samples: 200, loss: 358.002961\n",
      "iteration 2970 / 8000, samples: 200, loss: 191.742948\n",
      "iteration 2980 / 8000, samples: 200, loss: 390.452760\n",
      "iteration 2990 / 8000, samples: 200, loss: 320.729383\n",
      "iteration 3000 / 8000, samples: 200, loss: 363.951495\n",
      "iteration 3010 / 8000, samples: 200, loss: 190.537061\n",
      "iteration 3020 / 8000, samples: 200, loss: 331.986888\n",
      "iteration 3030 / 8000, samples: 200, loss: 197.310864\n",
      "iteration 3040 / 8000, samples: 200, loss: 235.451040\n",
      "iteration 3050 / 8000, samples: 200, loss: 233.224390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3060 / 8000, samples: 200, loss: 320.448220\n",
      "iteration 3070 / 8000, samples: 200, loss: 184.840753\n",
      "iteration 3080 / 8000, samples: 200, loss: 266.490771\n",
      "iteration 3090 / 8000, samples: 200, loss: 184.129155\n",
      "iteration 3100 / 8000, samples: 200, loss: 243.706917\n",
      "iteration 3110 / 8000, samples: 200, loss: 260.926889\n",
      "iteration 3120 / 8000, samples: 200, loss: 234.330504\n",
      "iteration 3130 / 8000, samples: 200, loss: 231.896349\n",
      "iteration 3140 / 8000, samples: 200, loss: 330.745662\n",
      "iteration 3150 / 8000, samples: 200, loss: 174.298711\n",
      "iteration 3160 / 8000, samples: 200, loss: 319.474986\n",
      "iteration 3170 / 8000, samples: 200, loss: 201.436183\n",
      "iteration 3180 / 8000, samples: 200, loss: 315.133982\n",
      "iteration 3190 / 8000, samples: 200, loss: 280.575722\n",
      "iteration 3200 / 8000, samples: 200, loss: 447.235335\n",
      "iteration 3210 / 8000, samples: 200, loss: 309.060283\n",
      "iteration 3220 / 8000, samples: 200, loss: 314.937993\n",
      "iteration 3230 / 8000, samples: 200, loss: 509.295417\n",
      "iteration 3240 / 8000, samples: 200, loss: 259.801295\n",
      "iteration 3250 / 8000, samples: 200, loss: 273.663884\n",
      "iteration 3260 / 8000, samples: 200, loss: 284.483582\n",
      "iteration 3270 / 8000, samples: 200, loss: 256.841001\n",
      "iteration 3280 / 8000, samples: 200, loss: 291.521099\n",
      "iteration 3290 / 8000, samples: 200, loss: 287.284391\n",
      "iteration 3300 / 8000, samples: 200, loss: 233.486640\n",
      "iteration 3310 / 8000, samples: 200, loss: 235.856420\n",
      "iteration 3320 / 8000, samples: 200, loss: 231.602823\n",
      "iteration 3330 / 8000, samples: 200, loss: 237.376768\n",
      "iteration 3340 / 8000, samples: 200, loss: 304.870574\n",
      "iteration 3350 / 8000, samples: 200, loss: 158.558080\n",
      "iteration 3360 / 8000, samples: 200, loss: 364.499224\n",
      "iteration 3370 / 8000, samples: 200, loss: 232.158650\n",
      "iteration 3380 / 8000, samples: 200, loss: 250.700132\n",
      "iteration 3390 / 8000, samples: 200, loss: 316.165409\n",
      "iteration 3400 / 8000, samples: 200, loss: 329.816699\n",
      "iteration 3410 / 8000, samples: 200, loss: 513.522639\n",
      "iteration 3420 / 8000, samples: 200, loss: 182.444144\n",
      "iteration 3430 / 8000, samples: 200, loss: 327.943605\n",
      "iteration 3440 / 8000, samples: 200, loss: 300.128899\n",
      "iteration 3450 / 8000, samples: 200, loss: 202.345706\n",
      "iteration 3460 / 8000, samples: 200, loss: 347.472771\n",
      "iteration 3470 / 8000, samples: 200, loss: 313.026261\n",
      "iteration 3480 / 8000, samples: 200, loss: 163.027254\n",
      "iteration 3490 / 8000, samples: 200, loss: 321.685153\n",
      "iteration 3500 / 8000, samples: 200, loss: 241.711861\n",
      "iteration 3510 / 8000, samples: 200, loss: 318.385568\n",
      "iteration 3520 / 8000, samples: 200, loss: 349.818534\n",
      "iteration 3530 / 8000, samples: 200, loss: 243.669448\n",
      "iteration 3540 / 8000, samples: 200, loss: 182.687648\n",
      "iteration 3550 / 8000, samples: 200, loss: 265.184578\n",
      "iteration 3560 / 8000, samples: 200, loss: 296.443261\n",
      "iteration 3570 / 8000, samples: 200, loss: 380.655325\n",
      "iteration 3580 / 8000, samples: 200, loss: 224.409125\n",
      "iteration 3590 / 8000, samples: 200, loss: 162.922396\n",
      "iteration 3600 / 8000, samples: 200, loss: 230.316753\n",
      "iteration 3610 / 8000, samples: 200, loss: 294.687392\n",
      "iteration 3620 / 8000, samples: 200, loss: 322.924756\n",
      "iteration 3630 / 8000, samples: 200, loss: 298.952032\n",
      "iteration 3640 / 8000, samples: 200, loss: 232.507008\n",
      "iteration 3650 / 8000, samples: 200, loss: 263.537788\n",
      "iteration 3660 / 8000, samples: 200, loss: 218.000230\n",
      "iteration 3670 / 8000, samples: 200, loss: 267.387446\n",
      "iteration 3680 / 8000, samples: 200, loss: 199.260361\n",
      "iteration 3690 / 8000, samples: 200, loss: 203.478464\n",
      "iteration 3700 / 8000, samples: 200, loss: 335.306418\n",
      "iteration 3710 / 8000, samples: 200, loss: 361.003967\n",
      "iteration 3720 / 8000, samples: 200, loss: 277.910968\n",
      "iteration 3730 / 8000, samples: 200, loss: 240.594860\n",
      "iteration 3740 / 8000, samples: 200, loss: 310.657149\n",
      "iteration 3750 / 8000, samples: 200, loss: 382.201299\n",
      "iteration 3760 / 8000, samples: 200, loss: 258.257735\n",
      "iteration 3770 / 8000, samples: 200, loss: 338.643241\n",
      "iteration 3780 / 8000, samples: 200, loss: 247.727631\n",
      "iteration 3790 / 8000, samples: 200, loss: 415.276423\n",
      "iteration 3800 / 8000, samples: 200, loss: 242.564239\n",
      "iteration 3810 / 8000, samples: 200, loss: 294.234783\n",
      "iteration 3820 / 8000, samples: 200, loss: 408.356414\n",
      "iteration 3830 / 8000, samples: 200, loss: 255.822281\n",
      "iteration 3840 / 8000, samples: 200, loss: 310.465487\n",
      "iteration 3850 / 8000, samples: 200, loss: 212.341495\n",
      "iteration 3860 / 8000, samples: 200, loss: 268.517527\n",
      "iteration 3870 / 8000, samples: 200, loss: 319.423392\n",
      "iteration 3880 / 8000, samples: 200, loss: 219.580447\n",
      "iteration 3890 / 8000, samples: 200, loss: 224.853908\n",
      "iteration 3900 / 8000, samples: 200, loss: 228.690150\n",
      "iteration 3910 / 8000, samples: 200, loss: 308.485356\n",
      "iteration 3920 / 8000, samples: 200, loss: 235.266755\n",
      "iteration 3930 / 8000, samples: 200, loss: 331.728109\n",
      "iteration 3940 / 8000, samples: 200, loss: 234.259571\n",
      "iteration 3950 / 8000, samples: 200, loss: 210.601819\n",
      "iteration 3960 / 8000, samples: 200, loss: 300.780201\n",
      "iteration 3970 / 8000, samples: 200, loss: 260.367532\n",
      "iteration 3980 / 8000, samples: 200, loss: 184.103665\n",
      "iteration 3990 / 8000, samples: 200, loss: 169.685901\n",
      "iteration 4000 / 8000, samples: 200, loss: 255.274953\n",
      "iteration 4010 / 8000, samples: 200, loss: 375.553115\n",
      "iteration 4020 / 8000, samples: 200, loss: 250.756484\n",
      "iteration 4030 / 8000, samples: 200, loss: 205.476796\n",
      "iteration 4040 / 8000, samples: 200, loss: 180.469607\n",
      "iteration 4050 / 8000, samples: 200, loss: 206.294029\n",
      "iteration 4060 / 8000, samples: 200, loss: 345.168386\n",
      "iteration 4070 / 8000, samples: 200, loss: 259.497582\n",
      "iteration 4080 / 8000, samples: 200, loss: 323.689389\n",
      "iteration 4090 / 8000, samples: 200, loss: 280.400010\n",
      "iteration 4100 / 8000, samples: 200, loss: 243.113648\n",
      "iteration 4110 / 8000, samples: 200, loss: 222.720259\n",
      "iteration 4120 / 8000, samples: 200, loss: 302.317620\n",
      "iteration 4130 / 8000, samples: 200, loss: 234.201110\n",
      "iteration 4140 / 8000, samples: 200, loss: 335.904722\n",
      "iteration 4150 / 8000, samples: 200, loss: 275.039601\n",
      "iteration 4160 / 8000, samples: 200, loss: 319.577909\n",
      "iteration 4170 / 8000, samples: 200, loss: 260.520104\n",
      "iteration 4180 / 8000, samples: 200, loss: 350.046729\n",
      "iteration 4190 / 8000, samples: 200, loss: 223.239727\n",
      "iteration 4200 / 8000, samples: 200, loss: 162.483273\n",
      "iteration 4210 / 8000, samples: 200, loss: 334.824477\n",
      "iteration 4220 / 8000, samples: 200, loss: 326.312565\n",
      "iteration 4230 / 8000, samples: 200, loss: 171.990466\n",
      "iteration 4240 / 8000, samples: 200, loss: 320.767299\n",
      "iteration 4250 / 8000, samples: 200, loss: 254.330575\n",
      "iteration 4260 / 8000, samples: 200, loss: 153.881482\n",
      "iteration 4270 / 8000, samples: 200, loss: 291.162003\n",
      "iteration 4280 / 8000, samples: 200, loss: 268.886443\n",
      "iteration 4290 / 8000, samples: 200, loss: 334.264785\n",
      "iteration 4300 / 8000, samples: 200, loss: 236.764413\n",
      "iteration 4310 / 8000, samples: 200, loss: 381.694179\n",
      "iteration 4320 / 8000, samples: 200, loss: 309.622259\n",
      "iteration 4330 / 8000, samples: 200, loss: 241.732699\n",
      "iteration 4340 / 8000, samples: 200, loss: 258.314687\n",
      "iteration 4350 / 8000, samples: 200, loss: 159.036943\n",
      "iteration 4360 / 8000, samples: 200, loss: 495.948124\n",
      "iteration 4370 / 8000, samples: 200, loss: 212.822006\n",
      "iteration 4380 / 8000, samples: 200, loss: 246.682540\n",
      "iteration 4390 / 8000, samples: 200, loss: 273.045682\n",
      "iteration 4400 / 8000, samples: 200, loss: 265.301743\n",
      "iteration 4410 / 8000, samples: 200, loss: 359.974792\n",
      "iteration 4420 / 8000, samples: 200, loss: 297.508130\n",
      "iteration 4430 / 8000, samples: 200, loss: 283.831582\n",
      "iteration 4440 / 8000, samples: 200, loss: 198.604681\n",
      "iteration 4450 / 8000, samples: 200, loss: 382.057443\n",
      "iteration 4460 / 8000, samples: 200, loss: 382.877159\n",
      "iteration 4470 / 8000, samples: 200, loss: 190.812179\n",
      "iteration 4480 / 8000, samples: 200, loss: 402.113997\n",
      "iteration 4490 / 8000, samples: 200, loss: 394.912628\n",
      "iteration 4500 / 8000, samples: 200, loss: 239.855271\n",
      "iteration 4510 / 8000, samples: 200, loss: 229.182374\n",
      "iteration 4520 / 8000, samples: 200, loss: 335.124865\n",
      "iteration 4530 / 8000, samples: 200, loss: 354.642277\n",
      "iteration 4540 / 8000, samples: 200, loss: 278.063814\n",
      "iteration 4550 / 8000, samples: 200, loss: 543.649154\n",
      "iteration 4560 / 8000, samples: 200, loss: 281.644188\n",
      "iteration 4570 / 8000, samples: 200, loss: 230.170111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4580 / 8000, samples: 200, loss: 328.711687\n",
      "iteration 4590 / 8000, samples: 200, loss: 297.786349\n",
      "iteration 4600 / 8000, samples: 200, loss: 348.361284\n",
      "iteration 4610 / 8000, samples: 200, loss: 317.082724\n",
      "iteration 4620 / 8000, samples: 200, loss: 246.756211\n",
      "iteration 4630 / 8000, samples: 200, loss: 265.346265\n",
      "iteration 4640 / 8000, samples: 200, loss: 272.449176\n",
      "iteration 4650 / 8000, samples: 200, loss: 141.150995\n",
      "iteration 4660 / 8000, samples: 200, loss: 213.236622\n",
      "iteration 4670 / 8000, samples: 200, loss: 217.091130\n",
      "iteration 4680 / 8000, samples: 200, loss: 374.299227\n",
      "iteration 4690 / 8000, samples: 200, loss: 247.404468\n",
      "iteration 4700 / 8000, samples: 200, loss: 324.301494\n",
      "iteration 4710 / 8000, samples: 200, loss: 448.222456\n",
      "iteration 4720 / 8000, samples: 200, loss: 293.709007\n",
      "iteration 4730 / 8000, samples: 200, loss: 216.722408\n",
      "iteration 4740 / 8000, samples: 200, loss: 188.422227\n",
      "iteration 4750 / 8000, samples: 200, loss: 275.477861\n",
      "iteration 4760 / 8000, samples: 200, loss: 430.701769\n",
      "iteration 4770 / 8000, samples: 200, loss: 266.383534\n",
      "iteration 4780 / 8000, samples: 200, loss: 278.875386\n",
      "iteration 4790 / 8000, samples: 200, loss: 251.817219\n",
      "iteration 4800 / 8000, samples: 200, loss: 295.534486\n",
      "iteration 4810 / 8000, samples: 200, loss: 348.789601\n",
      "iteration 4820 / 8000, samples: 200, loss: 315.402328\n",
      "iteration 4830 / 8000, samples: 200, loss: 199.156053\n",
      "iteration 4840 / 8000, samples: 200, loss: 259.138701\n",
      "iteration 4850 / 8000, samples: 200, loss: 229.671942\n",
      "iteration 4860 / 8000, samples: 200, loss: 150.751742\n",
      "iteration 4870 / 8000, samples: 200, loss: 288.441293\n",
      "iteration 4880 / 8000, samples: 200, loss: 242.131866\n",
      "iteration 4890 / 8000, samples: 200, loss: 256.506656\n",
      "iteration 4900 / 8000, samples: 200, loss: 224.078133\n",
      "iteration 4910 / 8000, samples: 200, loss: 313.464016\n",
      "iteration 4920 / 8000, samples: 200, loss: 147.178601\n",
      "iteration 4930 / 8000, samples: 200, loss: 316.025225\n",
      "iteration 4940 / 8000, samples: 200, loss: 213.831349\n",
      "iteration 4950 / 8000, samples: 200, loss: 262.943570\n",
      "iteration 4960 / 8000, samples: 200, loss: 248.814240\n",
      "iteration 4970 / 8000, samples: 200, loss: 253.667566\n",
      "iteration 4980 / 8000, samples: 200, loss: 267.310699\n",
      "iteration 4990 / 8000, samples: 200, loss: 204.587846\n",
      "iteration 5000 / 8000, samples: 200, loss: 281.094089\n",
      "iteration 5010 / 8000, samples: 200, loss: 157.120727\n",
      "iteration 5020 / 8000, samples: 200, loss: 400.582885\n",
      "iteration 5030 / 8000, samples: 200, loss: 301.654160\n",
      "iteration 5040 / 8000, samples: 200, loss: 261.439555\n",
      "iteration 5050 / 8000, samples: 200, loss: 125.069085\n",
      "iteration 5060 / 8000, samples: 200, loss: 220.927275\n",
      "iteration 5070 / 8000, samples: 200, loss: 202.249253\n",
      "iteration 5080 / 8000, samples: 200, loss: 327.259766\n",
      "iteration 5090 / 8000, samples: 200, loss: 287.827766\n",
      "iteration 5100 / 8000, samples: 200, loss: 155.488340\n",
      "iteration 5110 / 8000, samples: 200, loss: 235.920807\n",
      "iteration 5120 / 8000, samples: 200, loss: 333.066762\n",
      "iteration 5130 / 8000, samples: 200, loss: 274.076675\n",
      "iteration 5140 / 8000, samples: 200, loss: 142.844905\n",
      "iteration 5150 / 8000, samples: 200, loss: 222.046872\n",
      "iteration 5160 / 8000, samples: 200, loss: 311.961559\n",
      "iteration 5170 / 8000, samples: 200, loss: 302.700873\n",
      "iteration 5180 / 8000, samples: 200, loss: 247.850199\n",
      "iteration 5190 / 8000, samples: 200, loss: 148.445729\n",
      "iteration 5200 / 8000, samples: 200, loss: 192.281865\n",
      "iteration 5210 / 8000, samples: 200, loss: 256.291065\n",
      "iteration 5220 / 8000, samples: 200, loss: 259.114655\n",
      "iteration 5230 / 8000, samples: 200, loss: 283.427986\n",
      "iteration 5240 / 8000, samples: 200, loss: 299.586125\n",
      "iteration 5250 / 8000, samples: 200, loss: 233.257768\n",
      "iteration 5260 / 8000, samples: 200, loss: 343.140416\n",
      "iteration 5270 / 8000, samples: 200, loss: 356.798181\n",
      "iteration 5280 / 8000, samples: 200, loss: 294.344476\n",
      "iteration 5290 / 8000, samples: 200, loss: 356.599872\n",
      "iteration 5300 / 8000, samples: 200, loss: 246.157438\n",
      "iteration 5310 / 8000, samples: 200, loss: 361.521995\n",
      "iteration 5320 / 8000, samples: 200, loss: 203.243939\n",
      "iteration 5330 / 8000, samples: 200, loss: 272.639235\n",
      "iteration 5340 / 8000, samples: 200, loss: 327.961378\n",
      "iteration 5350 / 8000, samples: 200, loss: 246.447238\n",
      "iteration 5360 / 8000, samples: 200, loss: 211.118690\n",
      "iteration 5370 / 8000, samples: 200, loss: 244.476161\n",
      "iteration 5380 / 8000, samples: 200, loss: 195.575338\n",
      "iteration 5390 / 8000, samples: 200, loss: 287.764150\n",
      "iteration 5400 / 8000, samples: 200, loss: 390.191377\n",
      "iteration 5410 / 8000, samples: 200, loss: 253.416317\n",
      "iteration 5420 / 8000, samples: 200, loss: 207.822143\n",
      "iteration 5430 / 8000, samples: 200, loss: 218.254644\n",
      "iteration 5440 / 8000, samples: 200, loss: 196.414062\n",
      "iteration 5450 / 8000, samples: 200, loss: 186.081816\n",
      "iteration 5460 / 8000, samples: 200, loss: 201.064677\n",
      "iteration 5470 / 8000, samples: 200, loss: 208.383657\n",
      "iteration 5480 / 8000, samples: 200, loss: 166.671508\n",
      "iteration 5490 / 8000, samples: 200, loss: 429.261464\n",
      "iteration 5500 / 8000, samples: 200, loss: 326.684386\n",
      "iteration 5510 / 8000, samples: 200, loss: 246.568117\n",
      "iteration 5520 / 8000, samples: 200, loss: 308.204750\n",
      "iteration 5530 / 8000, samples: 200, loss: 264.706630\n",
      "iteration 5540 / 8000, samples: 200, loss: 293.941366\n",
      "iteration 5550 / 8000, samples: 200, loss: 227.403550\n",
      "iteration 5560 / 8000, samples: 200, loss: 211.406079\n",
      "iteration 5570 / 8000, samples: 200, loss: 246.661994\n",
      "iteration 5580 / 8000, samples: 200, loss: 244.128103\n",
      "iteration 5590 / 8000, samples: 200, loss: 245.611933\n",
      "iteration 5600 / 8000, samples: 200, loss: 237.638565\n",
      "iteration 5610 / 8000, samples: 200, loss: 285.720275\n",
      "iteration 5620 / 8000, samples: 200, loss: 284.528492\n",
      "iteration 5630 / 8000, samples: 200, loss: 206.522968\n",
      "iteration 5640 / 8000, samples: 200, loss: 281.510466\n",
      "iteration 5650 / 8000, samples: 200, loss: 220.352725\n",
      "iteration 5660 / 8000, samples: 200, loss: 202.761502\n",
      "iteration 5670 / 8000, samples: 200, loss: 240.495101\n",
      "iteration 5680 / 8000, samples: 200, loss: 214.577130\n",
      "iteration 5690 / 8000, samples: 200, loss: 360.562849\n",
      "iteration 5700 / 8000, samples: 200, loss: 236.751012\n",
      "iteration 5710 / 8000, samples: 200, loss: 379.437213\n",
      "iteration 5720 / 8000, samples: 200, loss: 261.229870\n",
      "iteration 5730 / 8000, samples: 200, loss: 236.876126\n",
      "iteration 5740 / 8000, samples: 200, loss: 358.045235\n",
      "iteration 5750 / 8000, samples: 200, loss: 248.460592\n",
      "iteration 5760 / 8000, samples: 200, loss: 323.402027\n",
      "iteration 5770 / 8000, samples: 200, loss: 385.681106\n",
      "iteration 5780 / 8000, samples: 200, loss: 363.120123\n",
      "iteration 5790 / 8000, samples: 200, loss: 452.751172\n",
      "iteration 5800 / 8000, samples: 200, loss: 251.317839\n",
      "iteration 5810 / 8000, samples: 200, loss: 276.316744\n",
      "iteration 5820 / 8000, samples: 200, loss: 246.143943\n",
      "iteration 5830 / 8000, samples: 200, loss: 231.018835\n",
      "iteration 5840 / 8000, samples: 200, loss: 262.304671\n",
      "iteration 5850 / 8000, samples: 200, loss: 194.563086\n",
      "iteration 5860 / 8000, samples: 200, loss: 212.530523\n",
      "iteration 5870 / 8000, samples: 200, loss: 205.306381\n",
      "iteration 5880 / 8000, samples: 200, loss: 289.199308\n",
      "iteration 5890 / 8000, samples: 200, loss: 348.588304\n",
      "iteration 5900 / 8000, samples: 200, loss: 187.412092\n",
      "iteration 5910 / 8000, samples: 200, loss: 312.403856\n",
      "iteration 5920 / 8000, samples: 200, loss: 263.577195\n",
      "iteration 5930 / 8000, samples: 200, loss: 167.526493\n",
      "iteration 5940 / 8000, samples: 200, loss: 278.882361\n",
      "iteration 5950 / 8000, samples: 200, loss: 246.052251\n",
      "iteration 5960 / 8000, samples: 200, loss: 189.177995\n",
      "iteration 5970 / 8000, samples: 200, loss: 220.059011\n",
      "iteration 5980 / 8000, samples: 200, loss: 246.839925\n",
      "iteration 5990 / 8000, samples: 200, loss: 280.996768\n",
      "iteration 6000 / 8000, samples: 200, loss: 310.910150\n",
      "iteration 6010 / 8000, samples: 200, loss: 410.593099\n",
      "iteration 6020 / 8000, samples: 200, loss: 184.442101\n",
      "iteration 6030 / 8000, samples: 200, loss: 264.433865\n",
      "iteration 6040 / 8000, samples: 200, loss: 235.554335\n",
      "iteration 6050 / 8000, samples: 200, loss: 250.833432\n",
      "iteration 6060 / 8000, samples: 200, loss: 205.564114\n",
      "iteration 6070 / 8000, samples: 200, loss: 218.105721\n",
      "iteration 6080 / 8000, samples: 200, loss: 442.831592\n",
      "iteration 6090 / 8000, samples: 200, loss: 211.414150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6100 / 8000, samples: 200, loss: 250.543461\n",
      "iteration 6110 / 8000, samples: 200, loss: 229.710313\n",
      "iteration 6120 / 8000, samples: 200, loss: 205.832703\n",
      "iteration 6130 / 8000, samples: 200, loss: 257.909443\n",
      "iteration 6140 / 8000, samples: 200, loss: 238.623465\n",
      "iteration 6150 / 8000, samples: 200, loss: 219.845721\n",
      "iteration 6160 / 8000, samples: 200, loss: 474.477869\n",
      "iteration 6170 / 8000, samples: 200, loss: 191.372476\n",
      "iteration 6180 / 8000, samples: 200, loss: 235.265353\n",
      "iteration 6190 / 8000, samples: 200, loss: 382.964771\n",
      "iteration 6200 / 8000, samples: 200, loss: 207.498829\n",
      "iteration 6210 / 8000, samples: 200, loss: 287.980463\n",
      "iteration 6220 / 8000, samples: 200, loss: 284.647924\n",
      "iteration 6230 / 8000, samples: 200, loss: 173.730160\n",
      "iteration 6240 / 8000, samples: 200, loss: 291.348192\n",
      "iteration 6250 / 8000, samples: 200, loss: 232.342768\n",
      "iteration 6260 / 8000, samples: 200, loss: 251.708824\n",
      "iteration 6270 / 8000, samples: 200, loss: 277.207818\n",
      "iteration 6280 / 8000, samples: 200, loss: 236.682815\n",
      "iteration 6290 / 8000, samples: 200, loss: 228.696081\n",
      "iteration 6300 / 8000, samples: 200, loss: 242.482934\n",
      "iteration 6310 / 8000, samples: 200, loss: 309.094678\n",
      "iteration 6320 / 8000, samples: 200, loss: 255.484437\n",
      "iteration 6330 / 8000, samples: 200, loss: 269.130582\n",
      "iteration 6340 / 8000, samples: 200, loss: 123.501883\n",
      "iteration 6350 / 8000, samples: 200, loss: 217.413589\n",
      "iteration 6360 / 8000, samples: 200, loss: 338.917710\n",
      "iteration 6370 / 8000, samples: 200, loss: 234.590132\n",
      "iteration 6380 / 8000, samples: 200, loss: 264.121231\n",
      "iteration 6390 / 8000, samples: 200, loss: 205.523189\n",
      "iteration 6400 / 8000, samples: 200, loss: 178.238205\n",
      "iteration 6410 / 8000, samples: 200, loss: 406.068637\n",
      "iteration 6420 / 8000, samples: 200, loss: 229.741679\n",
      "iteration 6430 / 8000, samples: 200, loss: 251.144305\n",
      "iteration 6440 / 8000, samples: 200, loss: 299.450456\n",
      "iteration 6450 / 8000, samples: 200, loss: 301.541620\n",
      "iteration 6460 / 8000, samples: 200, loss: 203.534413\n",
      "iteration 6470 / 8000, samples: 200, loss: 220.129776\n",
      "iteration 6480 / 8000, samples: 200, loss: 354.764835\n",
      "iteration 6490 / 8000, samples: 200, loss: 337.362322\n",
      "iteration 6500 / 8000, samples: 200, loss: 301.844551\n",
      "iteration 6510 / 8000, samples: 200, loss: 236.326620\n",
      "iteration 6520 / 8000, samples: 200, loss: 302.658949\n",
      "iteration 6530 / 8000, samples: 200, loss: 305.800279\n",
      "iteration 6540 / 8000, samples: 200, loss: 253.823130\n",
      "iteration 6550 / 8000, samples: 200, loss: 265.085202\n",
      "iteration 6560 / 8000, samples: 200, loss: 274.331803\n",
      "iteration 6570 / 8000, samples: 200, loss: 164.005752\n",
      "iteration 6580 / 8000, samples: 200, loss: 205.734964\n",
      "iteration 6590 / 8000, samples: 200, loss: 285.707878\n",
      "iteration 6600 / 8000, samples: 200, loss: 270.292114\n",
      "iteration 6610 / 8000, samples: 200, loss: 151.269189\n",
      "iteration 6620 / 8000, samples: 200, loss: 217.937904\n",
      "iteration 6630 / 8000, samples: 200, loss: 385.945605\n",
      "iteration 6640 / 8000, samples: 200, loss: 273.045431\n",
      "iteration 6650 / 8000, samples: 200, loss: 270.651081\n",
      "iteration 6660 / 8000, samples: 200, loss: 234.177016\n",
      "iteration 6670 / 8000, samples: 200, loss: 310.753437\n",
      "iteration 6680 / 8000, samples: 200, loss: 351.321844\n",
      "iteration 6690 / 8000, samples: 200, loss: 258.959377\n",
      "iteration 6700 / 8000, samples: 200, loss: 218.863595\n",
      "iteration 6710 / 8000, samples: 200, loss: 375.194962\n",
      "iteration 6720 / 8000, samples: 200, loss: 291.669912\n",
      "iteration 6730 / 8000, samples: 200, loss: 205.272775\n",
      "iteration 6740 / 8000, samples: 200, loss: 387.321803\n",
      "iteration 6750 / 8000, samples: 200, loss: 279.899511\n",
      "iteration 6760 / 8000, samples: 200, loss: 311.616290\n",
      "iteration 6770 / 8000, samples: 200, loss: 335.208238\n",
      "iteration 6780 / 8000, samples: 200, loss: 263.943945\n",
      "iteration 6790 / 8000, samples: 200, loss: 254.474681\n",
      "iteration 6800 / 8000, samples: 200, loss: 199.203540\n",
      "iteration 6810 / 8000, samples: 200, loss: 334.237982\n",
      "iteration 6820 / 8000, samples: 200, loss: 444.668560\n",
      "iteration 6830 / 8000, samples: 200, loss: 298.657582\n",
      "iteration 6840 / 8000, samples: 200, loss: 491.954717\n",
      "iteration 6850 / 8000, samples: 200, loss: 250.900832\n",
      "iteration 6860 / 8000, samples: 200, loss: 347.434245\n",
      "iteration 6870 / 8000, samples: 200, loss: 396.036337\n",
      "iteration 6880 / 8000, samples: 200, loss: 219.857663\n",
      "iteration 6890 / 8000, samples: 200, loss: 290.499046\n",
      "iteration 6900 / 8000, samples: 200, loss: 147.568042\n",
      "iteration 6910 / 8000, samples: 200, loss: 225.140430\n",
      "iteration 6920 / 8000, samples: 200, loss: 171.643724\n",
      "iteration 6930 / 8000, samples: 200, loss: 230.023305\n",
      "iteration 6940 / 8000, samples: 200, loss: 285.020745\n",
      "iteration 6950 / 8000, samples: 200, loss: 419.330535\n",
      "iteration 6960 / 8000, samples: 200, loss: 197.227786\n",
      "iteration 6970 / 8000, samples: 200, loss: 225.515402\n",
      "iteration 6980 / 8000, samples: 200, loss: 302.984618\n",
      "iteration 6990 / 8000, samples: 200, loss: 344.527350\n",
      "iteration 7000 / 8000, samples: 200, loss: 298.691043\n",
      "iteration 7010 / 8000, samples: 200, loss: 273.590637\n",
      "iteration 7020 / 8000, samples: 200, loss: 331.986450\n",
      "iteration 7030 / 8000, samples: 200, loss: 292.038414\n",
      "iteration 7040 / 8000, samples: 200, loss: 165.761671\n",
      "iteration 7050 / 8000, samples: 200, loss: 216.765495\n",
      "iteration 7060 / 8000, samples: 200, loss: 207.004385\n",
      "iteration 7070 / 8000, samples: 200, loss: 196.598805\n",
      "iteration 7080 / 8000, samples: 200, loss: 190.419465\n",
      "iteration 7090 / 8000, samples: 200, loss: 192.888975\n",
      "iteration 7100 / 8000, samples: 200, loss: 357.639469\n",
      "iteration 7110 / 8000, samples: 200, loss: 240.902971\n",
      "iteration 7120 / 8000, samples: 200, loss: 269.513241\n",
      "iteration 7130 / 8000, samples: 200, loss: 188.587732\n",
      "iteration 7140 / 8000, samples: 200, loss: 337.508121\n",
      "iteration 7150 / 8000, samples: 200, loss: 231.742358\n",
      "iteration 7160 / 8000, samples: 200, loss: 339.127760\n",
      "iteration 7170 / 8000, samples: 200, loss: 155.930261\n",
      "iteration 7180 / 8000, samples: 200, loss: 230.250628\n",
      "iteration 7190 / 8000, samples: 200, loss: 247.321825\n",
      "iteration 7200 / 8000, samples: 200, loss: 308.123867\n",
      "iteration 7210 / 8000, samples: 200, loss: 213.795888\n",
      "iteration 7220 / 8000, samples: 200, loss: 243.603963\n",
      "iteration 7230 / 8000, samples: 200, loss: 180.593848\n",
      "iteration 7240 / 8000, samples: 200, loss: 337.629693\n",
      "iteration 7250 / 8000, samples: 200, loss: 155.763904\n",
      "iteration 7260 / 8000, samples: 200, loss: 273.187220\n",
      "iteration 7270 / 8000, samples: 200, loss: 177.337391\n",
      "iteration 7280 / 8000, samples: 200, loss: 344.261765\n",
      "iteration 7290 / 8000, samples: 200, loss: 269.185347\n",
      "iteration 7300 / 8000, samples: 200, loss: 228.584533\n",
      "iteration 7310 / 8000, samples: 200, loss: 193.511614\n",
      "iteration 7320 / 8000, samples: 200, loss: 214.570599\n",
      "iteration 7330 / 8000, samples: 200, loss: 274.245355\n",
      "iteration 7340 / 8000, samples: 200, loss: 196.091428\n",
      "iteration 7350 / 8000, samples: 200, loss: 288.191717\n",
      "iteration 7360 / 8000, samples: 200, loss: 250.622316\n",
      "iteration 7370 / 8000, samples: 200, loss: 143.542639\n",
      "iteration 7380 / 8000, samples: 200, loss: 170.192013\n",
      "iteration 7390 / 8000, samples: 200, loss: 281.674255\n",
      "iteration 7400 / 8000, samples: 200, loss: 200.038089\n",
      "iteration 7410 / 8000, samples: 200, loss: 229.629297\n",
      "iteration 7420 / 8000, samples: 200, loss: 244.419938\n",
      "iteration 7430 / 8000, samples: 200, loss: 185.188049\n",
      "iteration 7440 / 8000, samples: 200, loss: 284.100666\n",
      "iteration 7450 / 8000, samples: 200, loss: 301.610366\n",
      "iteration 7460 / 8000, samples: 200, loss: 213.475066\n",
      "iteration 7470 / 8000, samples: 200, loss: 268.634421\n",
      "iteration 7480 / 8000, samples: 200, loss: 242.838151\n",
      "iteration 7490 / 8000, samples: 200, loss: 214.421725\n",
      "iteration 7500 / 8000, samples: 200, loss: 216.085001\n",
      "iteration 7510 / 8000, samples: 200, loss: 252.663512\n",
      "iteration 7520 / 8000, samples: 200, loss: 141.334426\n",
      "iteration 7530 / 8000, samples: 200, loss: 161.470592\n",
      "iteration 7540 / 8000, samples: 200, loss: 267.044811\n",
      "iteration 7550 / 8000, samples: 200, loss: 184.852791\n",
      "iteration 7560 / 8000, samples: 200, loss: 140.176179\n",
      "iteration 7570 / 8000, samples: 200, loss: 268.437650\n",
      "iteration 7580 / 8000, samples: 200, loss: 338.238257\n",
      "iteration 7590 / 8000, samples: 200, loss: 290.755670\n",
      "iteration 7600 / 8000, samples: 200, loss: 210.613487\n",
      "iteration 7610 / 8000, samples: 200, loss: 279.700814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7620 / 8000, samples: 200, loss: 248.702154\n",
      "iteration 7630 / 8000, samples: 200, loss: 232.692139\n",
      "iteration 7640 / 8000, samples: 200, loss: 304.088929\n",
      "iteration 7650 / 8000, samples: 200, loss: 280.670869\n",
      "iteration 7660 / 8000, samples: 200, loss: 168.065746\n",
      "iteration 7670 / 8000, samples: 200, loss: 221.830003\n",
      "iteration 7680 / 8000, samples: 200, loss: 245.889443\n",
      "iteration 7690 / 8000, samples: 200, loss: 253.207677\n",
      "iteration 7700 / 8000, samples: 200, loss: 174.472338\n",
      "iteration 7710 / 8000, samples: 200, loss: 207.997319\n",
      "iteration 7720 / 8000, samples: 200, loss: 150.486048\n",
      "iteration 7730 / 8000, samples: 200, loss: 197.704149\n",
      "iteration 7740 / 8000, samples: 200, loss: 351.033786\n",
      "iteration 7750 / 8000, samples: 200, loss: 339.638506\n",
      "iteration 7760 / 8000, samples: 200, loss: 205.783130\n",
      "iteration 7770 / 8000, samples: 200, loss: 440.982198\n",
      "iteration 7780 / 8000, samples: 200, loss: 357.596226\n",
      "iteration 7790 / 8000, samples: 200, loss: 159.558204\n",
      "iteration 7800 / 8000, samples: 200, loss: 170.886155\n",
      "iteration 7810 / 8000, samples: 200, loss: 258.778704\n",
      "iteration 7820 / 8000, samples: 200, loss: 211.517074\n",
      "iteration 7830 / 8000, samples: 200, loss: 241.998968\n",
      "iteration 7840 / 8000, samples: 200, loss: 260.616014\n",
      "iteration 7850 / 8000, samples: 200, loss: 374.551346\n",
      "iteration 7860 / 8000, samples: 200, loss: 229.336526\n",
      "iteration 7870 / 8000, samples: 200, loss: 262.435507\n",
      "iteration 7880 / 8000, samples: 200, loss: 403.287806\n",
      "iteration 7890 / 8000, samples: 200, loss: 225.490257\n",
      "iteration 7900 / 8000, samples: 200, loss: 376.292195\n",
      "iteration 7910 / 8000, samples: 200, loss: 322.870959\n",
      "iteration 7920 / 8000, samples: 200, loss: 307.619771\n",
      "iteration 7930 / 8000, samples: 200, loss: 198.938375\n",
      "iteration 7940 / 8000, samples: 200, loss: 218.116781\n",
      "iteration 7950 / 8000, samples: 200, loss: 310.825961\n",
      "iteration 7960 / 8000, samples: 200, loss: 313.095843\n",
      "iteration 7970 / 8000, samples: 200, loss: 294.723024\n",
      "iteration 7980 / 8000, samples: 200, loss: 357.616432\n",
      "iteration 7990 / 8000, samples: 200, loss: 247.528795\n"
     ]
    }
   ],
   "source": [
    "LC = LinearClassifier()\n",
    "\n",
    "print('start training ...')\n",
    "#train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True)\n",
    "#在dataTrain中不重复随机抽取batch_size个样本，迭代训练num_iters次\n",
    "loss_all = LC.train(dataTrain, labelTrain, num_iters = 8000, batch_size = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss is 249.670757\n",
      "start predicting ...\n",
      "the accuracy rate is 26.000000 \n"
     ]
    }
   ],
   "source": [
    "dataTest = dataTest - np.mean(dataTest, axis=0)\n",
    "print('last loss is %f' %(loss_all[-1]))\n",
    "#开始预测\n",
    "print('start predicting ...')\n",
    "y_pred = LC.predict(dataTest)\n",
    "\n",
    "hit = 0\n",
    "for i in xrange(y_pred.size):\n",
    "\tif (y_pred[i] == labelTest[i]):\n",
    "\t\thit += 1\n",
    "\n",
    "print('the accuracy rate is %f ' % (hit/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
