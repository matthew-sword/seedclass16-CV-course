{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import print_function\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 文件读取\n",
    "def unpickle(file):\n",
    "\twith open(file,'rb') as fo:\n",
    "\t\tdict = pickle.load(fo)\n",
    "\treturn dict\n",
    "\n",
    "def load_file(file):\n",
    "\tdictTrain = unpickle(file + \"data_batch_1\")\n",
    "\tdataTrain = dictTrain['data']\n",
    "\tlabelTrain = dictTrain['labels']\n",
    "\n",
    "\tfor i in range(2,6):\n",
    "\t\tdictTrain = unpickle(file + \"data_batch_\" + str(i))\n",
    "\t\tdataTrain = np.vstack([dataTrain,dictTrain['data']])\n",
    "\t\tlabelTrain = np.hstack([labelTrain,dictTrain['labels']])\n",
    "\n",
    "\tdictTest = unpickle(file + \"test_batch\")\n",
    "\tdataTest = dictTest['data']\n",
    "\tlabelTest = dictTest['labels']\n",
    "\tlabelTest = np.array(labelTest)\n",
    "\n",
    "\treturn dataTrain, labelTrain, dataTest, labelTest\n",
    "\n",
    "\n",
    "#softmax loss 函数\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "\t'''\n",
    "\t\tW:权重矩阵\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t'''\n",
    "\t#初始化数据\n",
    "\tloss = 0.0\n",
    "\tdW = np.zeros_like(W)\n",
    "\tnum_train = X.shape[0]\t#样本数\n",
    "\tnum_class = W.shape[1]\t#样本类别数\n",
    "\n",
    "\tfor i in xrange(num_train):\n",
    "\t\tscore = X[i].dot(W)\n",
    "\t\tscore -= np.max(score)\t#提高样本稳定性\n",
    "\n",
    "\t\tcorrect_score = score[y[i]]\n",
    "\t\texp_sum = np.sum(np.exp(score))\n",
    "\t\tloss += np.log(exp_sum) - correct_score\n",
    "\n",
    "\t\tfor j in xrange(num_class):\n",
    "\t\t\tif (j == y[i]):\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i] - X[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i]\n",
    "\n",
    "\n",
    "\tloss /= num_train\n",
    "\tloss += 0.5 * reg * np.sum(W*W)\n",
    "\n",
    "\tdW /= num_train\n",
    "\tdW += reg * W\n",
    "\n",
    "\treturn loss, dW\n",
    "\n",
    "#线性分类器\n",
    "class LinearClassifier(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.W = None\n",
    "\n",
    "\tdef train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\tstep_size:学习步进速度\n",
    "\t\treg:正则化强度\n",
    "\t\tnum_iters:迭代次数\n",
    "\t\tbatch_size:每次迭代图片样本数\n",
    "\t\tverbose:是否打印信息\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss_history:每次训练loss值\n",
    "\t\t'''\n",
    "\t\tnum_train, dim = X.shape\n",
    "\t\tnum_classes = np.max(y) + 1\n",
    "\t\n",
    "\t\tif self.W is None:\n",
    "\t\t\tself.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "\t\tloss_history = []\n",
    "\n",
    "\t\tfor it in xrange(num_iters):\n",
    "\t\t\t#从样本中不重复随机采batch_size个样本\n",
    "\t\t\tsample_index = np.random.choice(num_train, batch_size, replace=False)\n",
    "\n",
    "\t\t\tX_batch = X[sample_index, :]\n",
    "\t\t\ty_batch = y[sample_index]\n",
    "\n",
    "\t\t\tloss, grad = self.loss(X_batch, y_batch, reg)\n",
    "\t\t\tloss_history.append(loss)\n",
    "\n",
    "\t\t\tself.W += -step_size * grad\n",
    "\n",
    "\t\t\tif (verbose and it %10 == 0):\n",
    "\t\t\t\tprint('iteration %d / %d, samples: %d, loss: %f' % (it, num_iters, batch_size, loss))\n",
    "\n",
    "\t\treturn loss_history\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\ty_pred:标签预测值\n",
    "\t\t'''\n",
    "\t\ty_pred = np.zeros(X.shape[1])\n",
    "\n",
    "\t\tscore = X.dot(self.W)\n",
    "\t\ty_pred = np.argmax(score, axis = 1)\n",
    "\n",
    "\t\treturn y_pred\n",
    "\n",
    "\n",
    "\tdef loss(self, X_batch, y_batch, reg):\n",
    "\t\t'''\n",
    "\t\tX_batch:图片训练集(矩阵)\n",
    "\t\ty_batch:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t\t'''\n",
    "\t\treturn softmax_loss_naive(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集加载完成\n",
      "测试集加载完成\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from PIL import Image\n",
    "#开始训练\n",
    "file_path = './'\n",
    "\n",
    "dataTrFirst, labelTrain, dataTsFirst, labelTest = load_file(file_path)\n",
    "\n",
    "dataTr = np.zeros((dataTrFirst.shape[0],32*32))\n",
    "dataTs = np.zeros((dataTsFirst.shape[0],32*32))\n",
    "\n",
    "\n",
    "for i in range(dataTrFirst.shape[0] -45000):\n",
    "    img = dataTrFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTr[i] = res.reshape((1,32*32))\n",
    "print(\"训练集加载完成\")\n",
    "\n",
    "for i in range(dataTsFirst.shape[0] -1):\n",
    "    img = dataTsFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTs[i] = res.reshape((1,32*32))\n",
    "print(\"测试集加载完成\")\n",
    "\n",
    "# dataTrain = dataTrain - np.mean(dataTrain, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ...\n",
      "iteration 0 / 8000, samples: 200, loss: 5.403901\n",
      "iteration 10 / 8000, samples: 200, loss: 299.447268\n",
      "iteration 20 / 8000, samples: 200, loss: 366.026709\n",
      "iteration 30 / 8000, samples: 200, loss: 407.031004\n",
      "iteration 40 / 8000, samples: 200, loss: 262.697126\n",
      "iteration 50 / 8000, samples: 200, loss: 343.948993\n",
      "iteration 60 / 8000, samples: 200, loss: 347.827261\n",
      "iteration 70 / 8000, samples: 200, loss: 219.524843\n",
      "iteration 80 / 8000, samples: 200, loss: 246.094681\n",
      "iteration 90 / 8000, samples: 200, loss: 207.663587\n",
      "iteration 100 / 8000, samples: 200, loss: 202.247361\n",
      "iteration 110 / 8000, samples: 200, loss: 458.485661\n",
      "iteration 120 / 8000, samples: 200, loss: 371.438184\n",
      "iteration 130 / 8000, samples: 200, loss: 410.250262\n",
      "iteration 140 / 8000, samples: 200, loss: 208.647782\n",
      "iteration 150 / 8000, samples: 200, loss: 309.316653\n",
      "iteration 160 / 8000, samples: 200, loss: 244.007702\n",
      "iteration 170 / 8000, samples: 200, loss: 385.506135\n",
      "iteration 180 / 8000, samples: 200, loss: 315.488401\n",
      "iteration 190 / 8000, samples: 200, loss: 330.611827\n",
      "iteration 200 / 8000, samples: 200, loss: 237.370069\n",
      "iteration 210 / 8000, samples: 200, loss: 223.467505\n",
      "iteration 220 / 8000, samples: 200, loss: 228.357948\n",
      "iteration 230 / 8000, samples: 200, loss: 274.617573\n",
      "iteration 240 / 8000, samples: 200, loss: 240.142508\n",
      "iteration 250 / 8000, samples: 200, loss: 347.341382\n",
      "iteration 260 / 8000, samples: 200, loss: 301.005331\n",
      "iteration 270 / 8000, samples: 200, loss: 336.418683\n",
      "iteration 280 / 8000, samples: 200, loss: 245.528719\n",
      "iteration 290 / 8000, samples: 200, loss: 367.202440\n",
      "iteration 300 / 8000, samples: 200, loss: 216.475969\n",
      "iteration 310 / 8000, samples: 200, loss: 179.569908\n",
      "iteration 320 / 8000, samples: 200, loss: 199.978170\n",
      "iteration 330 / 8000, samples: 200, loss: 539.804426\n",
      "iteration 340 / 8000, samples: 200, loss: 221.898674\n",
      "iteration 350 / 8000, samples: 200, loss: 301.211666\n",
      "iteration 360 / 8000, samples: 200, loss: 268.573224\n",
      "iteration 370 / 8000, samples: 200, loss: 197.149428\n",
      "iteration 380 / 8000, samples: 200, loss: 475.158842\n",
      "iteration 390 / 8000, samples: 200, loss: 260.601048\n",
      "iteration 400 / 8000, samples: 200, loss: 323.196082\n",
      "iteration 410 / 8000, samples: 200, loss: 347.456186\n",
      "iteration 420 / 8000, samples: 200, loss: 270.723218\n",
      "iteration 430 / 8000, samples: 200, loss: 279.316326\n",
      "iteration 440 / 8000, samples: 200, loss: 240.293651\n",
      "iteration 450 / 8000, samples: 200, loss: 273.975636\n",
      "iteration 460 / 8000, samples: 200, loss: 208.358264\n",
      "iteration 470 / 8000, samples: 200, loss: 218.736362\n",
      "iteration 480 / 8000, samples: 200, loss: 302.142302\n",
      "iteration 490 / 8000, samples: 200, loss: 144.571964\n",
      "iteration 500 / 8000, samples: 200, loss: 300.455866\n",
      "iteration 510 / 8000, samples: 200, loss: 247.641134\n",
      "iteration 520 / 8000, samples: 200, loss: 300.445178\n",
      "iteration 530 / 8000, samples: 200, loss: 255.388676\n",
      "iteration 540 / 8000, samples: 200, loss: 198.501675\n",
      "iteration 550 / 8000, samples: 200, loss: 216.048518\n",
      "iteration 560 / 8000, samples: 200, loss: 219.268593\n",
      "iteration 570 / 8000, samples: 200, loss: 236.408771\n",
      "iteration 580 / 8000, samples: 200, loss: 321.672264\n",
      "iteration 590 / 8000, samples: 200, loss: 210.746667\n",
      "iteration 600 / 8000, samples: 200, loss: 346.399033\n",
      "iteration 610 / 8000, samples: 200, loss: 247.930336\n",
      "iteration 620 / 8000, samples: 200, loss: 390.755702\n",
      "iteration 630 / 8000, samples: 200, loss: 358.527905\n",
      "iteration 640 / 8000, samples: 200, loss: 240.712486\n",
      "iteration 650 / 8000, samples: 200, loss: 222.181899\n",
      "iteration 660 / 8000, samples: 200, loss: 205.972517\n",
      "iteration 670 / 8000, samples: 200, loss: 284.637700\n",
      "iteration 680 / 8000, samples: 200, loss: 213.209371\n",
      "iteration 690 / 8000, samples: 200, loss: 279.551658\n",
      "iteration 700 / 8000, samples: 200, loss: 254.450712\n",
      "iteration 710 / 8000, samples: 200, loss: 244.387425\n",
      "iteration 720 / 8000, samples: 200, loss: 206.641223\n",
      "iteration 730 / 8000, samples: 200, loss: 421.412511\n",
      "iteration 740 / 8000, samples: 200, loss: 245.118781\n",
      "iteration 750 / 8000, samples: 200, loss: 307.798119\n",
      "iteration 760 / 8000, samples: 200, loss: 359.920596\n",
      "iteration 770 / 8000, samples: 200, loss: 310.735180\n",
      "iteration 780 / 8000, samples: 200, loss: 370.041902\n",
      "iteration 790 / 8000, samples: 200, loss: 150.103565\n",
      "iteration 800 / 8000, samples: 200, loss: 218.247465\n",
      "iteration 810 / 8000, samples: 200, loss: 439.234320\n",
      "iteration 820 / 8000, samples: 200, loss: 268.331723\n",
      "iteration 830 / 8000, samples: 200, loss: 302.538304\n",
      "iteration 840 / 8000, samples: 200, loss: 350.646289\n",
      "iteration 850 / 8000, samples: 200, loss: 197.978091\n",
      "iteration 860 / 8000, samples: 200, loss: 209.926613\n",
      "iteration 870 / 8000, samples: 200, loss: 270.871357\n",
      "iteration 880 / 8000, samples: 200, loss: 320.036561\n",
      "iteration 890 / 8000, samples: 200, loss: 280.233669\n",
      "iteration 900 / 8000, samples: 200, loss: 407.678048\n",
      "iteration 910 / 8000, samples: 200, loss: 335.575184\n",
      "iteration 920 / 8000, samples: 200, loss: 255.688833\n",
      "iteration 930 / 8000, samples: 200, loss: 316.734310\n",
      "iteration 940 / 8000, samples: 200, loss: 201.766055\n",
      "iteration 950 / 8000, samples: 200, loss: 466.565485\n",
      "iteration 960 / 8000, samples: 200, loss: 316.093277\n",
      "iteration 970 / 8000, samples: 200, loss: 328.944724\n",
      "iteration 980 / 8000, samples: 200, loss: 247.470147\n",
      "iteration 990 / 8000, samples: 200, loss: 228.214591\n",
      "iteration 1000 / 8000, samples: 200, loss: 257.997039\n",
      "iteration 1010 / 8000, samples: 200, loss: 237.828014\n",
      "iteration 1020 / 8000, samples: 200, loss: 231.791962\n",
      "iteration 1030 / 8000, samples: 200, loss: 226.998858\n",
      "iteration 1040 / 8000, samples: 200, loss: 283.022293\n",
      "iteration 1050 / 8000, samples: 200, loss: 241.359592\n",
      "iteration 1060 / 8000, samples: 200, loss: 303.185015\n",
      "iteration 1070 / 8000, samples: 200, loss: 293.759396\n",
      "iteration 1080 / 8000, samples: 200, loss: 425.158370\n",
      "iteration 1090 / 8000, samples: 200, loss: 267.438041\n",
      "iteration 1100 / 8000, samples: 200, loss: 327.119800\n",
      "iteration 1110 / 8000, samples: 200, loss: 158.545574\n",
      "iteration 1120 / 8000, samples: 200, loss: 284.871895\n",
      "iteration 1130 / 8000, samples: 200, loss: 271.491231\n",
      "iteration 1140 / 8000, samples: 200, loss: 176.988945\n",
      "iteration 1150 / 8000, samples: 200, loss: 151.377924\n",
      "iteration 1160 / 8000, samples: 200, loss: 254.468651\n",
      "iteration 1170 / 8000, samples: 200, loss: 312.353662\n",
      "iteration 1180 / 8000, samples: 200, loss: 332.349859\n",
      "iteration 1190 / 8000, samples: 200, loss: 263.914547\n",
      "iteration 1200 / 8000, samples: 200, loss: 323.700724\n",
      "iteration 1210 / 8000, samples: 200, loss: 209.009299\n",
      "iteration 1220 / 8000, samples: 200, loss: 303.963497\n",
      "iteration 1230 / 8000, samples: 200, loss: 228.087292\n",
      "iteration 1240 / 8000, samples: 200, loss: 263.461605\n",
      "iteration 1250 / 8000, samples: 200, loss: 373.842622\n",
      "iteration 1260 / 8000, samples: 200, loss: 255.221957\n",
      "iteration 1270 / 8000, samples: 200, loss: 358.243629\n",
      "iteration 1280 / 8000, samples: 200, loss: 376.389068\n",
      "iteration 1290 / 8000, samples: 200, loss: 277.566617\n",
      "iteration 1300 / 8000, samples: 200, loss: 214.077441\n",
      "iteration 1310 / 8000, samples: 200, loss: 335.296031\n",
      "iteration 1320 / 8000, samples: 200, loss: 202.421702\n",
      "iteration 1330 / 8000, samples: 200, loss: 161.374502\n",
      "iteration 1340 / 8000, samples: 200, loss: 471.627548\n",
      "iteration 1350 / 8000, samples: 200, loss: 386.441778\n",
      "iteration 1360 / 8000, samples: 200, loss: 205.097714\n",
      "iteration 1370 / 8000, samples: 200, loss: 232.192066\n",
      "iteration 1380 / 8000, samples: 200, loss: 312.871915\n",
      "iteration 1390 / 8000, samples: 200, loss: 346.453807\n",
      "iteration 1400 / 8000, samples: 200, loss: 221.339069\n",
      "iteration 1410 / 8000, samples: 200, loss: 186.921550\n",
      "iteration 1420 / 8000, samples: 200, loss: 313.782740\n",
      "iteration 1430 / 8000, samples: 200, loss: 288.215752\n",
      "iteration 1440 / 8000, samples: 200, loss: 418.964127\n",
      "iteration 1450 / 8000, samples: 200, loss: 301.510265\n",
      "iteration 1460 / 8000, samples: 200, loss: 171.761332\n",
      "iteration 1470 / 8000, samples: 200, loss: 275.303317\n",
      "iteration 1480 / 8000, samples: 200, loss: 278.207060\n",
      "iteration 1490 / 8000, samples: 200, loss: 271.500288\n",
      "iteration 1500 / 8000, samples: 200, loss: 200.320808\n",
      "iteration 1510 / 8000, samples: 200, loss: 351.141777\n",
      "iteration 1520 / 8000, samples: 200, loss: 275.893598\n",
      "iteration 1530 / 8000, samples: 200, loss: 404.872202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1540 / 8000, samples: 200, loss: 298.460593\n",
      "iteration 1550 / 8000, samples: 200, loss: 328.075856\n",
      "iteration 1560 / 8000, samples: 200, loss: 275.338126\n",
      "iteration 1570 / 8000, samples: 200, loss: 266.436766\n",
      "iteration 1580 / 8000, samples: 200, loss: 198.095987\n",
      "iteration 1590 / 8000, samples: 200, loss: 284.163846\n",
      "iteration 1600 / 8000, samples: 200, loss: 249.349478\n",
      "iteration 1610 / 8000, samples: 200, loss: 167.297191\n",
      "iteration 1620 / 8000, samples: 200, loss: 261.833825\n",
      "iteration 1630 / 8000, samples: 200, loss: 252.410352\n",
      "iteration 1640 / 8000, samples: 200, loss: 205.213429\n",
      "iteration 1650 / 8000, samples: 200, loss: 163.968354\n",
      "iteration 1660 / 8000, samples: 200, loss: 261.044023\n",
      "iteration 1670 / 8000, samples: 200, loss: 309.419829\n",
      "iteration 1680 / 8000, samples: 200, loss: 216.753568\n",
      "iteration 1690 / 8000, samples: 200, loss: 256.301804\n",
      "iteration 1700 / 8000, samples: 200, loss: 247.468460\n",
      "iteration 1710 / 8000, samples: 200, loss: 240.908192\n",
      "iteration 1720 / 8000, samples: 200, loss: 362.585743\n",
      "iteration 1730 / 8000, samples: 200, loss: 259.788997\n",
      "iteration 1740 / 8000, samples: 200, loss: 209.432425\n",
      "iteration 1750 / 8000, samples: 200, loss: 259.690864\n",
      "iteration 1760 / 8000, samples: 200, loss: 291.737474\n",
      "iteration 1770 / 8000, samples: 200, loss: 181.685344\n",
      "iteration 1780 / 8000, samples: 200, loss: 331.893040\n",
      "iteration 1790 / 8000, samples: 200, loss: 164.928234\n",
      "iteration 1800 / 8000, samples: 200, loss: 230.871241\n",
      "iteration 1810 / 8000, samples: 200, loss: 224.725202\n",
      "iteration 1820 / 8000, samples: 200, loss: 340.310573\n",
      "iteration 1830 / 8000, samples: 200, loss: 198.997806\n",
      "iteration 1840 / 8000, samples: 200, loss: 331.161704\n",
      "iteration 1850 / 8000, samples: 200, loss: 253.941045\n",
      "iteration 1860 / 8000, samples: 200, loss: 208.955061\n",
      "iteration 1870 / 8000, samples: 200, loss: 250.099099\n",
      "iteration 1880 / 8000, samples: 200, loss: 157.798825\n",
      "iteration 1890 / 8000, samples: 200, loss: 310.838825\n",
      "iteration 1900 / 8000, samples: 200, loss: 295.773223\n",
      "iteration 1910 / 8000, samples: 200, loss: 198.917794\n",
      "iteration 1920 / 8000, samples: 200, loss: 368.634338\n",
      "iteration 1930 / 8000, samples: 200, loss: 232.562887\n",
      "iteration 1940 / 8000, samples: 200, loss: 249.190939\n",
      "iteration 1950 / 8000, samples: 200, loss: 311.727848\n",
      "iteration 1960 / 8000, samples: 200, loss: 309.856373\n",
      "iteration 1970 / 8000, samples: 200, loss: 211.962364\n",
      "iteration 1980 / 8000, samples: 200, loss: 240.931888\n",
      "iteration 1990 / 8000, samples: 200, loss: 270.798497\n",
      "iteration 2000 / 8000, samples: 200, loss: 335.572426\n",
      "iteration 2010 / 8000, samples: 200, loss: 149.022285\n",
      "iteration 2020 / 8000, samples: 200, loss: 192.621770\n",
      "iteration 2030 / 8000, samples: 200, loss: 200.426288\n",
      "iteration 2040 / 8000, samples: 200, loss: 269.599525\n",
      "iteration 2050 / 8000, samples: 200, loss: 212.995492\n",
      "iteration 2060 / 8000, samples: 200, loss: 254.321479\n",
      "iteration 2070 / 8000, samples: 200, loss: 235.470585\n",
      "iteration 2080 / 8000, samples: 200, loss: 315.200152\n",
      "iteration 2090 / 8000, samples: 200, loss: 295.985717\n",
      "iteration 2100 / 8000, samples: 200, loss: 291.953309\n",
      "iteration 2110 / 8000, samples: 200, loss: 220.367735\n",
      "iteration 2120 / 8000, samples: 200, loss: 167.534957\n",
      "iteration 2130 / 8000, samples: 200, loss: 221.468157\n",
      "iteration 2140 / 8000, samples: 200, loss: 269.002429\n",
      "iteration 2150 / 8000, samples: 200, loss: 368.477502\n",
      "iteration 2160 / 8000, samples: 200, loss: 277.429892\n",
      "iteration 2170 / 8000, samples: 200, loss: 217.127268\n",
      "iteration 2180 / 8000, samples: 200, loss: 233.577311\n",
      "iteration 2190 / 8000, samples: 200, loss: 267.317422\n",
      "iteration 2200 / 8000, samples: 200, loss: 302.258309\n",
      "iteration 2210 / 8000, samples: 200, loss: 429.941476\n",
      "iteration 2220 / 8000, samples: 200, loss: 237.323145\n",
      "iteration 2230 / 8000, samples: 200, loss: 249.469229\n",
      "iteration 2240 / 8000, samples: 200, loss: 179.503270\n",
      "iteration 2250 / 8000, samples: 200, loss: 366.451785\n",
      "iteration 2260 / 8000, samples: 200, loss: 231.965844\n",
      "iteration 2270 / 8000, samples: 200, loss: 309.475659\n",
      "iteration 2280 / 8000, samples: 200, loss: 302.368576\n",
      "iteration 2290 / 8000, samples: 200, loss: 445.256192\n",
      "iteration 2300 / 8000, samples: 200, loss: 209.177739\n",
      "iteration 2310 / 8000, samples: 200, loss: 136.391920\n",
      "iteration 2320 / 8000, samples: 200, loss: 334.796097\n",
      "iteration 2330 / 8000, samples: 200, loss: 355.038662\n",
      "iteration 2340 / 8000, samples: 200, loss: 292.129934\n",
      "iteration 2350 / 8000, samples: 200, loss: 187.666417\n",
      "iteration 2360 / 8000, samples: 200, loss: 270.909505\n",
      "iteration 2370 / 8000, samples: 200, loss: 202.835175\n",
      "iteration 2380 / 8000, samples: 200, loss: 205.448654\n",
      "iteration 2390 / 8000, samples: 200, loss: 224.383120\n",
      "iteration 2400 / 8000, samples: 200, loss: 241.665717\n",
      "iteration 2410 / 8000, samples: 200, loss: 266.780509\n",
      "iteration 2420 / 8000, samples: 200, loss: 218.326547\n",
      "iteration 2430 / 8000, samples: 200, loss: 190.221135\n",
      "iteration 2440 / 8000, samples: 200, loss: 326.965778\n",
      "iteration 2450 / 8000, samples: 200, loss: 277.029285\n",
      "iteration 2460 / 8000, samples: 200, loss: 252.073372\n",
      "iteration 2470 / 8000, samples: 200, loss: 285.888797\n",
      "iteration 2480 / 8000, samples: 200, loss: 220.876767\n",
      "iteration 2490 / 8000, samples: 200, loss: 251.372107\n",
      "iteration 2500 / 8000, samples: 200, loss: 614.553686\n",
      "iteration 2510 / 8000, samples: 200, loss: 294.563870\n",
      "iteration 2520 / 8000, samples: 200, loss: 340.697830\n",
      "iteration 2530 / 8000, samples: 200, loss: 435.340475\n",
      "iteration 2540 / 8000, samples: 200, loss: 226.835456\n",
      "iteration 2550 / 8000, samples: 200, loss: 422.961948\n",
      "iteration 2560 / 8000, samples: 200, loss: 392.384072\n",
      "iteration 2570 / 8000, samples: 200, loss: 273.191871\n",
      "iteration 2580 / 8000, samples: 200, loss: 393.286627\n",
      "iteration 2590 / 8000, samples: 200, loss: 204.074225\n",
      "iteration 2600 / 8000, samples: 200, loss: 302.730377\n",
      "iteration 2610 / 8000, samples: 200, loss: 360.107573\n",
      "iteration 2620 / 8000, samples: 200, loss: 193.568134\n",
      "iteration 2630 / 8000, samples: 200, loss: 225.881789\n",
      "iteration 2640 / 8000, samples: 200, loss: 222.213665\n",
      "iteration 2650 / 8000, samples: 200, loss: 233.785862\n",
      "iteration 2660 / 8000, samples: 200, loss: 297.315017\n",
      "iteration 2670 / 8000, samples: 200, loss: 355.196895\n",
      "iteration 2680 / 8000, samples: 200, loss: 244.963845\n",
      "iteration 2690 / 8000, samples: 200, loss: 278.288166\n",
      "iteration 2700 / 8000, samples: 200, loss: 286.287915\n",
      "iteration 2710 / 8000, samples: 200, loss: 264.296427\n",
      "iteration 2720 / 8000, samples: 200, loss: 231.829377\n",
      "iteration 2730 / 8000, samples: 200, loss: 305.093922\n",
      "iteration 2740 / 8000, samples: 200, loss: 231.256229\n",
      "iteration 2750 / 8000, samples: 200, loss: 181.617018\n",
      "iteration 2760 / 8000, samples: 200, loss: 260.677147\n",
      "iteration 2770 / 8000, samples: 200, loss: 277.014766\n",
      "iteration 2780 / 8000, samples: 200, loss: 260.764827\n",
      "iteration 2790 / 8000, samples: 200, loss: 218.899061\n",
      "iteration 2800 / 8000, samples: 200, loss: 243.981189\n",
      "iteration 2810 / 8000, samples: 200, loss: 263.904954\n",
      "iteration 2820 / 8000, samples: 200, loss: 222.337016\n",
      "iteration 2830 / 8000, samples: 200, loss: 218.789541\n",
      "iteration 2840 / 8000, samples: 200, loss: 207.151022\n",
      "iteration 2850 / 8000, samples: 200, loss: 260.537821\n",
      "iteration 2860 / 8000, samples: 200, loss: 449.188591\n",
      "iteration 2870 / 8000, samples: 200, loss: 448.174433\n",
      "iteration 2880 / 8000, samples: 200, loss: 292.904255\n",
      "iteration 2890 / 8000, samples: 200, loss: 166.311665\n",
      "iteration 2900 / 8000, samples: 200, loss: 404.675521\n",
      "iteration 2910 / 8000, samples: 200, loss: 271.972706\n",
      "iteration 2920 / 8000, samples: 200, loss: 258.331585\n",
      "iteration 2930 / 8000, samples: 200, loss: 188.559350\n",
      "iteration 2940 / 8000, samples: 200, loss: 229.039843\n",
      "iteration 2950 / 8000, samples: 200, loss: 351.251434\n",
      "iteration 2960 / 8000, samples: 200, loss: 200.171959\n",
      "iteration 2970 / 8000, samples: 200, loss: 233.840826\n",
      "iteration 2980 / 8000, samples: 200, loss: 241.017260\n",
      "iteration 2990 / 8000, samples: 200, loss: 273.978648\n",
      "iteration 3000 / 8000, samples: 200, loss: 281.643170\n",
      "iteration 3010 / 8000, samples: 200, loss: 298.965506\n",
      "iteration 3020 / 8000, samples: 200, loss: 333.975815\n",
      "iteration 3030 / 8000, samples: 200, loss: 234.889321\n",
      "iteration 3040 / 8000, samples: 200, loss: 228.347546\n",
      "iteration 3050 / 8000, samples: 200, loss: 300.697792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3060 / 8000, samples: 200, loss: 295.804183\n",
      "iteration 3070 / 8000, samples: 200, loss: 288.025025\n",
      "iteration 3080 / 8000, samples: 200, loss: 333.325739\n",
      "iteration 3090 / 8000, samples: 200, loss: 235.480404\n",
      "iteration 3100 / 8000, samples: 200, loss: 369.310442\n",
      "iteration 3110 / 8000, samples: 200, loss: 171.047803\n",
      "iteration 3120 / 8000, samples: 200, loss: 250.931389\n",
      "iteration 3130 / 8000, samples: 200, loss: 200.007943\n",
      "iteration 3140 / 8000, samples: 200, loss: 171.208628\n",
      "iteration 3150 / 8000, samples: 200, loss: 241.952438\n",
      "iteration 3160 / 8000, samples: 200, loss: 299.817491\n",
      "iteration 3170 / 8000, samples: 200, loss: 268.896882\n",
      "iteration 3180 / 8000, samples: 200, loss: 434.361476\n",
      "iteration 3190 / 8000, samples: 200, loss: 346.173013\n",
      "iteration 3200 / 8000, samples: 200, loss: 234.682644\n",
      "iteration 3210 / 8000, samples: 200, loss: 336.351911\n",
      "iteration 3220 / 8000, samples: 200, loss: 261.670538\n",
      "iteration 3230 / 8000, samples: 200, loss: 331.402435\n",
      "iteration 3240 / 8000, samples: 200, loss: 249.581551\n",
      "iteration 3250 / 8000, samples: 200, loss: 265.055204\n",
      "iteration 3260 / 8000, samples: 200, loss: 309.713646\n",
      "iteration 3270 / 8000, samples: 200, loss: 301.862288\n",
      "iteration 3280 / 8000, samples: 200, loss: 283.457692\n",
      "iteration 3290 / 8000, samples: 200, loss: 272.629692\n",
      "iteration 3300 / 8000, samples: 200, loss: 220.914687\n",
      "iteration 3310 / 8000, samples: 200, loss: 259.439124\n",
      "iteration 3320 / 8000, samples: 200, loss: 267.036965\n",
      "iteration 3330 / 8000, samples: 200, loss: 198.534365\n",
      "iteration 3340 / 8000, samples: 200, loss: 287.046810\n",
      "iteration 3350 / 8000, samples: 200, loss: 164.517859\n",
      "iteration 3360 / 8000, samples: 200, loss: 189.633004\n",
      "iteration 3370 / 8000, samples: 200, loss: 204.696909\n",
      "iteration 3380 / 8000, samples: 200, loss: 254.046714\n",
      "iteration 3390 / 8000, samples: 200, loss: 192.139933\n",
      "iteration 3400 / 8000, samples: 200, loss: 165.946866\n",
      "iteration 3410 / 8000, samples: 200, loss: 200.945362\n",
      "iteration 3420 / 8000, samples: 200, loss: 243.691569\n",
      "iteration 3430 / 8000, samples: 200, loss: 352.910088\n",
      "iteration 3440 / 8000, samples: 200, loss: 341.174890\n",
      "iteration 3450 / 8000, samples: 200, loss: 376.049259\n",
      "iteration 3460 / 8000, samples: 200, loss: 232.007444\n",
      "iteration 3470 / 8000, samples: 200, loss: 342.383697\n",
      "iteration 3480 / 8000, samples: 200, loss: 328.795304\n",
      "iteration 3490 / 8000, samples: 200, loss: 223.796060\n",
      "iteration 3500 / 8000, samples: 200, loss: 408.075792\n",
      "iteration 3510 / 8000, samples: 200, loss: 277.834242\n",
      "iteration 3520 / 8000, samples: 200, loss: 373.105352\n",
      "iteration 3530 / 8000, samples: 200, loss: 301.244017\n",
      "iteration 3540 / 8000, samples: 200, loss: 215.501210\n",
      "iteration 3550 / 8000, samples: 200, loss: 230.170014\n",
      "iteration 3560 / 8000, samples: 200, loss: 394.310333\n",
      "iteration 3570 / 8000, samples: 200, loss: 190.527084\n",
      "iteration 3580 / 8000, samples: 200, loss: 269.824600\n",
      "iteration 3590 / 8000, samples: 200, loss: 345.964820\n",
      "iteration 3600 / 8000, samples: 200, loss: 309.342729\n",
      "iteration 3610 / 8000, samples: 200, loss: 287.715937\n",
      "iteration 3620 / 8000, samples: 200, loss: 304.912918\n",
      "iteration 3630 / 8000, samples: 200, loss: 311.771712\n",
      "iteration 3640 / 8000, samples: 200, loss: 225.611710\n",
      "iteration 3650 / 8000, samples: 200, loss: 352.740887\n",
      "iteration 3660 / 8000, samples: 200, loss: 258.420188\n",
      "iteration 3670 / 8000, samples: 200, loss: 165.348946\n",
      "iteration 3680 / 8000, samples: 200, loss: 213.502067\n",
      "iteration 3690 / 8000, samples: 200, loss: 288.907981\n",
      "iteration 3700 / 8000, samples: 200, loss: 385.523882\n",
      "iteration 3710 / 8000, samples: 200, loss: 241.171438\n",
      "iteration 3720 / 8000, samples: 200, loss: 168.178187\n",
      "iteration 3730 / 8000, samples: 200, loss: 183.052186\n",
      "iteration 3740 / 8000, samples: 200, loss: 496.953785\n",
      "iteration 3750 / 8000, samples: 200, loss: 276.412651\n",
      "iteration 3760 / 8000, samples: 200, loss: 284.840204\n",
      "iteration 3770 / 8000, samples: 200, loss: 353.730985\n",
      "iteration 3780 / 8000, samples: 200, loss: 391.030422\n",
      "iteration 3790 / 8000, samples: 200, loss: 394.513958\n",
      "iteration 3800 / 8000, samples: 200, loss: 206.647211\n",
      "iteration 3810 / 8000, samples: 200, loss: 258.980434\n",
      "iteration 3820 / 8000, samples: 200, loss: 277.540144\n",
      "iteration 3830 / 8000, samples: 200, loss: 253.237396\n",
      "iteration 3840 / 8000, samples: 200, loss: 135.182440\n",
      "iteration 3850 / 8000, samples: 200, loss: 374.508274\n",
      "iteration 3860 / 8000, samples: 200, loss: 215.420852\n",
      "iteration 3870 / 8000, samples: 200, loss: 326.724045\n",
      "iteration 3880 / 8000, samples: 200, loss: 344.481334\n",
      "iteration 3890 / 8000, samples: 200, loss: 232.817476\n",
      "iteration 3900 / 8000, samples: 200, loss: 288.103437\n",
      "iteration 3910 / 8000, samples: 200, loss: 417.366052\n",
      "iteration 3920 / 8000, samples: 200, loss: 324.246686\n",
      "iteration 3930 / 8000, samples: 200, loss: 182.868302\n",
      "iteration 3940 / 8000, samples: 200, loss: 193.025957\n",
      "iteration 3950 / 8000, samples: 200, loss: 325.710132\n",
      "iteration 3960 / 8000, samples: 200, loss: 262.468742\n",
      "iteration 3970 / 8000, samples: 200, loss: 355.168254\n",
      "iteration 3980 / 8000, samples: 200, loss: 251.044475\n",
      "iteration 3990 / 8000, samples: 200, loss: 358.232238\n",
      "iteration 4000 / 8000, samples: 200, loss: 177.942214\n",
      "iteration 4010 / 8000, samples: 200, loss: 204.664572\n",
      "iteration 4020 / 8000, samples: 200, loss: 250.370579\n",
      "iteration 4030 / 8000, samples: 200, loss: 205.465056\n",
      "iteration 4040 / 8000, samples: 200, loss: 302.234920\n",
      "iteration 4050 / 8000, samples: 200, loss: 199.936970\n",
      "iteration 4060 / 8000, samples: 200, loss: 317.312687\n",
      "iteration 4070 / 8000, samples: 200, loss: 303.929199\n",
      "iteration 4080 / 8000, samples: 200, loss: 334.208919\n",
      "iteration 4090 / 8000, samples: 200, loss: 249.683963\n",
      "iteration 4100 / 8000, samples: 200, loss: 272.752235\n",
      "iteration 4110 / 8000, samples: 200, loss: 219.043469\n",
      "iteration 4120 / 8000, samples: 200, loss: 176.357980\n",
      "iteration 4130 / 8000, samples: 200, loss: 332.956019\n",
      "iteration 4140 / 8000, samples: 200, loss: 219.475573\n",
      "iteration 4150 / 8000, samples: 200, loss: 240.427972\n",
      "iteration 4160 / 8000, samples: 200, loss: 215.478433\n",
      "iteration 4170 / 8000, samples: 200, loss: 307.246220\n",
      "iteration 4180 / 8000, samples: 200, loss: 302.270765\n",
      "iteration 4190 / 8000, samples: 200, loss: 336.357547\n",
      "iteration 4200 / 8000, samples: 200, loss: 364.354197\n",
      "iteration 4210 / 8000, samples: 200, loss: 267.781433\n",
      "iteration 4220 / 8000, samples: 200, loss: 221.091310\n",
      "iteration 4230 / 8000, samples: 200, loss: 245.218439\n",
      "iteration 4240 / 8000, samples: 200, loss: 344.800162\n",
      "iteration 4250 / 8000, samples: 200, loss: 336.937432\n",
      "iteration 4260 / 8000, samples: 200, loss: 243.057750\n",
      "iteration 4270 / 8000, samples: 200, loss: 449.485652\n",
      "iteration 4280 / 8000, samples: 200, loss: 320.045572\n",
      "iteration 4290 / 8000, samples: 200, loss: 245.976985\n",
      "iteration 4300 / 8000, samples: 200, loss: 312.772781\n",
      "iteration 4310 / 8000, samples: 200, loss: 285.852986\n",
      "iteration 4320 / 8000, samples: 200, loss: 271.671120\n",
      "iteration 4330 / 8000, samples: 200, loss: 199.197190\n",
      "iteration 4340 / 8000, samples: 200, loss: 299.359317\n",
      "iteration 4350 / 8000, samples: 200, loss: 319.700168\n",
      "iteration 4360 / 8000, samples: 200, loss: 402.074798\n",
      "iteration 4370 / 8000, samples: 200, loss: 270.265358\n",
      "iteration 4380 / 8000, samples: 200, loss: 283.972728\n",
      "iteration 4390 / 8000, samples: 200, loss: 302.700152\n",
      "iteration 4400 / 8000, samples: 200, loss: 281.525413\n",
      "iteration 4410 / 8000, samples: 200, loss: 451.887661\n",
      "iteration 4420 / 8000, samples: 200, loss: 222.636956\n",
      "iteration 4430 / 8000, samples: 200, loss: 232.414751\n",
      "iteration 4440 / 8000, samples: 200, loss: 233.771393\n",
      "iteration 4450 / 8000, samples: 200, loss: 296.511173\n",
      "iteration 4460 / 8000, samples: 200, loss: 224.064645\n",
      "iteration 4470 / 8000, samples: 200, loss: 309.300951\n",
      "iteration 4480 / 8000, samples: 200, loss: 324.944206\n",
      "iteration 4490 / 8000, samples: 200, loss: 240.146482\n",
      "iteration 4500 / 8000, samples: 200, loss: 289.560356\n",
      "iteration 4510 / 8000, samples: 200, loss: 193.657708\n",
      "iteration 4520 / 8000, samples: 200, loss: 383.060224\n",
      "iteration 4530 / 8000, samples: 200, loss: 205.942409\n",
      "iteration 4540 / 8000, samples: 200, loss: 247.979461\n",
      "iteration 4550 / 8000, samples: 200, loss: 274.308989\n",
      "iteration 4560 / 8000, samples: 200, loss: 335.930624\n",
      "iteration 4570 / 8000, samples: 200, loss: 157.826088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4580 / 8000, samples: 200, loss: 368.821647\n",
      "iteration 4590 / 8000, samples: 200, loss: 331.595567\n",
      "iteration 4600 / 8000, samples: 200, loss: 225.268194\n",
      "iteration 4610 / 8000, samples: 200, loss: 318.093934\n",
      "iteration 4620 / 8000, samples: 200, loss: 320.204011\n",
      "iteration 4630 / 8000, samples: 200, loss: 159.225722\n",
      "iteration 4640 / 8000, samples: 200, loss: 395.447617\n",
      "iteration 4650 / 8000, samples: 200, loss: 196.667904\n",
      "iteration 4660 / 8000, samples: 200, loss: 348.125749\n",
      "iteration 4670 / 8000, samples: 200, loss: 228.300768\n",
      "iteration 4680 / 8000, samples: 200, loss: 246.600053\n",
      "iteration 4690 / 8000, samples: 200, loss: 350.444427\n",
      "iteration 4700 / 8000, samples: 200, loss: 205.410756\n",
      "iteration 4710 / 8000, samples: 200, loss: 289.521489\n",
      "iteration 4720 / 8000, samples: 200, loss: 273.585934\n",
      "iteration 4730 / 8000, samples: 200, loss: 262.521636\n",
      "iteration 4740 / 8000, samples: 200, loss: 220.736580\n",
      "iteration 4750 / 8000, samples: 200, loss: 193.995148\n",
      "iteration 4760 / 8000, samples: 200, loss: 252.895775\n",
      "iteration 4770 / 8000, samples: 200, loss: 174.024056\n",
      "iteration 4780 / 8000, samples: 200, loss: 253.723259\n",
      "iteration 4790 / 8000, samples: 200, loss: 251.175546\n",
      "iteration 4800 / 8000, samples: 200, loss: 309.826970\n",
      "iteration 4810 / 8000, samples: 200, loss: 244.851797\n",
      "iteration 4820 / 8000, samples: 200, loss: 248.841869\n",
      "iteration 4830 / 8000, samples: 200, loss: 214.790343\n",
      "iteration 4840 / 8000, samples: 200, loss: 235.231808\n",
      "iteration 4850 / 8000, samples: 200, loss: 229.159516\n",
      "iteration 4860 / 8000, samples: 200, loss: 311.680552\n",
      "iteration 4870 / 8000, samples: 200, loss: 361.000866\n",
      "iteration 4880 / 8000, samples: 200, loss: 252.325458\n",
      "iteration 4890 / 8000, samples: 200, loss: 308.843496\n",
      "iteration 4900 / 8000, samples: 200, loss: 330.832080\n",
      "iteration 4910 / 8000, samples: 200, loss: 218.083456\n",
      "iteration 4920 / 8000, samples: 200, loss: 180.283260\n",
      "iteration 4930 / 8000, samples: 200, loss: 246.920895\n",
      "iteration 4940 / 8000, samples: 200, loss: 245.005898\n",
      "iteration 4950 / 8000, samples: 200, loss: 158.319917\n",
      "iteration 4960 / 8000, samples: 200, loss: 164.976636\n",
      "iteration 4970 / 8000, samples: 200, loss: 143.249004\n",
      "iteration 4980 / 8000, samples: 200, loss: 314.525381\n",
      "iteration 4990 / 8000, samples: 200, loss: 308.355825\n",
      "iteration 5000 / 8000, samples: 200, loss: 221.553055\n",
      "iteration 5010 / 8000, samples: 200, loss: 360.797523\n",
      "iteration 5020 / 8000, samples: 200, loss: 280.912838\n",
      "iteration 5030 / 8000, samples: 200, loss: 210.543549\n",
      "iteration 5040 / 8000, samples: 200, loss: 246.501007\n",
      "iteration 5050 / 8000, samples: 200, loss: 380.068367\n",
      "iteration 5060 / 8000, samples: 200, loss: 197.671411\n",
      "iteration 5070 / 8000, samples: 200, loss: 274.395061\n",
      "iteration 5080 / 8000, samples: 200, loss: 205.019611\n",
      "iteration 5090 / 8000, samples: 200, loss: 220.030304\n",
      "iteration 5100 / 8000, samples: 200, loss: 186.451824\n",
      "iteration 5110 / 8000, samples: 200, loss: 274.684715\n",
      "iteration 5120 / 8000, samples: 200, loss: 301.490504\n",
      "iteration 5130 / 8000, samples: 200, loss: 204.348158\n",
      "iteration 5140 / 8000, samples: 200, loss: 227.319472\n",
      "iteration 5150 / 8000, samples: 200, loss: 372.883107\n",
      "iteration 5160 / 8000, samples: 200, loss: 237.726886\n",
      "iteration 5170 / 8000, samples: 200, loss: 182.798482\n",
      "iteration 5180 / 8000, samples: 200, loss: 267.305211\n",
      "iteration 5190 / 8000, samples: 200, loss: 246.528580\n",
      "iteration 5200 / 8000, samples: 200, loss: 300.445414\n",
      "iteration 5210 / 8000, samples: 200, loss: 229.424237\n",
      "iteration 5220 / 8000, samples: 200, loss: 296.436815\n",
      "iteration 5230 / 8000, samples: 200, loss: 329.090841\n",
      "iteration 5240 / 8000, samples: 200, loss: 186.284341\n",
      "iteration 5250 / 8000, samples: 200, loss: 256.532877\n",
      "iteration 5260 / 8000, samples: 200, loss: 357.428012\n",
      "iteration 5270 / 8000, samples: 200, loss: 334.087585\n",
      "iteration 5280 / 8000, samples: 200, loss: 207.387023\n",
      "iteration 5290 / 8000, samples: 200, loss: 250.790792\n",
      "iteration 5300 / 8000, samples: 200, loss: 276.056449\n",
      "iteration 5310 / 8000, samples: 200, loss: 313.233263\n",
      "iteration 5320 / 8000, samples: 200, loss: 249.527879\n",
      "iteration 5330 / 8000, samples: 200, loss: 359.030653\n",
      "iteration 5340 / 8000, samples: 200, loss: 405.421705\n",
      "iteration 5350 / 8000, samples: 200, loss: 225.066571\n",
      "iteration 5360 / 8000, samples: 200, loss: 341.826721\n",
      "iteration 5370 / 8000, samples: 200, loss: 248.517900\n",
      "iteration 5380 / 8000, samples: 200, loss: 280.405492\n",
      "iteration 5390 / 8000, samples: 200, loss: 280.007366\n",
      "iteration 5400 / 8000, samples: 200, loss: 177.302421\n",
      "iteration 5410 / 8000, samples: 200, loss: 224.574110\n",
      "iteration 5420 / 8000, samples: 200, loss: 254.327691\n",
      "iteration 5430 / 8000, samples: 200, loss: 234.126798\n",
      "iteration 5440 / 8000, samples: 200, loss: 200.367029\n",
      "iteration 5450 / 8000, samples: 200, loss: 266.849671\n",
      "iteration 5460 / 8000, samples: 200, loss: 188.188544\n",
      "iteration 5470 / 8000, samples: 200, loss: 234.677060\n",
      "iteration 5480 / 8000, samples: 200, loss: 371.708315\n",
      "iteration 5490 / 8000, samples: 200, loss: 199.334650\n",
      "iteration 5500 / 8000, samples: 200, loss: 202.525354\n",
      "iteration 5510 / 8000, samples: 200, loss: 383.894440\n",
      "iteration 5520 / 8000, samples: 200, loss: 312.131247\n",
      "iteration 5530 / 8000, samples: 200, loss: 272.741835\n",
      "iteration 5540 / 8000, samples: 200, loss: 222.338981\n",
      "iteration 5550 / 8000, samples: 200, loss: 382.269603\n",
      "iteration 5560 / 8000, samples: 200, loss: 417.204680\n",
      "iteration 5570 / 8000, samples: 200, loss: 201.600504\n",
      "iteration 5580 / 8000, samples: 200, loss: 456.470094\n",
      "iteration 5590 / 8000, samples: 200, loss: 295.857069\n",
      "iteration 5600 / 8000, samples: 200, loss: 220.800530\n",
      "iteration 5610 / 8000, samples: 200, loss: 267.180093\n",
      "iteration 5620 / 8000, samples: 200, loss: 265.529395\n",
      "iteration 5630 / 8000, samples: 200, loss: 244.826614\n",
      "iteration 5640 / 8000, samples: 200, loss: 387.232432\n",
      "iteration 5650 / 8000, samples: 200, loss: 356.762457\n",
      "iteration 5660 / 8000, samples: 200, loss: 265.676389\n",
      "iteration 5670 / 8000, samples: 200, loss: 183.657119\n",
      "iteration 5680 / 8000, samples: 200, loss: 292.806028\n",
      "iteration 5690 / 8000, samples: 200, loss: 287.890631\n",
      "iteration 5700 / 8000, samples: 200, loss: 282.524756\n",
      "iteration 5710 / 8000, samples: 200, loss: 294.440891\n",
      "iteration 5720 / 8000, samples: 200, loss: 363.965616\n",
      "iteration 5730 / 8000, samples: 200, loss: 228.943160\n",
      "iteration 5740 / 8000, samples: 200, loss: 322.659020\n",
      "iteration 5750 / 8000, samples: 200, loss: 279.164823\n",
      "iteration 5760 / 8000, samples: 200, loss: 208.466832\n",
      "iteration 5770 / 8000, samples: 200, loss: 359.006249\n",
      "iteration 5780 / 8000, samples: 200, loss: 243.534607\n",
      "iteration 5790 / 8000, samples: 200, loss: 218.263005\n",
      "iteration 5800 / 8000, samples: 200, loss: 298.491729\n",
      "iteration 5810 / 8000, samples: 200, loss: 268.297734\n",
      "iteration 5820 / 8000, samples: 200, loss: 234.523010\n",
      "iteration 5830 / 8000, samples: 200, loss: 173.993113\n",
      "iteration 5840 / 8000, samples: 200, loss: 235.814722\n",
      "iteration 5850 / 8000, samples: 200, loss: 216.123489\n",
      "iteration 5860 / 8000, samples: 200, loss: 247.482673\n",
      "iteration 5870 / 8000, samples: 200, loss: 306.903155\n",
      "iteration 5880 / 8000, samples: 200, loss: 199.579102\n",
      "iteration 5890 / 8000, samples: 200, loss: 167.685121\n",
      "iteration 5900 / 8000, samples: 200, loss: 316.652389\n",
      "iteration 5910 / 8000, samples: 200, loss: 266.817311\n",
      "iteration 5920 / 8000, samples: 200, loss: 344.579718\n",
      "iteration 5930 / 8000, samples: 200, loss: 247.734506\n",
      "iteration 5940 / 8000, samples: 200, loss: 203.428623\n",
      "iteration 5950 / 8000, samples: 200, loss: 220.970772\n",
      "iteration 5960 / 8000, samples: 200, loss: 176.038523\n",
      "iteration 5970 / 8000, samples: 200, loss: 404.262103\n",
      "iteration 5980 / 8000, samples: 200, loss: 191.293801\n",
      "iteration 5990 / 8000, samples: 200, loss: 214.818718\n",
      "iteration 6000 / 8000, samples: 200, loss: 203.870182\n",
      "iteration 6010 / 8000, samples: 200, loss: 356.939798\n",
      "iteration 6020 / 8000, samples: 200, loss: 191.783566\n",
      "iteration 6030 / 8000, samples: 200, loss: 188.061941\n",
      "iteration 6040 / 8000, samples: 200, loss: 200.212275\n",
      "iteration 6050 / 8000, samples: 200, loss: 253.951428\n",
      "iteration 6060 / 8000, samples: 200, loss: 201.283746\n",
      "iteration 6070 / 8000, samples: 200, loss: 322.939112\n",
      "iteration 6080 / 8000, samples: 200, loss: 303.534524\n",
      "iteration 6090 / 8000, samples: 200, loss: 402.879293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6100 / 8000, samples: 200, loss: 179.317263\n",
      "iteration 6110 / 8000, samples: 200, loss: 196.765871\n",
      "iteration 6120 / 8000, samples: 200, loss: 226.735057\n",
      "iteration 6130 / 8000, samples: 200, loss: 257.558722\n",
      "iteration 6140 / 8000, samples: 200, loss: 307.166607\n",
      "iteration 6150 / 8000, samples: 200, loss: 299.458398\n",
      "iteration 6160 / 8000, samples: 200, loss: 225.930741\n",
      "iteration 6170 / 8000, samples: 200, loss: 304.216165\n",
      "iteration 6180 / 8000, samples: 200, loss: 180.769063\n",
      "iteration 6190 / 8000, samples: 200, loss: 255.344486\n",
      "iteration 6200 / 8000, samples: 200, loss: 206.285694\n",
      "iteration 6210 / 8000, samples: 200, loss: 199.008152\n",
      "iteration 6220 / 8000, samples: 200, loss: 264.356083\n",
      "iteration 6230 / 8000, samples: 200, loss: 219.534037\n",
      "iteration 6240 / 8000, samples: 200, loss: 359.111552\n",
      "iteration 6250 / 8000, samples: 200, loss: 203.621596\n",
      "iteration 6260 / 8000, samples: 200, loss: 287.603423\n",
      "iteration 6270 / 8000, samples: 200, loss: 377.285775\n",
      "iteration 6280 / 8000, samples: 200, loss: 160.438664\n",
      "iteration 6290 / 8000, samples: 200, loss: 431.917902\n",
      "iteration 6300 / 8000, samples: 200, loss: 240.528268\n",
      "iteration 6310 / 8000, samples: 200, loss: 256.394573\n",
      "iteration 6320 / 8000, samples: 200, loss: 224.788463\n",
      "iteration 6330 / 8000, samples: 200, loss: 217.749116\n",
      "iteration 6340 / 8000, samples: 200, loss: 272.266879\n",
      "iteration 6350 / 8000, samples: 200, loss: 204.553212\n",
      "iteration 6360 / 8000, samples: 200, loss: 323.554816\n",
      "iteration 6370 / 8000, samples: 200, loss: 379.305501\n",
      "iteration 6380 / 8000, samples: 200, loss: 364.812101\n",
      "iteration 6390 / 8000, samples: 200, loss: 297.072253\n",
      "iteration 6400 / 8000, samples: 200, loss: 158.886813\n",
      "iteration 6410 / 8000, samples: 200, loss: 309.578058\n",
      "iteration 6420 / 8000, samples: 200, loss: 267.150630\n",
      "iteration 6430 / 8000, samples: 200, loss: 188.404068\n",
      "iteration 6440 / 8000, samples: 200, loss: 201.103815\n",
      "iteration 6450 / 8000, samples: 200, loss: 224.447752\n",
      "iteration 6460 / 8000, samples: 200, loss: 167.583374\n",
      "iteration 6470 / 8000, samples: 200, loss: 242.002882\n",
      "iteration 6480 / 8000, samples: 200, loss: 227.684100\n",
      "iteration 6490 / 8000, samples: 200, loss: 177.573608\n",
      "iteration 6500 / 8000, samples: 200, loss: 193.849424\n",
      "iteration 6510 / 8000, samples: 200, loss: 213.581485\n",
      "iteration 6520 / 8000, samples: 200, loss: 147.759651\n",
      "iteration 6530 / 8000, samples: 200, loss: 261.024799\n",
      "iteration 6540 / 8000, samples: 200, loss: 212.738335\n",
      "iteration 6550 / 8000, samples: 200, loss: 218.510843\n",
      "iteration 6560 / 8000, samples: 200, loss: 328.760666\n",
      "iteration 6570 / 8000, samples: 200, loss: 343.224710\n",
      "iteration 6580 / 8000, samples: 200, loss: 183.869059\n",
      "iteration 6590 / 8000, samples: 200, loss: 333.514055\n",
      "iteration 6600 / 8000, samples: 200, loss: 255.087168\n",
      "iteration 6610 / 8000, samples: 200, loss: 350.293426\n",
      "iteration 6620 / 8000, samples: 200, loss: 234.770185\n",
      "iteration 6630 / 8000, samples: 200, loss: 215.458210\n",
      "iteration 6640 / 8000, samples: 200, loss: 338.662186\n",
      "iteration 6650 / 8000, samples: 200, loss: 236.658832\n",
      "iteration 6660 / 8000, samples: 200, loss: 222.701977\n",
      "iteration 6670 / 8000, samples: 200, loss: 153.016270\n",
      "iteration 6680 / 8000, samples: 200, loss: 303.797014\n",
      "iteration 6690 / 8000, samples: 200, loss: 185.379400\n",
      "iteration 6700 / 8000, samples: 200, loss: 252.322260\n",
      "iteration 6710 / 8000, samples: 200, loss: 203.649970\n",
      "iteration 6720 / 8000, samples: 200, loss: 213.809436\n",
      "iteration 6730 / 8000, samples: 200, loss: 213.653655\n",
      "iteration 6740 / 8000, samples: 200, loss: 251.383889\n",
      "iteration 6750 / 8000, samples: 200, loss: 271.045447\n",
      "iteration 6760 / 8000, samples: 200, loss: 399.830389\n",
      "iteration 6770 / 8000, samples: 200, loss: 284.642466\n",
      "iteration 6780 / 8000, samples: 200, loss: 179.918284\n",
      "iteration 6790 / 8000, samples: 200, loss: 289.871945\n",
      "iteration 6800 / 8000, samples: 200, loss: 244.231549\n",
      "iteration 6810 / 8000, samples: 200, loss: 293.446690\n",
      "iteration 6820 / 8000, samples: 200, loss: 231.251674\n",
      "iteration 6830 / 8000, samples: 200, loss: 262.248838\n",
      "iteration 6840 / 8000, samples: 200, loss: 148.914785\n",
      "iteration 6850 / 8000, samples: 200, loss: 313.583085\n",
      "iteration 6860 / 8000, samples: 200, loss: 192.850747\n",
      "iteration 6870 / 8000, samples: 200, loss: 456.810922\n",
      "iteration 6880 / 8000, samples: 200, loss: 244.398559\n",
      "iteration 6890 / 8000, samples: 200, loss: 261.931887\n",
      "iteration 6900 / 8000, samples: 200, loss: 297.873138\n",
      "iteration 6910 / 8000, samples: 200, loss: 312.296228\n",
      "iteration 6920 / 8000, samples: 200, loss: 219.730954\n",
      "iteration 6930 / 8000, samples: 200, loss: 265.822777\n",
      "iteration 6940 / 8000, samples: 200, loss: 411.307473\n",
      "iteration 6950 / 8000, samples: 200, loss: 231.285018\n",
      "iteration 6960 / 8000, samples: 200, loss: 420.960616\n",
      "iteration 6970 / 8000, samples: 200, loss: 319.872972\n",
      "iteration 6980 / 8000, samples: 200, loss: 298.017798\n",
      "iteration 6990 / 8000, samples: 200, loss: 248.488152\n",
      "iteration 7000 / 8000, samples: 200, loss: 431.619087\n",
      "iteration 7010 / 8000, samples: 200, loss: 211.380402\n",
      "iteration 7020 / 8000, samples: 200, loss: 226.942770\n",
      "iteration 7030 / 8000, samples: 200, loss: 189.945909\n",
      "iteration 7040 / 8000, samples: 200, loss: 247.617044\n",
      "iteration 7050 / 8000, samples: 200, loss: 268.923956\n",
      "iteration 7060 / 8000, samples: 200, loss: 188.280941\n",
      "iteration 7070 / 8000, samples: 200, loss: 283.301925\n",
      "iteration 7080 / 8000, samples: 200, loss: 320.101432\n",
      "iteration 7090 / 8000, samples: 200, loss: 157.663032\n",
      "iteration 7100 / 8000, samples: 200, loss: 228.677581\n",
      "iteration 7110 / 8000, samples: 200, loss: 237.227209\n",
      "iteration 7120 / 8000, samples: 200, loss: 237.428227\n",
      "iteration 7130 / 8000, samples: 200, loss: 299.135953\n",
      "iteration 7140 / 8000, samples: 200, loss: 276.667519\n",
      "iteration 7150 / 8000, samples: 200, loss: 301.452871\n",
      "iteration 7160 / 8000, samples: 200, loss: 141.435732\n",
      "iteration 7170 / 8000, samples: 200, loss: 238.941741\n",
      "iteration 7180 / 8000, samples: 200, loss: 239.015912\n",
      "iteration 7190 / 8000, samples: 200, loss: 262.666902\n",
      "iteration 7200 / 8000, samples: 200, loss: 190.448900\n",
      "iteration 7210 / 8000, samples: 200, loss: 286.838099\n",
      "iteration 7220 / 8000, samples: 200, loss: 231.381379\n",
      "iteration 7230 / 8000, samples: 200, loss: 369.556307\n",
      "iteration 7240 / 8000, samples: 200, loss: 197.997517\n",
      "iteration 7250 / 8000, samples: 200, loss: 281.607803\n",
      "iteration 7260 / 8000, samples: 200, loss: 274.616498\n",
      "iteration 7270 / 8000, samples: 200, loss: 240.930650\n",
      "iteration 7280 / 8000, samples: 200, loss: 208.614496\n",
      "iteration 7290 / 8000, samples: 200, loss: 157.479132\n",
      "iteration 7300 / 8000, samples: 200, loss: 248.624787\n",
      "iteration 7310 / 8000, samples: 200, loss: 304.475169\n",
      "iteration 7320 / 8000, samples: 200, loss: 208.148938\n",
      "iteration 7330 / 8000, samples: 200, loss: 196.022045\n",
      "iteration 7340 / 8000, samples: 200, loss: 279.896787\n",
      "iteration 7350 / 8000, samples: 200, loss: 232.345930\n",
      "iteration 7360 / 8000, samples: 200, loss: 205.583175\n",
      "iteration 7370 / 8000, samples: 200, loss: 194.325097\n",
      "iteration 7380 / 8000, samples: 200, loss: 340.189373\n",
      "iteration 7390 / 8000, samples: 200, loss: 431.158277\n",
      "iteration 7400 / 8000, samples: 200, loss: 324.275115\n",
      "iteration 7410 / 8000, samples: 200, loss: 246.065846\n",
      "iteration 7420 / 8000, samples: 200, loss: 216.702824\n",
      "iteration 7430 / 8000, samples: 200, loss: 197.232173\n",
      "iteration 7440 / 8000, samples: 200, loss: 144.216865\n",
      "iteration 7450 / 8000, samples: 200, loss: 394.527396\n",
      "iteration 7460 / 8000, samples: 200, loss: 176.062394\n",
      "iteration 7470 / 8000, samples: 200, loss: 240.239267\n",
      "iteration 7480 / 8000, samples: 200, loss: 260.600101\n",
      "iteration 7490 / 8000, samples: 200, loss: 203.481688\n",
      "iteration 7500 / 8000, samples: 200, loss: 289.637760\n",
      "iteration 7510 / 8000, samples: 200, loss: 322.581337\n",
      "iteration 7520 / 8000, samples: 200, loss: 356.303169\n",
      "iteration 7530 / 8000, samples: 200, loss: 206.532852\n",
      "iteration 7540 / 8000, samples: 200, loss: 264.866532\n",
      "iteration 7550 / 8000, samples: 200, loss: 282.509363\n",
      "iteration 7560 / 8000, samples: 200, loss: 346.732184\n",
      "iteration 7570 / 8000, samples: 200, loss: 306.029048\n",
      "iteration 7580 / 8000, samples: 200, loss: 226.335836\n",
      "iteration 7590 / 8000, samples: 200, loss: 216.636602\n",
      "iteration 7600 / 8000, samples: 200, loss: 284.520517\n",
      "iteration 7610 / 8000, samples: 200, loss: 330.582520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7620 / 8000, samples: 200, loss: 279.698786\n",
      "iteration 7630 / 8000, samples: 200, loss: 295.839502\n",
      "iteration 7640 / 8000, samples: 200, loss: 259.897617\n",
      "iteration 7650 / 8000, samples: 200, loss: 264.756579\n",
      "iteration 7660 / 8000, samples: 200, loss: 276.965449\n",
      "iteration 7670 / 8000, samples: 200, loss: 166.313638\n",
      "iteration 7680 / 8000, samples: 200, loss: 269.055620\n",
      "iteration 7690 / 8000, samples: 200, loss: 222.057376\n",
      "iteration 7700 / 8000, samples: 200, loss: 193.301499\n",
      "iteration 7710 / 8000, samples: 200, loss: 387.721494\n",
      "iteration 7720 / 8000, samples: 200, loss: 180.760521\n",
      "iteration 7730 / 8000, samples: 200, loss: 188.895939\n",
      "iteration 7740 / 8000, samples: 200, loss: 185.371839\n",
      "iteration 7750 / 8000, samples: 200, loss: 316.984170\n",
      "iteration 7760 / 8000, samples: 200, loss: 182.636883\n",
      "iteration 7770 / 8000, samples: 200, loss: 227.869268\n",
      "iteration 7780 / 8000, samples: 200, loss: 171.533894\n",
      "iteration 7790 / 8000, samples: 200, loss: 164.741326\n",
      "iteration 7800 / 8000, samples: 200, loss: 315.610183\n",
      "iteration 7810 / 8000, samples: 200, loss: 325.388172\n",
      "iteration 7820 / 8000, samples: 200, loss: 336.235820\n",
      "iteration 7830 / 8000, samples: 200, loss: 311.222432\n",
      "iteration 7840 / 8000, samples: 200, loss: 258.048899\n",
      "iteration 7850 / 8000, samples: 200, loss: 284.156141\n",
      "iteration 7860 / 8000, samples: 200, loss: 239.373868\n",
      "iteration 7870 / 8000, samples: 200, loss: 273.450977\n",
      "iteration 7880 / 8000, samples: 200, loss: 277.477816\n",
      "iteration 7890 / 8000, samples: 200, loss: 269.842127\n",
      "iteration 7900 / 8000, samples: 200, loss: 320.657603\n",
      "iteration 7910 / 8000, samples: 200, loss: 165.406949\n",
      "iteration 7920 / 8000, samples: 200, loss: 187.017004\n",
      "iteration 7930 / 8000, samples: 200, loss: 246.895856\n",
      "iteration 7940 / 8000, samples: 200, loss: 230.842949\n",
      "iteration 7950 / 8000, samples: 200, loss: 235.137303\n",
      "iteration 7960 / 8000, samples: 200, loss: 192.100659\n",
      "iteration 7970 / 8000, samples: 200, loss: 230.945300\n",
      "iteration 7980 / 8000, samples: 200, loss: 225.506622\n",
      "iteration 7990 / 8000, samples: 200, loss: 414.954231\n"
     ]
    }
   ],
   "source": [
    "LC = LinearClassifier()\n",
    "\n",
    "print('start training ...')\n",
    "#train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True)\n",
    "#在dataTrain中不重复随机抽取batch_size个样本，迭代训练num_iters次\n",
    "loss_all = LC.train(dataTrain, labelTrain, num_iters = 8000, batch_size = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss is 417.171393\n",
      "start predicting ...\n",
      "the accuracy rate is 24.000000 \n"
     ]
    }
   ],
   "source": [
    "dataTest = dataTest - np.mean(dataTest, axis=0)\n",
    "print('last loss is %f' %(loss_all[-1]))\n",
    "#开始预测\n",
    "print('start predicting ...')\n",
    "y_pred = LC.predict(dataTest)\n",
    "\n",
    "hit = 0\n",
    "for i in xrange(y_pred.size):\n",
    "\tif (y_pred[i] == labelTest[i]):\n",
    "\t\thit += 1\n",
    "\n",
    "print('the accuracy rate is %f ' % (hit/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
