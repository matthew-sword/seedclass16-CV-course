{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import print_function\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 文件读取\n",
    "def unpickle(file):\n",
    "\twith open(file,'rb') as fo:\n",
    "\t\tdict = pickle.load(fo)\n",
    "\treturn dict\n",
    "\n",
    "def load_file(file):\n",
    "\tdictTrain = unpickle(file + \"data_batch_1\")\n",
    "\tdataTrain = dictTrain['data']\n",
    "\tlabelTrain = dictTrain['labels']\n",
    "\n",
    "\tfor i in range(2,6):\n",
    "\t\tdictTrain = unpickle(file + \"data_batch_\" + str(i))\n",
    "\t\tdataTrain = np.vstack([dataTrain,dictTrain['data']])\n",
    "\t\tlabelTrain = np.hstack([labelTrain,dictTrain['labels']])\n",
    "\n",
    "\tdictTest = unpickle(file + \"test_batch\")\n",
    "\tdataTest = dictTest['data']\n",
    "\tlabelTest = dictTest['labels']\n",
    "\tlabelTest = np.array(labelTest)\n",
    "\n",
    "\treturn dataTrain, labelTrain, dataTest, labelTest\n",
    "\n",
    "\n",
    "#softmax loss 函数\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "\t'''\n",
    "\t\tW:权重矩阵\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t'''\n",
    "\t#初始化数据\n",
    "\tloss = 0.0\n",
    "\tdW = np.zeros_like(W)\n",
    "\tnum_train = X.shape[0]\t#样本数\n",
    "\tnum_class = W.shape[1]\t#样本类别数\n",
    "\n",
    "\tfor i in xrange(num_train):\n",
    "\t\tscore = X[i].dot(W)\n",
    "\t\tscore -= np.max(score)\t#提高样本稳定性\n",
    "\n",
    "\t\tcorrect_score = score[y[i]]\n",
    "\t\texp_sum = np.sum(np.exp(score))\n",
    "\t\tloss += np.log(exp_sum) - correct_score\n",
    "\n",
    "\t\tfor j in xrange(num_class):\n",
    "\t\t\tif (j == y[i]):\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i] - X[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i]\n",
    "\n",
    "\n",
    "\tloss /= num_train\n",
    "\tloss += 0.5 * reg * np.sum(W*W)\n",
    "\n",
    "\tdW /= num_train\n",
    "\tdW += reg * W\n",
    "\n",
    "\treturn loss, dW\n",
    "\n",
    "#线性分类器\n",
    "class LinearClassifier(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.W = None\n",
    "\n",
    "\tdef train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\tstep_size:学习步进速度\n",
    "\t\treg:正则化强度\n",
    "\t\tnum_iters:迭代次数\n",
    "\t\tbatch_size:每次迭代图片样本数\n",
    "\t\tverbose:是否打印信息\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss_history:每次训练loss值\n",
    "\t\t'''\n",
    "\t\tnum_train, dim = X.shape\n",
    "\t\tnum_classes = np.max(y) + 1\n",
    "\t\n",
    "\t\tif self.W is None:\n",
    "\t\t\tself.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "\t\tloss_history = []\n",
    "\n",
    "\t\tfor it in xrange(num_iters):\n",
    "\t\t\t#从样本中不重复随机采batch_size个样本\n",
    "\t\t\tsample_index = np.random.choice(num_train, batch_size, replace=False)\n",
    "\n",
    "\t\t\tX_batch = X[sample_index, :]\n",
    "\t\t\ty_batch = y[sample_index]\n",
    "\n",
    "\t\t\tloss, grad = self.loss(X_batch, y_batch, reg)\n",
    "\t\t\tloss_history.append(loss)\n",
    "\n",
    "\t\t\tself.W += -step_size * grad\n",
    "\n",
    "\t\t\tif (verbose and it %10 == 0):\n",
    "\t\t\t\tprint('iteration %d / %d, samples: %d, loss: %f' % (it, num_iters, batch_size, loss))\n",
    "\n",
    "\t\treturn loss_history\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\ty_pred:标签预测值\n",
    "\t\t'''\n",
    "\t\ty_pred = np.zeros(X.shape[1])\n",
    "\n",
    "\t\tscore = X.dot(self.W)\n",
    "\t\ty_pred = np.argmax(score, axis = 1)\n",
    "\n",
    "\t\treturn y_pred\n",
    "\n",
    "\n",
    "\tdef loss(self, X_batch, y_batch, reg):\n",
    "\t\t'''\n",
    "\t\tX_batch:图片训练集(矩阵)\n",
    "\t\ty_batch:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t\t'''\n",
    "\t\treturn softmax_loss_naive(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ...\n",
      "iteration 0 / 8000, samples: 200, loss: 9.808217\n",
      "iteration 10 / 8000, samples: 200, loss: 10605.763411\n",
      "iteration 20 / 8000, samples: 200, loss: 9304.199885\n",
      "iteration 30 / 8000, samples: 200, loss: 19357.470584\n",
      "iteration 40 / 8000, samples: 200, loss: 16554.449563\n",
      "iteration 50 / 8000, samples: 200, loss: 13248.801083\n",
      "iteration 60 / 8000, samples: 200, loss: 10138.387817\n",
      "iteration 70 / 8000, samples: 200, loss: 10918.990310\n",
      "iteration 80 / 8000, samples: 200, loss: 11331.581816\n",
      "iteration 90 / 8000, samples: 200, loss: 11071.385903\n",
      "iteration 100 / 8000, samples: 200, loss: 9529.599919\n",
      "iteration 110 / 8000, samples: 200, loss: 9089.952142\n",
      "iteration 120 / 8000, samples: 200, loss: 7366.002454\n",
      "iteration 130 / 8000, samples: 200, loss: 9860.515369\n",
      "iteration 140 / 8000, samples: 200, loss: 11059.625637\n",
      "iteration 150 / 8000, samples: 200, loss: 7284.592109\n",
      "iteration 160 / 8000, samples: 200, loss: 7384.702900\n",
      "iteration 170 / 8000, samples: 200, loss: 10297.431417\n",
      "iteration 180 / 8000, samples: 200, loss: 7619.645294\n",
      "iteration 190 / 8000, samples: 200, loss: 5962.581648\n",
      "iteration 200 / 8000, samples: 200, loss: 6601.066652\n",
      "iteration 210 / 8000, samples: 200, loss: 9910.641655\n",
      "iteration 220 / 8000, samples: 200, loss: 6538.764645\n",
      "iteration 230 / 8000, samples: 200, loss: 7881.373758\n",
      "iteration 240 / 8000, samples: 200, loss: 10821.332698\n",
      "iteration 250 / 8000, samples: 200, loss: 8584.721582\n",
      "iteration 260 / 8000, samples: 200, loss: 8365.456641\n",
      "iteration 270 / 8000, samples: 200, loss: 6818.931295\n",
      "iteration 280 / 8000, samples: 200, loss: 7483.118717\n",
      "iteration 290 / 8000, samples: 200, loss: 6757.174136\n",
      "iteration 300 / 8000, samples: 200, loss: 8707.851701\n",
      "iteration 310 / 8000, samples: 200, loss: 9805.804600\n",
      "iteration 320 / 8000, samples: 200, loss: 7339.664130\n",
      "iteration 330 / 8000, samples: 200, loss: 4624.981243\n",
      "iteration 340 / 8000, samples: 200, loss: 8754.268666\n",
      "iteration 350 / 8000, samples: 200, loss: 7358.452922\n",
      "iteration 360 / 8000, samples: 200, loss: 6846.057709\n",
      "iteration 370 / 8000, samples: 200, loss: 8879.078615\n",
      "iteration 380 / 8000, samples: 200, loss: 7321.259411\n",
      "iteration 390 / 8000, samples: 200, loss: 5751.891569\n",
      "iteration 400 / 8000, samples: 200, loss: 7629.503490\n",
      "iteration 410 / 8000, samples: 200, loss: 7896.456068\n",
      "iteration 420 / 8000, samples: 200, loss: 6190.290340\n",
      "iteration 430 / 8000, samples: 200, loss: 8246.570768\n",
      "iteration 440 / 8000, samples: 200, loss: 6055.899620\n",
      "iteration 450 / 8000, samples: 200, loss: 10142.519149\n",
      "iteration 460 / 8000, samples: 200, loss: 7905.785772\n",
      "iteration 470 / 8000, samples: 200, loss: 6323.295780\n",
      "iteration 480 / 8000, samples: 200, loss: 5349.967947\n",
      "iteration 490 / 8000, samples: 200, loss: 11542.742971\n",
      "iteration 500 / 8000, samples: 200, loss: 8921.855696\n",
      "iteration 510 / 8000, samples: 200, loss: 8501.375818\n",
      "iteration 520 / 8000, samples: 200, loss: 5572.764684\n",
      "iteration 530 / 8000, samples: 200, loss: 7438.460120\n",
      "iteration 540 / 8000, samples: 200, loss: 7791.171438\n",
      "iteration 550 / 8000, samples: 200, loss: 8908.690929\n",
      "iteration 560 / 8000, samples: 200, loss: 5387.011937\n",
      "iteration 570 / 8000, samples: 200, loss: 10646.419717\n",
      "iteration 580 / 8000, samples: 200, loss: 8816.824908\n",
      "iteration 590 / 8000, samples: 200, loss: 4761.845348\n",
      "iteration 600 / 8000, samples: 200, loss: 7950.980191\n",
      "iteration 610 / 8000, samples: 200, loss: 8289.754281\n",
      "iteration 620 / 8000, samples: 200, loss: 5626.563504\n",
      "iteration 630 / 8000, samples: 200, loss: 5450.882332\n",
      "iteration 640 / 8000, samples: 200, loss: 6785.687979\n",
      "iteration 650 / 8000, samples: 200, loss: 5826.498347\n",
      "iteration 660 / 8000, samples: 200, loss: 5228.964152\n",
      "iteration 670 / 8000, samples: 200, loss: 6163.912506\n",
      "iteration 680 / 8000, samples: 200, loss: 6490.922910\n",
      "iteration 690 / 8000, samples: 200, loss: 6056.325977\n",
      "iteration 700 / 8000, samples: 200, loss: 5515.861124\n",
      "iteration 710 / 8000, samples: 200, loss: 7322.222377\n",
      "iteration 720 / 8000, samples: 200, loss: 7219.271423\n",
      "iteration 730 / 8000, samples: 200, loss: 4569.993243\n",
      "iteration 740 / 8000, samples: 200, loss: 4071.042666\n",
      "iteration 750 / 8000, samples: 200, loss: 8003.304648\n",
      "iteration 760 / 8000, samples: 200, loss: 4875.337753\n",
      "iteration 770 / 8000, samples: 200, loss: 6886.770882\n",
      "iteration 780 / 8000, samples: 200, loss: 6256.790981\n",
      "iteration 790 / 8000, samples: 200, loss: 7707.690313\n",
      "iteration 800 / 8000, samples: 200, loss: 8320.198720\n",
      "iteration 810 / 8000, samples: 200, loss: 9021.632729\n",
      "iteration 820 / 8000, samples: 200, loss: 4907.286455\n",
      "iteration 830 / 8000, samples: 200, loss: 7736.187825\n",
      "iteration 840 / 8000, samples: 200, loss: 7893.563398\n",
      "iteration 850 / 8000, samples: 200, loss: 8277.916811\n",
      "iteration 860 / 8000, samples: 200, loss: 7941.679905\n",
      "iteration 870 / 8000, samples: 200, loss: 5061.471374\n",
      "iteration 880 / 8000, samples: 200, loss: 8961.772833\n",
      "iteration 890 / 8000, samples: 200, loss: 4497.398935\n",
      "iteration 900 / 8000, samples: 200, loss: 5380.650378\n",
      "iteration 910 / 8000, samples: 200, loss: 5697.535952\n",
      "iteration 920 / 8000, samples: 200, loss: 5997.309243\n",
      "iteration 930 / 8000, samples: 200, loss: 6040.631211\n",
      "iteration 940 / 8000, samples: 200, loss: 4821.964970\n",
      "iteration 950 / 8000, samples: 200, loss: 5569.612601\n",
      "iteration 960 / 8000, samples: 200, loss: 5679.645754\n",
      "iteration 970 / 8000, samples: 200, loss: 6825.944640\n",
      "iteration 980 / 8000, samples: 200, loss: 7387.705064\n",
      "iteration 990 / 8000, samples: 200, loss: 6871.121383\n",
      "iteration 1000 / 8000, samples: 200, loss: 7144.847461\n",
      "iteration 1010 / 8000, samples: 200, loss: 6754.642870\n",
      "iteration 1020 / 8000, samples: 200, loss: 6664.239501\n",
      "iteration 1030 / 8000, samples: 200, loss: 6791.914386\n",
      "iteration 1040 / 8000, samples: 200, loss: 6438.840202\n",
      "iteration 1050 / 8000, samples: 200, loss: 7586.348454\n",
      "iteration 1060 / 8000, samples: 200, loss: 6263.656493\n",
      "iteration 1070 / 8000, samples: 200, loss: 7833.650389\n",
      "iteration 1080 / 8000, samples: 200, loss: 6712.185575\n",
      "iteration 1090 / 8000, samples: 200, loss: 5827.467450\n",
      "iteration 1100 / 8000, samples: 200, loss: 5529.568632\n",
      "iteration 1110 / 8000, samples: 200, loss: 5913.951879\n",
      "iteration 1120 / 8000, samples: 200, loss: 6028.174221\n",
      "iteration 1130 / 8000, samples: 200, loss: 6807.334649\n",
      "iteration 1140 / 8000, samples: 200, loss: 3788.267345\n",
      "iteration 1150 / 8000, samples: 200, loss: 4393.319397\n",
      "iteration 1160 / 8000, samples: 200, loss: 4938.564077\n",
      "iteration 1170 / 8000, samples: 200, loss: 4161.914407\n",
      "iteration 1180 / 8000, samples: 200, loss: 3472.097230\n",
      "iteration 1190 / 8000, samples: 200, loss: 7912.387367\n",
      "iteration 1200 / 8000, samples: 200, loss: 5682.046563\n",
      "iteration 1210 / 8000, samples: 200, loss: 6529.986142\n",
      "iteration 1220 / 8000, samples: 200, loss: 6393.402962\n",
      "iteration 1230 / 8000, samples: 200, loss: 7309.257331\n",
      "iteration 1240 / 8000, samples: 200, loss: 4610.092357\n",
      "iteration 1250 / 8000, samples: 200, loss: 6433.197021\n",
      "iteration 1260 / 8000, samples: 200, loss: 7841.333899\n",
      "iteration 1270 / 8000, samples: 200, loss: 6745.042666\n",
      "iteration 1280 / 8000, samples: 200, loss: 5547.208716\n",
      "iteration 1290 / 8000, samples: 200, loss: 6209.410368\n",
      "iteration 1300 / 8000, samples: 200, loss: 6186.829225\n",
      "iteration 1310 / 8000, samples: 200, loss: 6674.713706\n",
      "iteration 1320 / 8000, samples: 200, loss: 5753.457512\n",
      "iteration 1330 / 8000, samples: 200, loss: 6994.261956\n",
      "iteration 1340 / 8000, samples: 200, loss: 10813.086965\n",
      "iteration 1350 / 8000, samples: 200, loss: 6234.199755\n",
      "iteration 1360 / 8000, samples: 200, loss: 4648.450914\n",
      "iteration 1370 / 8000, samples: 200, loss: 6690.601296\n",
      "iteration 1380 / 8000, samples: 200, loss: 5118.157418\n",
      "iteration 1390 / 8000, samples: 200, loss: 6187.044469\n",
      "iteration 1400 / 8000, samples: 200, loss: 6511.008756\n",
      "iteration 1410 / 8000, samples: 200, loss: 7137.208858\n",
      "iteration 1420 / 8000, samples: 200, loss: 8375.055143\n",
      "iteration 1430 / 8000, samples: 200, loss: 9951.151093\n",
      "iteration 1440 / 8000, samples: 200, loss: 6996.030071\n",
      "iteration 1450 / 8000, samples: 200, loss: 6366.051944\n",
      "iteration 1460 / 8000, samples: 200, loss: 5600.890123\n",
      "iteration 1470 / 8000, samples: 200, loss: 5666.736520\n",
      "iteration 1480 / 8000, samples: 200, loss: 8006.960929\n",
      "iteration 1490 / 8000, samples: 200, loss: 5251.471625\n",
      "iteration 1500 / 8000, samples: 200, loss: 8614.029969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1510 / 8000, samples: 200, loss: 5623.901465\n",
      "iteration 1520 / 8000, samples: 200, loss: 4787.045802\n",
      "iteration 1530 / 8000, samples: 200, loss: 7464.028349\n",
      "iteration 1540 / 8000, samples: 200, loss: 6361.864695\n",
      "iteration 1550 / 8000, samples: 200, loss: 6162.751724\n",
      "iteration 1560 / 8000, samples: 200, loss: 4658.702686\n",
      "iteration 1570 / 8000, samples: 200, loss: 4390.125049\n",
      "iteration 1580 / 8000, samples: 200, loss: 7231.073791\n",
      "iteration 1590 / 8000, samples: 200, loss: 5856.447442\n",
      "iteration 1600 / 8000, samples: 200, loss: 8388.256449\n",
      "iteration 1610 / 8000, samples: 200, loss: 8684.496906\n",
      "iteration 1620 / 8000, samples: 200, loss: 5143.354317\n",
      "iteration 1630 / 8000, samples: 200, loss: 4439.868837\n",
      "iteration 1640 / 8000, samples: 200, loss: 5612.203192\n",
      "iteration 1650 / 8000, samples: 200, loss: 4151.878924\n",
      "iteration 1660 / 8000, samples: 200, loss: 3483.072186\n",
      "iteration 1670 / 8000, samples: 200, loss: 4831.612373\n",
      "iteration 1680 / 8000, samples: 200, loss: 5696.018566\n",
      "iteration 1690 / 8000, samples: 200, loss: 6779.040774\n",
      "iteration 1700 / 8000, samples: 200, loss: 5969.098065\n",
      "iteration 1710 / 8000, samples: 200, loss: 6337.578727\n",
      "iteration 1720 / 8000, samples: 200, loss: 7067.305104\n",
      "iteration 1730 / 8000, samples: 200, loss: 7401.682780\n",
      "iteration 1740 / 8000, samples: 200, loss: 4608.163388\n",
      "iteration 1750 / 8000, samples: 200, loss: 8543.874174\n",
      "iteration 1760 / 8000, samples: 200, loss: 4745.669759\n",
      "iteration 1770 / 8000, samples: 200, loss: 7030.718844\n",
      "iteration 1780 / 8000, samples: 200, loss: 7643.486678\n",
      "iteration 1790 / 8000, samples: 200, loss: 3448.606335\n",
      "iteration 1800 / 8000, samples: 200, loss: 6421.515714\n",
      "iteration 1810 / 8000, samples: 200, loss: 7475.302713\n",
      "iteration 1820 / 8000, samples: 200, loss: 8174.321515\n",
      "iteration 1830 / 8000, samples: 200, loss: 8077.561999\n",
      "iteration 1840 / 8000, samples: 200, loss: 10414.199550\n",
      "iteration 1850 / 8000, samples: 200, loss: 4073.800959\n",
      "iteration 1860 / 8000, samples: 200, loss: 4048.332390\n",
      "iteration 1870 / 8000, samples: 200, loss: 6650.229843\n",
      "iteration 1880 / 8000, samples: 200, loss: 4035.932327\n",
      "iteration 1890 / 8000, samples: 200, loss: 5452.854328\n",
      "iteration 1900 / 8000, samples: 200, loss: 7214.407163\n",
      "iteration 1910 / 8000, samples: 200, loss: 6231.710002\n",
      "iteration 1920 / 8000, samples: 200, loss: 10572.802080\n",
      "iteration 1930 / 8000, samples: 200, loss: 6044.987494\n",
      "iteration 1940 / 8000, samples: 200, loss: 4786.156631\n",
      "iteration 1950 / 8000, samples: 200, loss: 4879.671275\n",
      "iteration 1960 / 8000, samples: 200, loss: 5652.612943\n",
      "iteration 1970 / 8000, samples: 200, loss: 5995.093963\n",
      "iteration 1980 / 8000, samples: 200, loss: 5464.047828\n",
      "iteration 1990 / 8000, samples: 200, loss: 7093.350861\n",
      "iteration 2000 / 8000, samples: 200, loss: 4305.208575\n",
      "iteration 2010 / 8000, samples: 200, loss: 5553.757665\n",
      "iteration 2020 / 8000, samples: 200, loss: 5303.587790\n",
      "iteration 2030 / 8000, samples: 200, loss: 4854.381474\n",
      "iteration 2040 / 8000, samples: 200, loss: 6033.193651\n",
      "iteration 2050 / 8000, samples: 200, loss: 3547.298243\n",
      "iteration 2060 / 8000, samples: 200, loss: 7981.135498\n",
      "iteration 2070 / 8000, samples: 200, loss: 4888.429187\n",
      "iteration 2080 / 8000, samples: 200, loss: 6176.759590\n",
      "iteration 2090 / 8000, samples: 200, loss: 6502.545166\n",
      "iteration 2100 / 8000, samples: 200, loss: 4957.330259\n",
      "iteration 2110 / 8000, samples: 200, loss: 5478.667964\n",
      "iteration 2120 / 8000, samples: 200, loss: 6556.621505\n",
      "iteration 2130 / 8000, samples: 200, loss: 7256.026853\n",
      "iteration 2140 / 8000, samples: 200, loss: 5545.864330\n",
      "iteration 2150 / 8000, samples: 200, loss: 6132.738317\n",
      "iteration 2160 / 8000, samples: 200, loss: 4532.693277\n",
      "iteration 2170 / 8000, samples: 200, loss: 6913.893093\n",
      "iteration 2180 / 8000, samples: 200, loss: 3368.320316\n",
      "iteration 2190 / 8000, samples: 200, loss: 4282.063032\n",
      "iteration 2200 / 8000, samples: 200, loss: 5372.786395\n",
      "iteration 2210 / 8000, samples: 200, loss: 4989.273288\n",
      "iteration 2220 / 8000, samples: 200, loss: 5093.737629\n",
      "iteration 2230 / 8000, samples: 200, loss: 6099.696278\n",
      "iteration 2240 / 8000, samples: 200, loss: 8219.902684\n",
      "iteration 2250 / 8000, samples: 200, loss: 7049.936995\n",
      "iteration 2260 / 8000, samples: 200, loss: 6687.318236\n",
      "iteration 2270 / 8000, samples: 200, loss: 3549.234220\n",
      "iteration 2280 / 8000, samples: 200, loss: 5020.562605\n",
      "iteration 2290 / 8000, samples: 200, loss: 4852.897209\n",
      "iteration 2300 / 8000, samples: 200, loss: 5707.304837\n",
      "iteration 2310 / 8000, samples: 200, loss: 7184.414674\n",
      "iteration 2320 / 8000, samples: 200, loss: 8493.550715\n",
      "iteration 2330 / 8000, samples: 200, loss: 6697.897698\n",
      "iteration 2340 / 8000, samples: 200, loss: 4462.102164\n",
      "iteration 2350 / 8000, samples: 200, loss: 3904.033351\n",
      "iteration 2360 / 8000, samples: 200, loss: 5509.486264\n",
      "iteration 2370 / 8000, samples: 200, loss: 5645.495649\n",
      "iteration 2380 / 8000, samples: 200, loss: 6730.480621\n",
      "iteration 2390 / 8000, samples: 200, loss: 4001.490984\n",
      "iteration 2400 / 8000, samples: 200, loss: 7608.006391\n",
      "iteration 2410 / 8000, samples: 200, loss: 4553.626082\n",
      "iteration 2420 / 8000, samples: 200, loss: 7413.724029\n",
      "iteration 2430 / 8000, samples: 200, loss: 6165.101176\n",
      "iteration 2440 / 8000, samples: 200, loss: 7288.359853\n",
      "iteration 2450 / 8000, samples: 200, loss: 7413.661394\n",
      "iteration 2460 / 8000, samples: 200, loss: 7409.404228\n",
      "iteration 2470 / 8000, samples: 200, loss: 4618.790373\n",
      "iteration 2480 / 8000, samples: 200, loss: 8861.115027\n",
      "iteration 2490 / 8000, samples: 200, loss: 5822.674108\n",
      "iteration 2500 / 8000, samples: 200, loss: 5634.628221\n",
      "iteration 2510 / 8000, samples: 200, loss: 3592.668968\n",
      "iteration 2520 / 8000, samples: 200, loss: 4869.009724\n",
      "iteration 2530 / 8000, samples: 200, loss: 7548.361583\n",
      "iteration 2540 / 8000, samples: 200, loss: 6253.571222\n",
      "iteration 2550 / 8000, samples: 200, loss: 4868.521720\n",
      "iteration 2560 / 8000, samples: 200, loss: 6926.818251\n",
      "iteration 2570 / 8000, samples: 200, loss: 7574.807632\n",
      "iteration 2580 / 8000, samples: 200, loss: 6842.251219\n",
      "iteration 2590 / 8000, samples: 200, loss: 5655.967524\n",
      "iteration 2600 / 8000, samples: 200, loss: 3323.889148\n",
      "iteration 2610 / 8000, samples: 200, loss: 5238.632492\n",
      "iteration 2620 / 8000, samples: 200, loss: 7982.955976\n",
      "iteration 2630 / 8000, samples: 200, loss: 7189.621199\n",
      "iteration 2640 / 8000, samples: 200, loss: 6700.703594\n",
      "iteration 2650 / 8000, samples: 200, loss: 5643.390991\n",
      "iteration 2660 / 8000, samples: 200, loss: 5025.421571\n",
      "iteration 2670 / 8000, samples: 200, loss: 2903.570751\n",
      "iteration 2680 / 8000, samples: 200, loss: 5536.660733\n",
      "iteration 2690 / 8000, samples: 200, loss: 3827.520253\n",
      "iteration 2700 / 8000, samples: 200, loss: 4048.892285\n",
      "iteration 2710 / 8000, samples: 200, loss: 5790.339459\n",
      "iteration 2720 / 8000, samples: 200, loss: 5917.747850\n",
      "iteration 2730 / 8000, samples: 200, loss: 3695.806192\n",
      "iteration 2740 / 8000, samples: 200, loss: 6359.770893\n",
      "iteration 2750 / 8000, samples: 200, loss: 8641.194627\n",
      "iteration 2760 / 8000, samples: 200, loss: 6103.791684\n",
      "iteration 2770 / 8000, samples: 200, loss: 6564.604140\n",
      "iteration 2780 / 8000, samples: 200, loss: 7818.700460\n",
      "iteration 2790 / 8000, samples: 200, loss: 5726.440870\n",
      "iteration 2800 / 8000, samples: 200, loss: 5283.564146\n",
      "iteration 2810 / 8000, samples: 200, loss: 6426.966001\n",
      "iteration 2820 / 8000, samples: 200, loss: 5976.296804\n",
      "iteration 2830 / 8000, samples: 200, loss: 5417.692112\n",
      "iteration 2840 / 8000, samples: 200, loss: 6566.219724\n",
      "iteration 2850 / 8000, samples: 200, loss: 3528.758381\n",
      "iteration 2860 / 8000, samples: 200, loss: 7443.020956\n",
      "iteration 2870 / 8000, samples: 200, loss: 5907.266235\n",
      "iteration 2880 / 8000, samples: 200, loss: 9874.041943\n",
      "iteration 2890 / 8000, samples: 200, loss: 5830.420372\n",
      "iteration 2900 / 8000, samples: 200, loss: 5121.071047\n",
      "iteration 2910 / 8000, samples: 200, loss: 5487.420596\n",
      "iteration 2920 / 8000, samples: 200, loss: 4358.579809\n",
      "iteration 2930 / 8000, samples: 200, loss: 6589.145776\n",
      "iteration 2940 / 8000, samples: 200, loss: 8954.106044\n",
      "iteration 2950 / 8000, samples: 200, loss: 6391.176659\n",
      "iteration 2960 / 8000, samples: 200, loss: 4489.870820\n",
      "iteration 2970 / 8000, samples: 200, loss: 4811.019051\n",
      "iteration 2980 / 8000, samples: 200, loss: 5702.557768\n",
      "iteration 2990 / 8000, samples: 200, loss: 5521.570954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000 / 8000, samples: 200, loss: 6902.769420\n",
      "iteration 3010 / 8000, samples: 200, loss: 5546.401474\n",
      "iteration 3020 / 8000, samples: 200, loss: 4402.681200\n",
      "iteration 3030 / 8000, samples: 200, loss: 6695.867150\n",
      "iteration 3040 / 8000, samples: 200, loss: 5392.888579\n",
      "iteration 3050 / 8000, samples: 200, loss: 5185.743481\n",
      "iteration 3060 / 8000, samples: 200, loss: 5762.328866\n",
      "iteration 3070 / 8000, samples: 200, loss: 8852.478527\n",
      "iteration 3080 / 8000, samples: 200, loss: 5267.851889\n",
      "iteration 3090 / 8000, samples: 200, loss: 8897.619818\n",
      "iteration 3100 / 8000, samples: 200, loss: 7287.597613\n",
      "iteration 3110 / 8000, samples: 200, loss: 4689.460758\n",
      "iteration 3120 / 8000, samples: 200, loss: 5959.293745\n",
      "iteration 3130 / 8000, samples: 200, loss: 7201.109522\n",
      "iteration 3140 / 8000, samples: 200, loss: 7423.420253\n",
      "iteration 3150 / 8000, samples: 200, loss: 4023.460111\n",
      "iteration 3160 / 8000, samples: 200, loss: 5711.269175\n",
      "iteration 3170 / 8000, samples: 200, loss: 4586.373140\n",
      "iteration 3180 / 8000, samples: 200, loss: 5221.012423\n",
      "iteration 3190 / 8000, samples: 200, loss: 4072.842904\n",
      "iteration 3200 / 8000, samples: 200, loss: 7655.240676\n",
      "iteration 3210 / 8000, samples: 200, loss: 4493.508737\n",
      "iteration 3220 / 8000, samples: 200, loss: 5588.819187\n",
      "iteration 3230 / 8000, samples: 200, loss: 6357.364583\n",
      "iteration 3240 / 8000, samples: 200, loss: 3746.055472\n",
      "iteration 3250 / 8000, samples: 200, loss: 4495.061215\n",
      "iteration 3260 / 8000, samples: 200, loss: 4725.603109\n",
      "iteration 3270 / 8000, samples: 200, loss: 6531.246021\n",
      "iteration 3280 / 8000, samples: 200, loss: 3909.909568\n",
      "iteration 3290 / 8000, samples: 200, loss: 8443.519670\n",
      "iteration 3300 / 8000, samples: 200, loss: 3760.725148\n",
      "iteration 3310 / 8000, samples: 200, loss: 4364.361686\n",
      "iteration 3320 / 8000, samples: 200, loss: 5615.118578\n",
      "iteration 3330 / 8000, samples: 200, loss: 5410.069138\n",
      "iteration 3340 / 8000, samples: 200, loss: 6875.035357\n",
      "iteration 3350 / 8000, samples: 200, loss: 6211.761729\n",
      "iteration 3360 / 8000, samples: 200, loss: 4981.112548\n",
      "iteration 3370 / 8000, samples: 200, loss: 6211.880027\n",
      "iteration 3380 / 8000, samples: 200, loss: 4713.638637\n",
      "iteration 3390 / 8000, samples: 200, loss: 9248.342512\n",
      "iteration 3400 / 8000, samples: 200, loss: 6382.283929\n",
      "iteration 3410 / 8000, samples: 200, loss: 5888.502835\n",
      "iteration 3420 / 8000, samples: 200, loss: 5649.864856\n",
      "iteration 3430 / 8000, samples: 200, loss: 4639.837282\n",
      "iteration 3440 / 8000, samples: 200, loss: 4133.387208\n",
      "iteration 3450 / 8000, samples: 200, loss: 3882.993448\n",
      "iteration 3460 / 8000, samples: 200, loss: 7851.646954\n",
      "iteration 3470 / 8000, samples: 200, loss: 3688.070083\n",
      "iteration 3480 / 8000, samples: 200, loss: 5243.328934\n",
      "iteration 3490 / 8000, samples: 200, loss: 5203.178541\n",
      "iteration 3500 / 8000, samples: 200, loss: 3790.682953\n",
      "iteration 3510 / 8000, samples: 200, loss: 6013.898397\n",
      "iteration 3520 / 8000, samples: 200, loss: 4689.288119\n",
      "iteration 3530 / 8000, samples: 200, loss: 5819.832485\n",
      "iteration 3540 / 8000, samples: 200, loss: 8037.044520\n",
      "iteration 3550 / 8000, samples: 200, loss: 4980.313574\n",
      "iteration 3560 / 8000, samples: 200, loss: 7730.369326\n",
      "iteration 3570 / 8000, samples: 200, loss: 5468.459339\n",
      "iteration 3580 / 8000, samples: 200, loss: 4679.309787\n",
      "iteration 3590 / 8000, samples: 200, loss: 4499.478719\n",
      "iteration 3600 / 8000, samples: 200, loss: 6648.473290\n",
      "iteration 3610 / 8000, samples: 200, loss: 7553.888389\n",
      "iteration 3620 / 8000, samples: 200, loss: 6042.882514\n",
      "iteration 3630 / 8000, samples: 200, loss: 5717.566229\n",
      "iteration 3640 / 8000, samples: 200, loss: 6469.643679\n",
      "iteration 3650 / 8000, samples: 200, loss: 5287.599604\n",
      "iteration 3660 / 8000, samples: 200, loss: 5607.850382\n",
      "iteration 3670 / 8000, samples: 200, loss: 8143.586695\n",
      "iteration 3680 / 8000, samples: 200, loss: 4184.701273\n",
      "iteration 3690 / 8000, samples: 200, loss: 5403.952235\n",
      "iteration 3700 / 8000, samples: 200, loss: 6294.131765\n",
      "iteration 3710 / 8000, samples: 200, loss: 7706.767995\n",
      "iteration 3720 / 8000, samples: 200, loss: 7528.905200\n",
      "iteration 3730 / 8000, samples: 200, loss: 9946.403024\n",
      "iteration 3740 / 8000, samples: 200, loss: 7476.658551\n",
      "iteration 3750 / 8000, samples: 200, loss: 5779.870309\n",
      "iteration 3760 / 8000, samples: 200, loss: 5271.732408\n",
      "iteration 3770 / 8000, samples: 200, loss: 3267.977598\n",
      "iteration 3780 / 8000, samples: 200, loss: 4978.736084\n",
      "iteration 3790 / 8000, samples: 200, loss: 4598.456759\n",
      "iteration 3800 / 8000, samples: 200, loss: 5838.522448\n",
      "iteration 3810 / 8000, samples: 200, loss: 4538.665872\n",
      "iteration 3820 / 8000, samples: 200, loss: 6272.765632\n",
      "iteration 3830 / 8000, samples: 200, loss: 7101.524967\n",
      "iteration 3840 / 8000, samples: 200, loss: 5651.033730\n",
      "iteration 3850 / 8000, samples: 200, loss: 4506.951800\n",
      "iteration 3860 / 8000, samples: 200, loss: 6848.459102\n",
      "iteration 3870 / 8000, samples: 200, loss: 4805.970999\n",
      "iteration 3880 / 8000, samples: 200, loss: 7256.898536\n",
      "iteration 3890 / 8000, samples: 200, loss: 6728.054966\n",
      "iteration 3900 / 8000, samples: 200, loss: 5874.456979\n",
      "iteration 3910 / 8000, samples: 200, loss: 4431.285023\n",
      "iteration 3920 / 8000, samples: 200, loss: 5573.010660\n",
      "iteration 3930 / 8000, samples: 200, loss: 5265.457078\n",
      "iteration 3940 / 8000, samples: 200, loss: 8362.740185\n",
      "iteration 3950 / 8000, samples: 200, loss: 6180.857094\n",
      "iteration 3960 / 8000, samples: 200, loss: 5535.053222\n",
      "iteration 3970 / 8000, samples: 200, loss: 6399.828642\n",
      "iteration 3980 / 8000, samples: 200, loss: 4265.166774\n",
      "iteration 3990 / 8000, samples: 200, loss: 4493.580134\n",
      "iteration 4000 / 8000, samples: 200, loss: 5989.101740\n",
      "iteration 4010 / 8000, samples: 200, loss: 7111.493296\n",
      "iteration 4020 / 8000, samples: 200, loss: 4582.709432\n",
      "iteration 4030 / 8000, samples: 200, loss: 6032.982815\n",
      "iteration 4040 / 8000, samples: 200, loss: 4122.112999\n",
      "iteration 4050 / 8000, samples: 200, loss: 8688.821638\n",
      "iteration 4060 / 8000, samples: 200, loss: 6554.670857\n",
      "iteration 4070 / 8000, samples: 200, loss: 6141.602929\n",
      "iteration 4080 / 8000, samples: 200, loss: 5607.921365\n",
      "iteration 4090 / 8000, samples: 200, loss: 6110.569382\n",
      "iteration 4100 / 8000, samples: 200, loss: 5352.383341\n",
      "iteration 4110 / 8000, samples: 200, loss: 6683.134564\n",
      "iteration 4120 / 8000, samples: 200, loss: 6831.219920\n",
      "iteration 4130 / 8000, samples: 200, loss: 4251.345815\n",
      "iteration 4140 / 8000, samples: 200, loss: 7040.169971\n",
      "iteration 4150 / 8000, samples: 200, loss: 4192.393472\n",
      "iteration 4160 / 8000, samples: 200, loss: 7019.640715\n",
      "iteration 4170 / 8000, samples: 200, loss: 4720.074904\n",
      "iteration 4180 / 8000, samples: 200, loss: 4762.167419\n",
      "iteration 4190 / 8000, samples: 200, loss: 6898.859736\n",
      "iteration 4200 / 8000, samples: 200, loss: 5304.040415\n",
      "iteration 4210 / 8000, samples: 200, loss: 5980.972898\n",
      "iteration 4220 / 8000, samples: 200, loss: 6160.131707\n",
      "iteration 4230 / 8000, samples: 200, loss: 6701.538179\n",
      "iteration 4240 / 8000, samples: 200, loss: 5317.405161\n",
      "iteration 4250 / 8000, samples: 200, loss: 6739.800651\n",
      "iteration 4260 / 8000, samples: 200, loss: 5643.794275\n",
      "iteration 4270 / 8000, samples: 200, loss: 3167.924529\n",
      "iteration 4280 / 8000, samples: 200, loss: 8319.220026\n",
      "iteration 4290 / 8000, samples: 200, loss: 5904.543652\n",
      "iteration 4300 / 8000, samples: 200, loss: 5384.023694\n",
      "iteration 4310 / 8000, samples: 200, loss: 4672.973239\n",
      "iteration 4320 / 8000, samples: 200, loss: 4603.056333\n",
      "iteration 4330 / 8000, samples: 200, loss: 5066.747459\n",
      "iteration 4340 / 8000, samples: 200, loss: 5607.591776\n",
      "iteration 4350 / 8000, samples: 200, loss: 5717.971376\n",
      "iteration 4360 / 8000, samples: 200, loss: 9294.400240\n",
      "iteration 4370 / 8000, samples: 200, loss: 5286.150790\n",
      "iteration 4380 / 8000, samples: 200, loss: 7388.622195\n",
      "iteration 4390 / 8000, samples: 200, loss: 5272.778187\n",
      "iteration 4400 / 8000, samples: 200, loss: 5015.612837\n",
      "iteration 4410 / 8000, samples: 200, loss: 6606.823848\n",
      "iteration 4420 / 8000, samples: 200, loss: 6983.753270\n",
      "iteration 4430 / 8000, samples: 200, loss: 6853.531785\n",
      "iteration 4440 / 8000, samples: 200, loss: 6274.108344\n",
      "iteration 4450 / 8000, samples: 200, loss: 5568.534038\n",
      "iteration 4460 / 8000, samples: 200, loss: 6498.298258\n",
      "iteration 4470 / 8000, samples: 200, loss: 6421.475190\n",
      "iteration 4480 / 8000, samples: 200, loss: 4185.253042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4490 / 8000, samples: 200, loss: 4834.954044\n",
      "iteration 4500 / 8000, samples: 200, loss: 5923.936903\n",
      "iteration 4510 / 8000, samples: 200, loss: 6386.132531\n",
      "iteration 4520 / 8000, samples: 200, loss: 6607.929668\n",
      "iteration 4530 / 8000, samples: 200, loss: 6549.451319\n",
      "iteration 4540 / 8000, samples: 200, loss: 6563.691820\n",
      "iteration 4550 / 8000, samples: 200, loss: 5193.809062\n",
      "iteration 4560 / 8000, samples: 200, loss: 7614.169309\n",
      "iteration 4570 / 8000, samples: 200, loss: 6303.885803\n",
      "iteration 4580 / 8000, samples: 200, loss: 6012.680944\n",
      "iteration 4590 / 8000, samples: 200, loss: 4727.219086\n",
      "iteration 4600 / 8000, samples: 200, loss: 6174.627290\n",
      "iteration 4610 / 8000, samples: 200, loss: 4930.535576\n",
      "iteration 4620 / 8000, samples: 200, loss: 6981.769405\n",
      "iteration 4630 / 8000, samples: 200, loss: 7406.536716\n",
      "iteration 4640 / 8000, samples: 200, loss: 7493.838805\n",
      "iteration 4650 / 8000, samples: 200, loss: 5689.701774\n",
      "iteration 4660 / 8000, samples: 200, loss: 5853.660563\n",
      "iteration 4670 / 8000, samples: 200, loss: 6976.483864\n",
      "iteration 4680 / 8000, samples: 200, loss: 4117.361119\n",
      "iteration 4690 / 8000, samples: 200, loss: 9779.518654\n",
      "iteration 4700 / 8000, samples: 200, loss: 7274.436920\n",
      "iteration 4710 / 8000, samples: 200, loss: 6588.788944\n",
      "iteration 4720 / 8000, samples: 200, loss: 6622.588205\n",
      "iteration 4730 / 8000, samples: 200, loss: 6018.477617\n",
      "iteration 4740 / 8000, samples: 200, loss: 5525.231896\n",
      "iteration 4750 / 8000, samples: 200, loss: 6518.470150\n",
      "iteration 4760 / 8000, samples: 200, loss: 6985.821148\n",
      "iteration 4770 / 8000, samples: 200, loss: 9007.886976\n",
      "iteration 4780 / 8000, samples: 200, loss: 5111.285120\n",
      "iteration 4790 / 8000, samples: 200, loss: 3656.910250\n",
      "iteration 4800 / 8000, samples: 200, loss: 3762.957134\n",
      "iteration 4810 / 8000, samples: 200, loss: 6402.616088\n",
      "iteration 4820 / 8000, samples: 200, loss: 5125.447847\n",
      "iteration 4830 / 8000, samples: 200, loss: 3772.188613\n",
      "iteration 4840 / 8000, samples: 200, loss: 8306.193466\n",
      "iteration 4850 / 8000, samples: 200, loss: 5926.222628\n",
      "iteration 4860 / 8000, samples: 200, loss: 7526.821245\n",
      "iteration 4870 / 8000, samples: 200, loss: 4437.633143\n",
      "iteration 4880 / 8000, samples: 200, loss: 4604.331441\n",
      "iteration 4890 / 8000, samples: 200, loss: 5595.860868\n",
      "iteration 4900 / 8000, samples: 200, loss: 4420.613820\n",
      "iteration 4910 / 8000, samples: 200, loss: 4505.002825\n",
      "iteration 4920 / 8000, samples: 200, loss: 4382.636917\n",
      "iteration 4930 / 8000, samples: 200, loss: 3543.303939\n",
      "iteration 4940 / 8000, samples: 200, loss: 6264.331240\n",
      "iteration 4950 / 8000, samples: 200, loss: 5009.867022\n",
      "iteration 4960 / 8000, samples: 200, loss: 4569.817586\n",
      "iteration 4970 / 8000, samples: 200, loss: 9744.744552\n",
      "iteration 4980 / 8000, samples: 200, loss: 6374.624567\n",
      "iteration 4990 / 8000, samples: 200, loss: 3864.433507\n",
      "iteration 5000 / 8000, samples: 200, loss: 7862.474568\n",
      "iteration 5010 / 8000, samples: 200, loss: 5468.252593\n",
      "iteration 5020 / 8000, samples: 200, loss: 3696.330600\n",
      "iteration 5030 / 8000, samples: 200, loss: 4239.885767\n",
      "iteration 5040 / 8000, samples: 200, loss: 5921.080351\n",
      "iteration 5050 / 8000, samples: 200, loss: 5605.757188\n",
      "iteration 5060 / 8000, samples: 200, loss: 5821.812262\n",
      "iteration 5070 / 8000, samples: 200, loss: 3525.126535\n",
      "iteration 5080 / 8000, samples: 200, loss: 7322.061188\n",
      "iteration 5090 / 8000, samples: 200, loss: 8669.530966\n",
      "iteration 5100 / 8000, samples: 200, loss: 6059.114124\n",
      "iteration 5110 / 8000, samples: 200, loss: 6681.385981\n",
      "iteration 5120 / 8000, samples: 200, loss: 7977.526770\n",
      "iteration 5130 / 8000, samples: 200, loss: 4915.378062\n",
      "iteration 5140 / 8000, samples: 200, loss: 5395.769881\n",
      "iteration 5150 / 8000, samples: 200, loss: 5635.511139\n",
      "iteration 5160 / 8000, samples: 200, loss: 4275.941184\n",
      "iteration 5170 / 8000, samples: 200, loss: 5861.714769\n",
      "iteration 5180 / 8000, samples: 200, loss: 4660.150251\n",
      "iteration 5190 / 8000, samples: 200, loss: 4298.671999\n",
      "iteration 5200 / 8000, samples: 200, loss: 4848.595088\n",
      "iteration 5210 / 8000, samples: 200, loss: 5093.401038\n",
      "iteration 5220 / 8000, samples: 200, loss: 4265.025062\n",
      "iteration 5230 / 8000, samples: 200, loss: 5063.592998\n",
      "iteration 5240 / 8000, samples: 200, loss: 5071.609074\n",
      "iteration 5250 / 8000, samples: 200, loss: 2872.860436\n",
      "iteration 5260 / 8000, samples: 200, loss: 4859.653688\n",
      "iteration 5270 / 8000, samples: 200, loss: 6503.738672\n",
      "iteration 5280 / 8000, samples: 200, loss: 4982.505784\n",
      "iteration 5290 / 8000, samples: 200, loss: 7589.841920\n",
      "iteration 5300 / 8000, samples: 200, loss: 4287.579037\n",
      "iteration 5310 / 8000, samples: 200, loss: 3629.437489\n",
      "iteration 5320 / 8000, samples: 200, loss: 5175.963629\n",
      "iteration 5330 / 8000, samples: 200, loss: 10995.770441\n",
      "iteration 5340 / 8000, samples: 200, loss: 5090.535909\n",
      "iteration 5350 / 8000, samples: 200, loss: 5073.819779\n",
      "iteration 5360 / 8000, samples: 200, loss: 6392.652966\n",
      "iteration 5370 / 8000, samples: 200, loss: 5026.592541\n",
      "iteration 5380 / 8000, samples: 200, loss: 7363.565721\n",
      "iteration 5390 / 8000, samples: 200, loss: 5062.988690\n",
      "iteration 5400 / 8000, samples: 200, loss: 4947.217722\n",
      "iteration 5410 / 8000, samples: 200, loss: 4795.349179\n",
      "iteration 5420 / 8000, samples: 200, loss: 6965.445509\n",
      "iteration 5430 / 8000, samples: 200, loss: 4593.001773\n",
      "iteration 5440 / 8000, samples: 200, loss: 4866.244351\n",
      "iteration 5450 / 8000, samples: 200, loss: 5858.694411\n",
      "iteration 5460 / 8000, samples: 200, loss: 7801.045088\n",
      "iteration 5470 / 8000, samples: 200, loss: 3127.576937\n",
      "iteration 5480 / 8000, samples: 200, loss: 5009.640356\n",
      "iteration 5490 / 8000, samples: 200, loss: 5081.590395\n",
      "iteration 5500 / 8000, samples: 200, loss: 8648.909904\n",
      "iteration 5510 / 8000, samples: 200, loss: 5912.795948\n",
      "iteration 5520 / 8000, samples: 200, loss: 4551.916221\n",
      "iteration 5530 / 8000, samples: 200, loss: 6486.127109\n",
      "iteration 5540 / 8000, samples: 200, loss: 6471.354880\n",
      "iteration 5550 / 8000, samples: 200, loss: 4227.732291\n",
      "iteration 5560 / 8000, samples: 200, loss: 4379.195397\n",
      "iteration 5570 / 8000, samples: 200, loss: 5694.498840\n",
      "iteration 5580 / 8000, samples: 200, loss: 6834.367833\n",
      "iteration 5590 / 8000, samples: 200, loss: 6176.530136\n",
      "iteration 5600 / 8000, samples: 200, loss: 4070.774182\n",
      "iteration 5610 / 8000, samples: 200, loss: 6520.237059\n",
      "iteration 5620 / 8000, samples: 200, loss: 5449.646125\n",
      "iteration 5630 / 8000, samples: 200, loss: 3642.511502\n",
      "iteration 5640 / 8000, samples: 200, loss: 5240.025837\n",
      "iteration 5650 / 8000, samples: 200, loss: 4286.493818\n",
      "iteration 5660 / 8000, samples: 200, loss: 5302.364913\n",
      "iteration 5670 / 8000, samples: 200, loss: 4156.456823\n",
      "iteration 5680 / 8000, samples: 200, loss: 8635.063305\n",
      "iteration 5690 / 8000, samples: 200, loss: 5790.299547\n",
      "iteration 5700 / 8000, samples: 200, loss: 5733.759341\n",
      "iteration 5710 / 8000, samples: 200, loss: 5921.537929\n",
      "iteration 5720 / 8000, samples: 200, loss: 5871.084983\n",
      "iteration 5730 / 8000, samples: 200, loss: 8918.697360\n",
      "iteration 5740 / 8000, samples: 200, loss: 3690.485552\n",
      "iteration 5750 / 8000, samples: 200, loss: 5852.291661\n",
      "iteration 5760 / 8000, samples: 200, loss: 4948.734802\n",
      "iteration 5770 / 8000, samples: 200, loss: 4773.714456\n",
      "iteration 5780 / 8000, samples: 200, loss: 5775.113894\n",
      "iteration 5790 / 8000, samples: 200, loss: 3816.185717\n",
      "iteration 5800 / 8000, samples: 200, loss: 7676.871884\n",
      "iteration 5810 / 8000, samples: 200, loss: 7452.638202\n",
      "iteration 5820 / 8000, samples: 200, loss: 4689.190530\n",
      "iteration 5830 / 8000, samples: 200, loss: 3829.118900\n",
      "iteration 5840 / 8000, samples: 200, loss: 6873.791286\n",
      "iteration 5850 / 8000, samples: 200, loss: 5138.963682\n",
      "iteration 5860 / 8000, samples: 200, loss: 5589.308161\n",
      "iteration 5870 / 8000, samples: 200, loss: 5368.798671\n",
      "iteration 5880 / 8000, samples: 200, loss: 6799.466076\n",
      "iteration 5890 / 8000, samples: 200, loss: 3960.474534\n",
      "iteration 5900 / 8000, samples: 200, loss: 5688.924237\n",
      "iteration 5910 / 8000, samples: 200, loss: 4568.005365\n",
      "iteration 5920 / 8000, samples: 200, loss: 6510.724993\n",
      "iteration 5930 / 8000, samples: 200, loss: 5871.672268\n",
      "iteration 5940 / 8000, samples: 200, loss: 7854.781529\n",
      "iteration 5950 / 8000, samples: 200, loss: 4208.679300\n",
      "iteration 5960 / 8000, samples: 200, loss: 6298.388030\n",
      "iteration 5970 / 8000, samples: 200, loss: 3830.516835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5980 / 8000, samples: 200, loss: 5921.212658\n",
      "iteration 5990 / 8000, samples: 200, loss: 5810.316665\n",
      "iteration 6000 / 8000, samples: 200, loss: 4594.197560\n",
      "iteration 6010 / 8000, samples: 200, loss: 4581.494761\n",
      "iteration 6020 / 8000, samples: 200, loss: 6653.562217\n",
      "iteration 6030 / 8000, samples: 200, loss: 4140.277410\n",
      "iteration 6040 / 8000, samples: 200, loss: 7241.776221\n",
      "iteration 6050 / 8000, samples: 200, loss: 3817.651921\n",
      "iteration 6060 / 8000, samples: 200, loss: 5394.933146\n",
      "iteration 6070 / 8000, samples: 200, loss: 3543.612058\n",
      "iteration 6080 / 8000, samples: 200, loss: 6062.724759\n",
      "iteration 6090 / 8000, samples: 200, loss: 5859.096476\n",
      "iteration 6100 / 8000, samples: 200, loss: 3887.003758\n",
      "iteration 6110 / 8000, samples: 200, loss: 3978.432958\n",
      "iteration 6120 / 8000, samples: 200, loss: 6533.233415\n",
      "iteration 6130 / 8000, samples: 200, loss: 6406.341141\n",
      "iteration 6140 / 8000, samples: 200, loss: 6705.314074\n",
      "iteration 6150 / 8000, samples: 200, loss: 5707.394052\n",
      "iteration 6160 / 8000, samples: 200, loss: 6116.019738\n",
      "iteration 6170 / 8000, samples: 200, loss: 3670.595462\n",
      "iteration 6180 / 8000, samples: 200, loss: 5437.607590\n",
      "iteration 6190 / 8000, samples: 200, loss: 4661.779467\n",
      "iteration 6200 / 8000, samples: 200, loss: 4403.840468\n",
      "iteration 6210 / 8000, samples: 200, loss: 6298.983768\n",
      "iteration 6220 / 8000, samples: 200, loss: 5536.961884\n",
      "iteration 6230 / 8000, samples: 200, loss: 4764.032650\n",
      "iteration 6240 / 8000, samples: 200, loss: 6298.701424\n",
      "iteration 6250 / 8000, samples: 200, loss: 6101.153604\n",
      "iteration 6260 / 8000, samples: 200, loss: 4399.987910\n",
      "iteration 6270 / 8000, samples: 200, loss: 4023.348852\n",
      "iteration 6280 / 8000, samples: 200, loss: 6517.956986\n",
      "iteration 6290 / 8000, samples: 200, loss: 5103.426557\n",
      "iteration 6300 / 8000, samples: 200, loss: 8688.238716\n",
      "iteration 6310 / 8000, samples: 200, loss: 4711.685038\n",
      "iteration 6320 / 8000, samples: 200, loss: 4901.892090\n",
      "iteration 6330 / 8000, samples: 200, loss: 5750.080093\n",
      "iteration 6340 / 8000, samples: 200, loss: 4966.079439\n",
      "iteration 6350 / 8000, samples: 200, loss: 4215.355413\n",
      "iteration 6360 / 8000, samples: 200, loss: 4315.262225\n",
      "iteration 6370 / 8000, samples: 200, loss: 4613.377484\n",
      "iteration 6380 / 8000, samples: 200, loss: 5507.714122\n",
      "iteration 6390 / 8000, samples: 200, loss: 6747.369085\n",
      "iteration 6400 / 8000, samples: 200, loss: 4096.759643\n",
      "iteration 6410 / 8000, samples: 200, loss: 5618.808491\n",
      "iteration 6420 / 8000, samples: 200, loss: 9245.942410\n",
      "iteration 6430 / 8000, samples: 200, loss: 5763.570932\n",
      "iteration 6440 / 8000, samples: 200, loss: 3557.975553\n",
      "iteration 6450 / 8000, samples: 200, loss: 7043.130625\n",
      "iteration 6460 / 8000, samples: 200, loss: 3427.565132\n",
      "iteration 6470 / 8000, samples: 200, loss: 4481.122064\n",
      "iteration 6480 / 8000, samples: 200, loss: 7165.553593\n",
      "iteration 6490 / 8000, samples: 200, loss: 7333.346811\n",
      "iteration 6500 / 8000, samples: 200, loss: 7842.834103\n",
      "iteration 6510 / 8000, samples: 200, loss: 8632.982442\n",
      "iteration 6520 / 8000, samples: 200, loss: 4661.186180\n",
      "iteration 6530 / 8000, samples: 200, loss: 5722.722415\n",
      "iteration 6540 / 8000, samples: 200, loss: 5958.931795\n",
      "iteration 6550 / 8000, samples: 200, loss: 3760.720052\n",
      "iteration 6560 / 8000, samples: 200, loss: 5660.641567\n",
      "iteration 6570 / 8000, samples: 200, loss: 4368.535618\n",
      "iteration 6580 / 8000, samples: 200, loss: 4149.513858\n",
      "iteration 6590 / 8000, samples: 200, loss: 8004.704788\n",
      "iteration 6600 / 8000, samples: 200, loss: 7330.683742\n",
      "iteration 6610 / 8000, samples: 200, loss: 4394.884753\n",
      "iteration 6620 / 8000, samples: 200, loss: 6150.884410\n",
      "iteration 6630 / 8000, samples: 200, loss: 7315.755805\n",
      "iteration 6640 / 8000, samples: 200, loss: 4808.121563\n",
      "iteration 6650 / 8000, samples: 200, loss: 5595.932868\n",
      "iteration 6660 / 8000, samples: 200, loss: 5177.429771\n",
      "iteration 6670 / 8000, samples: 200, loss: 4676.235164\n",
      "iteration 6680 / 8000, samples: 200, loss: 5671.640097\n",
      "iteration 6690 / 8000, samples: 200, loss: 4299.262505\n",
      "iteration 6700 / 8000, samples: 200, loss: 5726.129110\n",
      "iteration 6710 / 8000, samples: 200, loss: 5254.071749\n",
      "iteration 6720 / 8000, samples: 200, loss: 5591.625105\n",
      "iteration 6730 / 8000, samples: 200, loss: 5121.447070\n",
      "iteration 6740 / 8000, samples: 200, loss: 6444.347845\n",
      "iteration 6750 / 8000, samples: 200, loss: 6604.301583\n",
      "iteration 6760 / 8000, samples: 200, loss: 3195.957448\n",
      "iteration 6770 / 8000, samples: 200, loss: 5146.693534\n",
      "iteration 6780 / 8000, samples: 200, loss: 5672.035031\n",
      "iteration 6790 / 8000, samples: 200, loss: 5482.115916\n",
      "iteration 6800 / 8000, samples: 200, loss: 5851.611036\n",
      "iteration 6810 / 8000, samples: 200, loss: 6199.738880\n",
      "iteration 6820 / 8000, samples: 200, loss: 11286.554774\n",
      "iteration 6830 / 8000, samples: 200, loss: 4917.553401\n",
      "iteration 6840 / 8000, samples: 200, loss: 5751.491049\n",
      "iteration 6850 / 8000, samples: 200, loss: 4721.207138\n",
      "iteration 6860 / 8000, samples: 200, loss: 8995.441805\n",
      "iteration 6870 / 8000, samples: 200, loss: 8907.116128\n",
      "iteration 6880 / 8000, samples: 200, loss: 7632.984034\n",
      "iteration 6890 / 8000, samples: 200, loss: 4800.609683\n",
      "iteration 6900 / 8000, samples: 200, loss: 5851.254342\n",
      "iteration 6910 / 8000, samples: 200, loss: 4326.471403\n",
      "iteration 6920 / 8000, samples: 200, loss: 4072.331341\n",
      "iteration 6930 / 8000, samples: 200, loss: 6355.997393\n",
      "iteration 6940 / 8000, samples: 200, loss: 4277.721897\n",
      "iteration 6950 / 8000, samples: 200, loss: 6116.670394\n",
      "iteration 6960 / 8000, samples: 200, loss: 4497.518387\n",
      "iteration 6970 / 8000, samples: 200, loss: 3627.311129\n",
      "iteration 6980 / 8000, samples: 200, loss: 5273.451530\n",
      "iteration 6990 / 8000, samples: 200, loss: 5434.848953\n",
      "iteration 7000 / 8000, samples: 200, loss: 4978.198213\n",
      "iteration 7010 / 8000, samples: 200, loss: 5363.474905\n",
      "iteration 7020 / 8000, samples: 200, loss: 4564.133076\n",
      "iteration 7030 / 8000, samples: 200, loss: 3910.438566\n",
      "iteration 7040 / 8000, samples: 200, loss: 4960.989174\n",
      "iteration 7050 / 8000, samples: 200, loss: 7026.394857\n",
      "iteration 7060 / 8000, samples: 200, loss: 4777.341997\n",
      "iteration 7070 / 8000, samples: 200, loss: 8832.025227\n",
      "iteration 7080 / 8000, samples: 200, loss: 5574.037138\n",
      "iteration 7090 / 8000, samples: 200, loss: 7509.566258\n",
      "iteration 7100 / 8000, samples: 200, loss: 7223.239912\n",
      "iteration 7110 / 8000, samples: 200, loss: 2575.953575\n",
      "iteration 7120 / 8000, samples: 200, loss: 5442.937919\n",
      "iteration 7130 / 8000, samples: 200, loss: 4832.701558\n",
      "iteration 7140 / 8000, samples: 200, loss: 5300.865362\n",
      "iteration 7150 / 8000, samples: 200, loss: 4148.196911\n",
      "iteration 7160 / 8000, samples: 200, loss: 7816.503107\n",
      "iteration 7170 / 8000, samples: 200, loss: 4638.837314\n",
      "iteration 7180 / 8000, samples: 200, loss: 5099.382240\n",
      "iteration 7190 / 8000, samples: 200, loss: 4060.214417\n",
      "iteration 7200 / 8000, samples: 200, loss: 3779.486540\n",
      "iteration 7210 / 8000, samples: 200, loss: 5123.964881\n",
      "iteration 7220 / 8000, samples: 200, loss: 3885.583215\n",
      "iteration 7230 / 8000, samples: 200, loss: 5967.659970\n",
      "iteration 7240 / 8000, samples: 200, loss: 6656.183110\n",
      "iteration 7250 / 8000, samples: 200, loss: 5367.960877\n",
      "iteration 7260 / 8000, samples: 200, loss: 8723.538353\n",
      "iteration 7270 / 8000, samples: 200, loss: 3801.724685\n",
      "iteration 7280 / 8000, samples: 200, loss: 6141.544715\n",
      "iteration 7290 / 8000, samples: 200, loss: 7523.949729\n",
      "iteration 7300 / 8000, samples: 200, loss: 4977.007420\n",
      "iteration 7310 / 8000, samples: 200, loss: 4415.603658\n",
      "iteration 7320 / 8000, samples: 200, loss: 7388.607613\n",
      "iteration 7330 / 8000, samples: 200, loss: 4249.429700\n",
      "iteration 7340 / 8000, samples: 200, loss: 10845.538659\n",
      "iteration 7350 / 8000, samples: 200, loss: 7551.503531\n",
      "iteration 7360 / 8000, samples: 200, loss: 9214.841440\n",
      "iteration 7370 / 8000, samples: 200, loss: 4861.812549\n",
      "iteration 7380 / 8000, samples: 200, loss: 4081.814402\n",
      "iteration 7390 / 8000, samples: 200, loss: 5284.443954\n",
      "iteration 7400 / 8000, samples: 200, loss: 7772.117678\n",
      "iteration 7410 / 8000, samples: 200, loss: 8444.756899\n",
      "iteration 7420 / 8000, samples: 200, loss: 6678.576876\n",
      "iteration 7430 / 8000, samples: 200, loss: 5061.301431\n",
      "iteration 7440 / 8000, samples: 200, loss: 6142.230957\n",
      "iteration 7450 / 8000, samples: 200, loss: 5611.504878\n",
      "iteration 7460 / 8000, samples: 200, loss: 6674.462413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7470 / 8000, samples: 200, loss: 4883.571387\n",
      "iteration 7480 / 8000, samples: 200, loss: 5457.763132\n",
      "iteration 7490 / 8000, samples: 200, loss: 5834.362031\n",
      "iteration 7500 / 8000, samples: 200, loss: 4644.752470\n",
      "iteration 7510 / 8000, samples: 200, loss: 6061.524176\n",
      "iteration 7520 / 8000, samples: 200, loss: 5105.913802\n",
      "iteration 7530 / 8000, samples: 200, loss: 6709.325092\n",
      "iteration 7540 / 8000, samples: 200, loss: 4264.911792\n",
      "iteration 7550 / 8000, samples: 200, loss: 4387.266900\n",
      "iteration 7560 / 8000, samples: 200, loss: 5596.831950\n",
      "iteration 7570 / 8000, samples: 200, loss: 6179.258345\n",
      "iteration 7580 / 8000, samples: 200, loss: 5989.904380\n",
      "iteration 7590 / 8000, samples: 200, loss: 4271.106230\n",
      "iteration 7600 / 8000, samples: 200, loss: 4363.402325\n",
      "iteration 7610 / 8000, samples: 200, loss: 4952.695210\n",
      "iteration 7620 / 8000, samples: 200, loss: 5157.540779\n",
      "iteration 7630 / 8000, samples: 200, loss: 5451.920061\n",
      "iteration 7640 / 8000, samples: 200, loss: 3282.001379\n",
      "iteration 7650 / 8000, samples: 200, loss: 4342.712535\n",
      "iteration 7660 / 8000, samples: 200, loss: 6155.255189\n",
      "iteration 7670 / 8000, samples: 200, loss: 4830.424370\n",
      "iteration 7680 / 8000, samples: 200, loss: 4479.911101\n",
      "iteration 7690 / 8000, samples: 200, loss: 6498.127552\n",
      "iteration 7700 / 8000, samples: 200, loss: 6131.131758\n",
      "iteration 7710 / 8000, samples: 200, loss: 7177.913398\n",
      "iteration 7720 / 8000, samples: 200, loss: 6060.630496\n",
      "iteration 7730 / 8000, samples: 200, loss: 5186.107151\n",
      "iteration 7740 / 8000, samples: 200, loss: 6000.652821\n",
      "iteration 7750 / 8000, samples: 200, loss: 5192.014575\n",
      "iteration 7760 / 8000, samples: 200, loss: 7172.649631\n",
      "iteration 7770 / 8000, samples: 200, loss: 4016.643808\n",
      "iteration 7780 / 8000, samples: 200, loss: 5474.701063\n",
      "iteration 7790 / 8000, samples: 200, loss: 4270.058681\n",
      "iteration 7800 / 8000, samples: 200, loss: 4941.263363\n",
      "iteration 7810 / 8000, samples: 200, loss: 5085.675091\n",
      "iteration 7820 / 8000, samples: 200, loss: 7192.471936\n",
      "iteration 7830 / 8000, samples: 200, loss: 5232.861616\n",
      "iteration 7840 / 8000, samples: 200, loss: 2872.888102\n",
      "iteration 7850 / 8000, samples: 200, loss: 6571.590936\n",
      "iteration 7860 / 8000, samples: 200, loss: 7006.526857\n",
      "iteration 7870 / 8000, samples: 200, loss: 3743.766365\n",
      "iteration 7880 / 8000, samples: 200, loss: 7052.180791\n",
      "iteration 7890 / 8000, samples: 200, loss: 7589.324218\n",
      "iteration 7900 / 8000, samples: 200, loss: 5271.195603\n",
      "iteration 7910 / 8000, samples: 200, loss: 3488.112386\n",
      "iteration 7920 / 8000, samples: 200, loss: 6196.044596\n",
      "iteration 7930 / 8000, samples: 200, loss: 4027.531370\n",
      "iteration 7940 / 8000, samples: 200, loss: 3461.585523\n",
      "iteration 7950 / 8000, samples: 200, loss: 8384.963580\n",
      "iteration 7960 / 8000, samples: 200, loss: 7652.714388\n",
      "iteration 7970 / 8000, samples: 200, loss: 7323.601631\n",
      "iteration 7980 / 8000, samples: 200, loss: 6484.816419\n",
      "iteration 7990 / 8000, samples: 200, loss: 6473.496732\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from PIL import Image\n",
    "#开始训练\n",
    "file_path = './'\n",
    "\n",
    "dataTrFirst, labelTrain, dataTsFirst, labelTest = load_file(file_path)\n",
    "\n",
    "dataTr = np.zeros((dataTrFirst.shape[0],32*32))\n",
    "dataTs = np.zeros((dataTsFirst.shape[0],32*32))\n",
    "\n",
    "\n",
    "for i in range(dataTrFirst.shape[0] -45000):\n",
    "    img = dataTrFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTr[i] = res.reshape((1,32*32))\n",
    "print(\"训练集加载完成\")\n",
    "\n",
    "for i in range(dataTsFirst.shape[0] -1):\n",
    "    img = dataTsFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTs[i] = res.reshape((1,32*32))\n",
    "print(\"测试集加载完成\")\n",
    "\n",
    "dataTrain = dataTrain - np.mean(dataTrain, axis=0)\n",
    "\n",
    "LC = LinearClassifier()\n",
    "\n",
    "print('start training ...')\n",
    "#train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True)\n",
    "#在dataTrain中不重复随机抽取batch_size个样本，迭代训练num_iters次\n",
    "loss_all = LC.train(dataTrain, labelTrain, num_iters = 8000, batch_size = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss is 5050.960131\n",
      "start predicting ...\n",
      "the accuracy rate is 25.000000 \n"
     ]
    }
   ],
   "source": [
    "print('last loss is %f' %(loss_all[-1]))\n",
    "#开始预测\n",
    "print('start predicting ...')\n",
    "y_pred = LC.predict(dataTest)\n",
    "\n",
    "hit = 0\n",
    "for i in xrange(y_pred.size):\n",
    "\tif (y_pred[i] == labelTest[i]):\n",
    "\t\thit += 1\n",
    "\n",
    "print('the accuracy rate is %f ' % (hit/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
