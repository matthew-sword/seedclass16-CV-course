{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "\n",
    "import pickle\n",
    "from past.builtins import xrange\n",
    "\n",
    "\n",
    "'''\n",
    "文件读取和数据获取\n",
    "'''\n",
    "def unpickle(file):\n",
    "  with open(file,'rb') as fo:\n",
    "    dict=pickle.load(fo)\n",
    "  return dict\n",
    "\n",
    "\n",
    "\n",
    "def load_cifar10(file):\n",
    "    dictTrain = unpickle(file + \"data_batch_1\")\n",
    "    dataTrain = dictTrain['data']\n",
    "    labelTrain = dictTrain['labels']\n",
    "\n",
    "    for i in range(2,6):\n",
    "        dictTrain = unpickle(file+\"data_batch_\"+str(i))\n",
    "        dataTrain = np.vstack([dataTrain, dictTrain['data']])\n",
    "        labelTrain = np.hstack([labelTrain, dictTrain['labels']])\n",
    "\n",
    "    dictTest = unpickle(file + \"test_batch\")\n",
    "    dataTest = dictTest['data']\n",
    "    labelTest = dictTest['labels']\n",
    "    labelTest = np.array(labelTest)\n",
    "\n",
    "    return dataTrain, labelTrain, dataTest, labelTest\n",
    "\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "    Inputs:\n",
    "    - W: 权重矩阵\n",
    "    - X: 图片样本数据集(矩阵)\n",
    "    - y: 训练图片的标签\n",
    "    - reg: (float) regularization strength\n",
    "    Returns a tuple of:\n",
    "    - loss 损失函数值\n",
    "    - dW矩阵\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W) #初始化dW矩阵\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "    num_train=X.shape[0]\n",
    "    num_class=W.shape[1]    #标签数\n",
    "\n",
    "    for i in xrange(num_train):\n",
    "        score = X[i].dot(W) #分类器预测结果\n",
    "        score-=np.max(score)    #提高计算中的数值稳定性\n",
    "\n",
    "        correct_score = score[y[i]]   #取分类正确的评分值\n",
    "        exp_sum=np.sum(np.exp(score))\n",
    "\n",
    "        loss+=np.log(exp_sum)-correct_score #计算一张图片的loss值\n",
    "\n",
    "        for j in xrange(num_class):\n",
    "            \n",
    "            if j==y[i]: #图片标签\n",
    "                dW[:,j]+=np.exp(score[j])/exp_sum*X[i]-X[i]\n",
    "            else:\n",
    "                dW[:,j]+=np.exp(score[j])/exp_sum*X[i]\n",
    "\n",
    "    #平均loss            \n",
    "    loss/=num_train #一个训练集合平均loss\n",
    "    loss+=0.5*reg*np.sum(W*W)   #对W中元素平方后求和，计算L\n",
    "    dW/=num_train\n",
    "    dW+=reg*W\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "    return loss, dW\n",
    "\n",
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "\n",
    "  #############################################################################\n",
    "    num_train=X.shape[0] \n",
    "\n",
    "    score = X.dot(W)\n",
    "    score -= np.max(score, axis = 1)[:, np.newaxis]    #axis = 1每一行的最大值，score仍为500*10\n",
    "\n",
    "    correct_score=score[range(num_train), y]    #correct_score变为500*1\n",
    "    exp_score = np.exp(score)\n",
    "    sum_exp_score = np.sum(exp_score, axis = 1)    #sum_exp_score为500*1\n",
    "\n",
    "    loss = np.sum(np.log(sum_exp_score)) - np.sum(correct_score)\n",
    "    exp_score /= sum_exp_score[:,np.newaxis]  #exp_score为500*10\n",
    "\n",
    "    for i in xrange(num_train):\n",
    "        dW += exp_score[i] * X[i][:,np.newaxis]   # X[i][:,np.newaxis]将X[i]增加一列纬度\n",
    "        dW[:, y[i]] -= X[i]\n",
    "\n",
    "    loss/=num_train\n",
    "    loss+=0.5*reg*np.sum(W*W)\n",
    "    dW/=num_train\n",
    "    dW+=reg*W\n",
    "    \n",
    "\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "    return loss, dW\n",
    "\n",
    "\n",
    "class LinearClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=1000,batch_size=64, verbose=True):\n",
    "        ####随机梯度下降\n",
    "        \"\"\"\n",
    "        Train this linear classifier using stochastic gradient descent.\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "        training samples each of dimension D.\n",
    "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "        means that X[i] has label 0 <= c < C for C classes.\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - reg: (float) regularization strength.\n",
    "        - num_iters: (integer) number of steps to take when optimizing\n",
    "        - batch_size: (integer) number of training examples to use at each step.\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            # lazily initialize W\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)  #生成随机矩阵\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in xrange(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Sample batch_size elements from the training data and their           #\n",
    "        # corresponding labels to use in this round of gradient descent.        #\n",
    "        # Store the data in X_batch and their corresponding labels in           #\n",
    "        # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
    "        # and y_batch should have shape (batch_size,)                           #\n",
    "        #                                                                       #\n",
    "        # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
    "        # replacement is faster than sampling without replacement.              #\n",
    "        #########################################################################\n",
    "            sample_index = np.random.choice(num_train, batch_size, replace=False)\n",
    "            X_batch = X[sample_index, :]   # select the batch sample\n",
    "            y_batch = y[sample_index]      # select the batch label     \n",
    "        #########################################################################\n",
    "        #                       END OF YOUR CODE                                #\n",
    "        #########################################################################\n",
    "        # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg) #获取loss数值\n",
    "            loss_history.append(loss) #把loss添加到末尾\n",
    "        # perform parameter update\n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Update the weights using the gradient and the learning rate.          #\n",
    "        #########################################################################\n",
    "        # perform parameter update\n",
    "            self.W += -learning_rate * grad    \n",
    "        #########################################################################\n",
    "        #                       END OF YOUR CODE                                #\n",
    "        #########################################################################\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[1])\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Implement this method. Store the predicted labels in y_pred.            #\n",
    "        ###########################################################################\n",
    "        score = X.dot(self.W)\n",
    "        y_pred = np.argmax(score,axis=1)\n",
    "        ###########################################################################\n",
    "        #                           END OF YOUR CODE                              #\n",
    "        ###########################################################################\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative. \n",
    "        Subclasses will override this.\n",
    "        \n",
    "        Inputs:\n",
    "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
    "        data points; each point has dimension D.\n",
    "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "        - reg: (float) regularization strength\n",
    "        \n",
    "        Returns: A tuple containing:\n",
    "        - loss as a single float\n",
    "        - gradient with respect to self.W; an array of the same shape as W\n",
    "        \"\"\"\n",
    "        return softmax_loss_naive(self.W, X_batch, y_batch, reg)\n",
    "\n",
    "class Softmax(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return softmax_loss_naive(self.W, X_batch, y_batch, reg)\n",
    "        #return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "训练开始\n",
    "'''\n",
    "\n",
    "file_path = \"./\"\n",
    "\n",
    "#获取数据集\n",
    "dataTrain1, labelTrain1, dataTest1, labelTest1 = load_cifar10(file_path)\n",
    "dataTrain1 = dataTrain1 - np.mean(dataTrain1, axis=0)\n",
    "print(dataTrain1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "iteration 0 / 1000: loss 5.546904\n",
      "iteration 100 / 1000: loss 77.025339\n",
      "iteration 200 / 1000: loss 14.170819\n",
      "iteration 300 / 1000: loss 0.000108\n",
      "iteration 400 / 1000: loss 0.000108\n",
      "iteration 500 / 1000: loss 0.000108\n",
      "iteration 600 / 1000: loss 0.000108\n",
      "iteration 700 / 1000: loss 0.000108\n",
      "iteration 800 / 1000: loss 0.000108\n",
      "iteration 900 / 1000: loss 0.000109\n",
      "trainning 1 times \n",
      "\n",
      "iteration 0 / 1000: loss 131.185026\n",
      "iteration 100 / 1000: loss 92.095410\n",
      "iteration 200 / 1000: loss 34.485935\n",
      "iteration 300 / 1000: loss 17.329344\n",
      "iteration 400 / 1000: loss 29.738989\n",
      "iteration 500 / 1000: loss 3.536845\n",
      "iteration 600 / 1000: loss 10.412464\n",
      "iteration 700 / 1000: loss 6.997282\n",
      "iteration 800 / 1000: loss 1.603583\n",
      "iteration 900 / 1000: loss 0.000383\n",
      "trainning 2 times \n",
      "\n",
      "iteration 0 / 1000: loss 130.254421\n",
      "iteration 100 / 1000: loss 64.105502\n",
      "iteration 200 / 1000: loss 60.580995\n",
      "iteration 300 / 1000: loss 22.445744\n",
      "iteration 400 / 1000: loss 32.718385\n",
      "iteration 500 / 1000: loss 41.861436\n",
      "iteration 600 / 1000: loss 39.440094\n",
      "iteration 700 / 1000: loss 2.878348\n",
      "iteration 800 / 1000: loss 10.028444\n",
      "iteration 900 / 1000: loss 10.856002\n",
      "trainning 3 times \n",
      "\n",
      "iteration 0 / 1000: loss 86.885038\n",
      "iteration 100 / 1000: loss 161.329173\n",
      "iteration 200 / 1000: loss 161.015439\n",
      "iteration 300 / 1000: loss 85.408150\n",
      "iteration 400 / 1000: loss 62.265720\n",
      "iteration 500 / 1000: loss 40.038397\n",
      "iteration 600 / 1000: loss 175.567411\n",
      "iteration 700 / 1000: loss 38.874423\n",
      "iteration 800 / 1000: loss 37.578941\n",
      "iteration 900 / 1000: loss 18.254174\n",
      "trainning 4 times \n",
      "\n",
      "iteration 0 / 1000: loss 181.541325\n",
      "iteration 100 / 1000: loss 71.215821\n",
      "iteration 200 / 1000: loss 178.343040\n",
      "iteration 300 / 1000: loss 157.472854\n",
      "iteration 400 / 1000: loss 78.928296\n",
      "iteration 500 / 1000: loss 190.306169\n",
      "iteration 600 / 1000: loss 76.624304\n",
      "iteration 700 / 1000: loss 56.624223\n",
      "iteration 800 / 1000: loss 79.791895\n",
      "iteration 900 / 1000: loss 47.003334\n",
      "trainning 5 times \n",
      "\n",
      "iteration 0 / 1000: loss 79.934269\n",
      "iteration 100 / 1000: loss 79.560781\n",
      "iteration 200 / 1000: loss 107.033194\n",
      "iteration 300 / 1000: loss 72.282408\n",
      "iteration 400 / 1000: loss 110.289639\n",
      "iteration 500 / 1000: loss 100.128183\n",
      "iteration 600 / 1000: loss 62.217734\n",
      "iteration 700 / 1000: loss 33.060821\n",
      "iteration 800 / 1000: loss 68.885596\n",
      "iteration 900 / 1000: loss 35.012816\n",
      "trainning 6 times \n",
      "\n",
      "iteration 0 / 1000: loss 113.231904\n",
      "iteration 100 / 1000: loss 201.795102\n",
      "iteration 200 / 1000: loss 83.077677\n",
      "iteration 300 / 1000: loss 160.647285\n",
      "iteration 400 / 1000: loss 89.867046\n",
      "iteration 500 / 1000: loss 231.949048\n",
      "iteration 600 / 1000: loss 95.330962\n",
      "iteration 700 / 1000: loss 48.040764\n",
      "iteration 800 / 1000: loss 114.493240\n",
      "iteration 900 / 1000: loss 102.229252\n",
      "trainning 7 times \n",
      "\n",
      "iteration 0 / 1000: loss 158.127852\n",
      "iteration 100 / 1000: loss 128.411147\n",
      "iteration 200 / 1000: loss 106.519812\n",
      "iteration 300 / 1000: loss 108.197848\n",
      "iteration 400 / 1000: loss 97.886397\n",
      "iteration 500 / 1000: loss 187.606481\n",
      "iteration 600 / 1000: loss 92.788942\n",
      "iteration 700 / 1000: loss 92.441351\n",
      "iteration 800 / 1000: loss 94.323714\n",
      "iteration 900 / 1000: loss 173.956949\n",
      "trainning 8 times \n",
      "\n",
      "iteration 0 / 1000: loss 120.779095\n",
      "iteration 100 / 1000: loss 67.456349\n",
      "iteration 200 / 1000: loss 111.682289\n",
      "iteration 300 / 1000: loss 173.480601\n",
      "iteration 400 / 1000: loss 170.888117\n",
      "iteration 500 / 1000: loss 184.081743\n",
      "iteration 600 / 1000: loss 109.824331\n",
      "iteration 700 / 1000: loss 132.635479\n",
      "iteration 800 / 1000: loss 227.442147\n",
      "iteration 900 / 1000: loss 126.584446\n",
      "trainning 9 times \n",
      "\n",
      "iteration 0 / 1000: loss 139.141924\n",
      "iteration 100 / 1000: loss 220.752228\n",
      "iteration 200 / 1000: loss 148.074519\n",
      "iteration 300 / 1000: loss 108.988220\n",
      "iteration 400 / 1000: loss 127.379135\n",
      "iteration 500 / 1000: loss 124.128102\n",
      "iteration 600 / 1000: loss 62.898759\n",
      "iteration 700 / 1000: loss 132.119375\n",
      "iteration 800 / 1000: loss 69.967088\n",
      "iteration 900 / 1000: loss 134.506095\n",
      "trainning 10 times \n",
      "\n",
      "iteration 0 / 1000: loss 172.968851\n",
      "iteration 100 / 1000: loss 139.366384\n",
      "iteration 200 / 1000: loss 98.033933\n",
      "iteration 300 / 1000: loss 90.703726\n",
      "iteration 400 / 1000: loss 155.761659\n",
      "iteration 500 / 1000: loss 129.055061\n",
      "iteration 600 / 1000: loss 260.949648\n",
      "iteration 700 / 1000: loss 83.806299\n",
      "iteration 800 / 1000: loss 88.099129\n",
      "iteration 900 / 1000: loss 85.268722\n",
      "trainning 11 times \n",
      "\n",
      "iteration 0 / 1000: loss 99.045274\n",
      "iteration 100 / 1000: loss 113.310311\n",
      "iteration 200 / 1000: loss 105.169278\n",
      "iteration 300 / 1000: loss 137.044172\n",
      "iteration 400 / 1000: loss 134.853416\n",
      "iteration 500 / 1000: loss 123.288262\n",
      "iteration 600 / 1000: loss 75.230190\n",
      "iteration 700 / 1000: loss 104.755314\n",
      "iteration 800 / 1000: loss 106.192761\n",
      "iteration 900 / 1000: loss 139.358376\n",
      "trainning 12 times \n",
      "\n",
      "iteration 0 / 1000: loss 122.058755\n",
      "iteration 100 / 1000: loss 108.798613\n",
      "iteration 200 / 1000: loss 150.451901\n",
      "iteration 300 / 1000: loss 169.658107\n",
      "iteration 400 / 1000: loss 244.903755\n",
      "iteration 500 / 1000: loss 99.824211\n",
      "iteration 600 / 1000: loss 198.268988\n",
      "iteration 700 / 1000: loss 99.318069\n",
      "iteration 800 / 1000: loss 145.247975\n",
      "iteration 900 / 1000: loss 135.327294\n",
      "trainning 13 times \n",
      "\n",
      "iteration 0 / 1000: loss 98.339681\n",
      "iteration 100 / 1000: loss 160.466728\n",
      "iteration 200 / 1000: loss 118.451526\n",
      "iteration 300 / 1000: loss 140.075827\n",
      "iteration 400 / 1000: loss 167.611512\n",
      "iteration 500 / 1000: loss 136.196985\n",
      "iteration 600 / 1000: loss 136.414019\n",
      "iteration 700 / 1000: loss 120.479421\n",
      "iteration 800 / 1000: loss 159.101778\n",
      "iteration 900 / 1000: loss 96.902804\n",
      "trainning 14 times \n",
      "\n",
      "iteration 0 / 1000: loss 107.420479\n",
      "iteration 100 / 1000: loss 168.431591\n",
      "iteration 200 / 1000: loss 108.820348\n",
      "iteration 300 / 1000: loss 253.421281\n",
      "iteration 400 / 1000: loss 206.365356\n",
      "iteration 500 / 1000: loss 138.877874\n",
      "iteration 600 / 1000: loss 135.499671\n",
      "iteration 700 / 1000: loss 145.435571\n",
      "iteration 800 / 1000: loss 76.403820\n",
      "iteration 900 / 1000: loss 206.896916\n",
      "trainning 15 times \n",
      "\n",
      "iteration 0 / 1000: loss 133.540889\n",
      "iteration 100 / 1000: loss 186.255039\n",
      "iteration 200 / 1000: loss 215.158333\n",
      "iteration 300 / 1000: loss 222.874831\n",
      "iteration 400 / 1000: loss 301.886534\n",
      "iteration 500 / 1000: loss 175.200335\n",
      "iteration 600 / 1000: loss 187.066153\n",
      "iteration 700 / 1000: loss 123.436397\n",
      "iteration 800 / 1000: loss 172.522356\n",
      "iteration 900 / 1000: loss 233.772294\n",
      "trainning 16 times \n",
      "\n",
      "iteration 0 / 1000: loss 176.463562\n",
      "iteration 100 / 1000: loss 215.646657\n",
      "iteration 200 / 1000: loss 504.304688\n",
      "iteration 300 / 1000: loss 176.532468\n",
      "iteration 400 / 1000: loss 237.288479\n",
      "iteration 500 / 1000: loss 105.535058\n",
      "iteration 600 / 1000: loss 128.591705\n",
      "iteration 700 / 1000: loss 129.010490\n",
      "iteration 800 / 1000: loss 160.226368\n",
      "iteration 900 / 1000: loss 293.329853\n",
      "trainning 17 times \n",
      "\n",
      "iteration 0 / 1000: loss 232.127730\n",
      "iteration 100 / 1000: loss 204.168004\n",
      "iteration 200 / 1000: loss 222.912378\n",
      "iteration 300 / 1000: loss 147.372684\n",
      "iteration 400 / 1000: loss 178.347476\n",
      "iteration 500 / 1000: loss 282.067863\n",
      "iteration 600 / 1000: loss 156.890789\n",
      "iteration 700 / 1000: loss 259.765848\n",
      "iteration 800 / 1000: loss 169.685132\n",
      "iteration 900 / 1000: loss 148.491507\n",
      "trainning 18 times \n",
      "\n",
      "iteration 0 / 1000: loss 137.644532\n",
      "iteration 100 / 1000: loss 161.829870\n",
      "iteration 200 / 1000: loss 177.207457\n",
      "iteration 300 / 1000: loss 314.571427\n",
      "iteration 400 / 1000: loss 166.578562\n",
      "iteration 500 / 1000: loss 195.340113\n",
      "iteration 600 / 1000: loss 230.540465\n",
      "iteration 700 / 1000: loss 194.605241\n",
      "iteration 800 / 1000: loss 178.675730\n",
      "iteration 900 / 1000: loss 187.270168\n",
      "trainning 19 times \n",
      "\n",
      "iteration 0 / 1000: loss 127.095015\n",
      "iteration 100 / 1000: loss 149.619205\n",
      "iteration 200 / 1000: loss 165.231860\n",
      "iteration 300 / 1000: loss 208.613702\n",
      "iteration 400 / 1000: loss 144.358615\n",
      "iteration 500 / 1000: loss 195.162952\n",
      "iteration 600 / 1000: loss 121.247444\n",
      "iteration 700 / 1000: loss 150.999344\n",
      "iteration 800 / 1000: loss 313.077873\n",
      "iteration 900 / 1000: loss 146.287906\n",
      "trainning 20 times \n",
      "\n",
      "iteration 0 / 1000: loss 171.496406\n",
      "iteration 100 / 1000: loss 125.110445\n",
      "iteration 200 / 1000: loss 290.400763\n",
      "iteration 300 / 1000: loss 240.284447\n",
      "iteration 400 / 1000: loss 159.634309\n",
      "iteration 500 / 1000: loss 148.249191\n",
      "iteration 600 / 1000: loss 251.791396\n",
      "iteration 700 / 1000: loss 229.601699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1000: loss 134.554205\n",
      "iteration 900 / 1000: loss 134.085945\n",
      "trainning 21 times \n",
      "\n",
      "iteration 0 / 1000: loss 313.516257\n",
      "iteration 100 / 1000: loss 258.828571\n",
      "iteration 200 / 1000: loss 204.425233\n",
      "iteration 300 / 1000: loss 149.102617\n",
      "iteration 400 / 1000: loss 150.263201\n",
      "iteration 500 / 1000: loss 348.793621\n",
      "iteration 600 / 1000: loss 152.764922\n",
      "iteration 700 / 1000: loss 174.803053\n",
      "iteration 800 / 1000: loss 193.252389\n",
      "iteration 900 / 1000: loss 144.368820\n",
      "trainning 22 times \n",
      "\n",
      "iteration 0 / 1000: loss 171.490452\n",
      "iteration 100 / 1000: loss 160.055415\n",
      "iteration 200 / 1000: loss 268.231494\n",
      "iteration 300 / 1000: loss 280.608978\n",
      "iteration 400 / 1000: loss 170.076657\n",
      "iteration 500 / 1000: loss 251.820471\n",
      "iteration 600 / 1000: loss 113.708602\n",
      "iteration 700 / 1000: loss 269.804878\n",
      "iteration 800 / 1000: loss 85.267288\n",
      "iteration 900 / 1000: loss 147.648867\n",
      "trainning 23 times \n",
      "\n",
      "iteration 0 / 1000: loss 212.441299\n",
      "iteration 100 / 1000: loss 180.289313\n",
      "iteration 200 / 1000: loss 195.806378\n",
      "iteration 300 / 1000: loss 191.330319\n",
      "iteration 400 / 1000: loss 341.990905\n",
      "iteration 500 / 1000: loss 290.127447\n",
      "iteration 600 / 1000: loss 271.161053\n",
      "iteration 700 / 1000: loss 249.117117\n",
      "iteration 800 / 1000: loss 161.816739\n",
      "iteration 900 / 1000: loss 287.570493\n",
      "trainning 24 times \n",
      "\n",
      "iteration 0 / 1000: loss 215.920479\n",
      "iteration 100 / 1000: loss 248.399540\n",
      "iteration 200 / 1000: loss 149.500020\n",
      "iteration 300 / 1000: loss 359.263677\n",
      "iteration 400 / 1000: loss 132.317519\n",
      "iteration 500 / 1000: loss 212.197938\n",
      "iteration 600 / 1000: loss 239.673329\n",
      "iteration 700 / 1000: loss 307.991414\n",
      "iteration 800 / 1000: loss 120.622496\n",
      "iteration 900 / 1000: loss 176.326884\n",
      "trainning 25 times \n",
      "\n",
      "iteration 0 / 1000: loss 322.143959\n",
      "iteration 100 / 1000: loss 145.520555\n",
      "iteration 200 / 1000: loss 289.067466\n",
      "iteration 300 / 1000: loss 117.676287\n",
      "iteration 400 / 1000: loss 252.157465\n",
      "iteration 500 / 1000: loss 155.865886\n",
      "iteration 600 / 1000: loss 123.879150\n",
      "iteration 700 / 1000: loss 209.359496\n",
      "iteration 800 / 1000: loss 125.490656\n",
      "iteration 900 / 1000: loss 190.978660\n",
      "trainning 26 times \n",
      "\n",
      "iteration 0 / 1000: loss 165.367816\n",
      "iteration 100 / 1000: loss 294.048116\n",
      "iteration 200 / 1000: loss 205.841366\n",
      "iteration 300 / 1000: loss 295.338070\n",
      "iteration 400 / 1000: loss 370.562785\n",
      "iteration 500 / 1000: loss 266.936183\n",
      "iteration 600 / 1000: loss 202.145867\n",
      "iteration 700 / 1000: loss 204.525739\n",
      "iteration 800 / 1000: loss 123.606693\n",
      "iteration 900 / 1000: loss 271.856046\n",
      "trainning 27 times \n",
      "\n",
      "iteration 0 / 1000: loss 244.259595\n",
      "iteration 100 / 1000: loss 274.074763\n",
      "iteration 200 / 1000: loss 227.414724\n",
      "iteration 300 / 1000: loss 210.405498\n",
      "iteration 400 / 1000: loss 230.505314\n",
      "iteration 500 / 1000: loss 236.914821\n",
      "iteration 600 / 1000: loss 146.469850\n",
      "iteration 700 / 1000: loss 232.575359\n",
      "iteration 800 / 1000: loss 208.655809\n",
      "iteration 900 / 1000: loss 147.706479\n",
      "trainning 28 times \n",
      "\n",
      "iteration 0 / 1000: loss 233.398404\n",
      "iteration 100 / 1000: loss 153.904907\n",
      "iteration 200 / 1000: loss 188.170079\n",
      "iteration 300 / 1000: loss 297.492565\n",
      "iteration 400 / 1000: loss 195.734642\n",
      "iteration 500 / 1000: loss 307.111070\n",
      "iteration 600 / 1000: loss 192.915465\n",
      "iteration 700 / 1000: loss 160.380272\n",
      "iteration 800 / 1000: loss 276.433751\n",
      "iteration 900 / 1000: loss 123.560839\n",
      "trainning 29 times \n",
      "\n",
      "iteration 0 / 1000: loss 151.452440\n",
      "iteration 100 / 1000: loss 145.248362\n",
      "iteration 200 / 1000: loss 217.143174\n",
      "iteration 300 / 1000: loss 437.515019\n",
      "iteration 400 / 1000: loss 218.747709\n",
      "iteration 500 / 1000: loss 213.542678\n",
      "iteration 600 / 1000: loss 190.282795\n",
      "iteration 700 / 1000: loss 193.951813\n",
      "iteration 800 / 1000: loss 263.891945\n",
      "iteration 900 / 1000: loss 238.091560\n",
      "trainning 30 times \n",
      "\n",
      "iteration 0 / 1000: loss 222.922377\n",
      "iteration 100 / 1000: loss 176.751241\n",
      "iteration 200 / 1000: loss 262.303338\n",
      "iteration 300 / 1000: loss 396.325137\n",
      "iteration 400 / 1000: loss 209.452295\n",
      "iteration 500 / 1000: loss 206.546367\n",
      "iteration 600 / 1000: loss 339.590276\n",
      "iteration 700 / 1000: loss 305.482069\n",
      "iteration 800 / 1000: loss 255.941894\n",
      "iteration 900 / 1000: loss 214.853415\n",
      "trainning 31 times \n",
      "\n",
      "iteration 0 / 1000: loss 186.216348\n",
      "iteration 100 / 1000: loss 368.878423\n",
      "iteration 200 / 1000: loss 118.210493\n",
      "iteration 300 / 1000: loss 154.864964\n",
      "iteration 400 / 1000: loss 133.815779\n",
      "iteration 500 / 1000: loss 163.030429\n",
      "iteration 600 / 1000: loss 395.244644\n",
      "iteration 700 / 1000: loss 261.279636\n",
      "iteration 800 / 1000: loss 188.263924\n",
      "iteration 900 / 1000: loss 172.234219\n",
      "trainning 32 times \n",
      "\n",
      "iteration 0 / 1000: loss 137.758672\n",
      "iteration 100 / 1000: loss 258.858041\n",
      "iteration 200 / 1000: loss 203.452691\n",
      "iteration 300 / 1000: loss 263.990401\n",
      "iteration 400 / 1000: loss 217.253361\n",
      "iteration 500 / 1000: loss 163.400420\n",
      "iteration 600 / 1000: loss 176.047524\n",
      "iteration 700 / 1000: loss 209.554986\n",
      "iteration 800 / 1000: loss 314.245800\n",
      "iteration 900 / 1000: loss 221.999370\n",
      "trainning 33 times \n",
      "\n",
      "iteration 0 / 1000: loss 245.823059\n",
      "iteration 100 / 1000: loss 182.393183\n",
      "iteration 200 / 1000: loss 133.210791\n",
      "iteration 300 / 1000: loss 251.893551\n",
      "iteration 400 / 1000: loss 478.131179\n",
      "iteration 500 / 1000: loss 217.286981\n",
      "iteration 600 / 1000: loss 225.319050\n",
      "iteration 700 / 1000: loss 212.390556\n",
      "iteration 800 / 1000: loss 168.689782\n",
      "iteration 900 / 1000: loss 282.031950\n",
      "trainning 34 times \n",
      "\n",
      "iteration 0 / 1000: loss 225.557041\n",
      "iteration 100 / 1000: loss 285.823365\n",
      "iteration 200 / 1000: loss 149.787006\n",
      "iteration 300 / 1000: loss 331.102991\n",
      "iteration 400 / 1000: loss 298.703753\n",
      "iteration 500 / 1000: loss 148.091096\n",
      "iteration 600 / 1000: loss 161.229097\n",
      "iteration 700 / 1000: loss 263.800605\n",
      "iteration 800 / 1000: loss 157.699313\n",
      "iteration 900 / 1000: loss 213.110758\n",
      "trainning 35 times \n",
      "\n",
      "iteration 0 / 1000: loss 370.021414\n",
      "iteration 100 / 1000: loss 236.773255\n",
      "iteration 200 / 1000: loss 244.703649\n",
      "iteration 300 / 1000: loss 183.735459\n",
      "iteration 400 / 1000: loss 178.489107\n",
      "iteration 500 / 1000: loss 197.581737\n",
      "iteration 600 / 1000: loss 170.090839\n",
      "iteration 700 / 1000: loss 247.087596\n",
      "iteration 800 / 1000: loss 181.362312\n",
      "iteration 900 / 1000: loss 158.848162\n",
      "trainning 36 times \n",
      "\n",
      "iteration 0 / 1000: loss 202.910697\n",
      "iteration 100 / 1000: loss 202.673849\n",
      "iteration 200 / 1000: loss 199.301059\n",
      "iteration 300 / 1000: loss 159.333810\n",
      "iteration 400 / 1000: loss 149.584763\n",
      "iteration 500 / 1000: loss 193.796705\n",
      "iteration 600 / 1000: loss 220.091145\n",
      "iteration 700 / 1000: loss 204.422726\n",
      "iteration 800 / 1000: loss 191.144226\n",
      "iteration 900 / 1000: loss 148.850804\n",
      "trainning 37 times \n",
      "\n",
      "iteration 0 / 1000: loss 332.208069\n",
      "iteration 100 / 1000: loss 97.434609\n",
      "iteration 200 / 1000: loss 420.885378\n",
      "iteration 300 / 1000: loss 200.244026\n",
      "iteration 400 / 1000: loss 180.566380\n",
      "iteration 500 / 1000: loss 158.399458\n",
      "iteration 600 / 1000: loss 210.897280\n",
      "iteration 700 / 1000: loss 253.102556\n",
      "iteration 800 / 1000: loss 190.518819\n",
      "iteration 900 / 1000: loss 179.497350\n",
      "trainning 38 times \n",
      "\n",
      "iteration 0 / 1000: loss 195.052427\n",
      "iteration 100 / 1000: loss 236.114313\n",
      "iteration 200 / 1000: loss 230.633541\n",
      "iteration 300 / 1000: loss 255.578303\n",
      "iteration 400 / 1000: loss 151.622652\n",
      "iteration 500 / 1000: loss 242.327448\n",
      "iteration 600 / 1000: loss 202.370820\n",
      "iteration 700 / 1000: loss 263.411046\n",
      "iteration 800 / 1000: loss 239.594585\n",
      "iteration 900 / 1000: loss 146.559325\n",
      "trainning 39 times \n",
      "\n",
      "iteration 0 / 1000: loss 277.795878\n",
      "iteration 100 / 1000: loss 148.056122\n",
      "iteration 200 / 1000: loss 151.018095\n",
      "iteration 300 / 1000: loss 179.600246\n",
      "iteration 400 / 1000: loss 170.614268\n",
      "iteration 500 / 1000: loss 272.101418\n",
      "iteration 600 / 1000: loss 232.332232\n",
      "iteration 700 / 1000: loss 206.955404\n",
      "iteration 800 / 1000: loss 185.402191\n",
      "iteration 900 / 1000: loss 224.629441\n",
      "trainning 40 times \n",
      "\n",
      "iteration 0 / 1000: loss 231.751658\n",
      "iteration 100 / 1000: loss 243.768291\n",
      "iteration 200 / 1000: loss 226.978382\n",
      "iteration 300 / 1000: loss 181.620317\n",
      "iteration 400 / 1000: loss 232.454328\n",
      "iteration 500 / 1000: loss 193.425650\n",
      "iteration 600 / 1000: loss 177.558225\n",
      "iteration 700 / 1000: loss 227.706875\n",
      "iteration 800 / 1000: loss 218.682274\n",
      "iteration 900 / 1000: loss 315.495811\n",
      "trainning 41 times \n",
      "\n",
      "iteration 0 / 1000: loss 304.423019\n",
      "iteration 100 / 1000: loss 273.961036\n",
      "iteration 200 / 1000: loss 192.208753\n",
      "iteration 300 / 1000: loss 166.228639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 1000: loss 263.553682\n",
      "iteration 500 / 1000: loss 161.201784\n",
      "iteration 600 / 1000: loss 132.515395\n",
      "iteration 700 / 1000: loss 157.443750\n",
      "iteration 800 / 1000: loss 214.995536\n",
      "iteration 900 / 1000: loss 236.183123\n",
      "trainning 42 times \n",
      "\n",
      "iteration 0 / 1000: loss 409.660290\n",
      "iteration 100 / 1000: loss 272.586402\n",
      "iteration 200 / 1000: loss 285.926259\n",
      "iteration 300 / 1000: loss 226.145749\n",
      "iteration 400 / 1000: loss 208.517332\n",
      "iteration 500 / 1000: loss 205.772025\n",
      "iteration 600 / 1000: loss 250.601399\n",
      "iteration 700 / 1000: loss 238.727407\n",
      "iteration 800 / 1000: loss 191.626800\n",
      "iteration 900 / 1000: loss 199.516827\n",
      "trainning 43 times \n",
      "\n",
      "iteration 0 / 1000: loss 220.881522\n",
      "iteration 100 / 1000: loss 239.429654\n",
      "iteration 200 / 1000: loss 393.980792\n",
      "iteration 300 / 1000: loss 402.138441\n",
      "iteration 400 / 1000: loss 205.665170\n",
      "iteration 500 / 1000: loss 179.763650\n",
      "iteration 600 / 1000: loss 187.752858\n",
      "iteration 700 / 1000: loss 209.659158\n",
      "iteration 800 / 1000: loss 221.785781\n",
      "iteration 900 / 1000: loss 257.270774\n",
      "trainning 44 times \n",
      "\n",
      "iteration 0 / 1000: loss 209.147770\n",
      "iteration 100 / 1000: loss 197.905751\n",
      "iteration 200 / 1000: loss 406.718030\n",
      "iteration 300 / 1000: loss 292.449696\n",
      "iteration 400 / 1000: loss 258.285295\n",
      "iteration 500 / 1000: loss 315.346930\n",
      "iteration 600 / 1000: loss 242.177157\n",
      "iteration 700 / 1000: loss 242.773510\n",
      "iteration 800 / 1000: loss 180.865452\n",
      "iteration 900 / 1000: loss 186.182948\n",
      "trainning 45 times \n",
      "\n",
      "iteration 0 / 1000: loss 216.433102\n",
      "iteration 100 / 1000: loss 339.625022\n",
      "iteration 200 / 1000: loss 167.581074\n",
      "iteration 300 / 1000: loss 184.717221\n",
      "iteration 400 / 1000: loss 224.683638\n",
      "iteration 500 / 1000: loss 197.693027\n",
      "iteration 600 / 1000: loss 258.746198\n",
      "iteration 700 / 1000: loss 220.953370\n",
      "iteration 800 / 1000: loss 176.026540\n",
      "iteration 900 / 1000: loss 254.163953\n",
      "trainning 46 times \n",
      "\n",
      "iteration 0 / 1000: loss 229.832988\n",
      "iteration 100 / 1000: loss 262.598109\n",
      "iteration 200 / 1000: loss 239.354153\n",
      "iteration 300 / 1000: loss 222.787460\n",
      "iteration 400 / 1000: loss 241.877943\n",
      "iteration 500 / 1000: loss 155.649930\n",
      "iteration 600 / 1000: loss 263.302668\n",
      "iteration 700 / 1000: loss 232.111227\n",
      "iteration 800 / 1000: loss 202.055448\n",
      "iteration 900 / 1000: loss 300.986549\n",
      "trainning 47 times \n",
      "\n",
      "iteration 0 / 1000: loss 307.902541\n",
      "iteration 100 / 1000: loss 320.557914\n",
      "iteration 200 / 1000: loss 158.977756\n",
      "iteration 300 / 1000: loss 252.503876\n",
      "iteration 400 / 1000: loss 251.051340\n",
      "iteration 500 / 1000: loss 198.480027\n",
      "iteration 600 / 1000: loss 169.446062\n",
      "iteration 700 / 1000: loss 242.853565\n",
      "iteration 800 / 1000: loss 269.751053\n",
      "iteration 900 / 1000: loss 166.501572\n",
      "trainning 48 times \n",
      "\n",
      "iteration 0 / 1000: loss 172.883485\n",
      "iteration 100 / 1000: loss 334.410122\n",
      "iteration 200 / 1000: loss 276.151959\n",
      "iteration 300 / 1000: loss 155.600869\n",
      "iteration 400 / 1000: loss 192.618450\n",
      "iteration 500 / 1000: loss 222.806574\n",
      "iteration 600 / 1000: loss 228.206099\n",
      "iteration 700 / 1000: loss 221.569462\n",
      "iteration 800 / 1000: loss 244.641494\n",
      "iteration 900 / 1000: loss 145.122465\n",
      "trainning 49 times \n",
      "\n",
      "iteration 0 / 1000: loss 144.006378\n",
      "iteration 100 / 1000: loss 116.286463\n",
      "iteration 200 / 1000: loss 348.977766\n",
      "iteration 300 / 1000: loss 270.968517\n",
      "iteration 400 / 1000: loss 231.792311\n",
      "iteration 500 / 1000: loss 249.750608\n",
      "iteration 600 / 1000: loss 251.630832\n",
      "iteration 700 / 1000: loss 157.438518\n",
      "iteration 800 / 1000: loss 156.649351\n",
      "iteration 900 / 1000: loss 293.000294\n",
      "trainning 50 times \n",
      "\n"
     ]
    }
   ],
   "source": [
    "LC = LinearClassifier()\n",
    "SM = Softmax()\n",
    "\n",
    "print(dataTrain1.shape)\n",
    "\n",
    "for i in range(50):\n",
    "    LC.train(dataTrain1[:(1+i)*500,:], labelTrain1[:500*(i+1)])\n",
    "  \n",
    "    print('trainning %d times \\n' % (i+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30720000\n",
      "10000\n",
      "2053\n",
      "accuracy is 20.000000\n"
     ]
    }
   ],
   "source": [
    "print(dataTest1.size)\n",
    "pre = LC.predict(dataTest1[:,:])\n",
    "print(pre.size)\n",
    "acc = 0\n",
    "\n",
    "# print(pre.size)\n",
    "# print(labelTest1.size)\n",
    "\n",
    "for i in range(pre.size):\n",
    "  if (pre[i] == labelTest1[i]):\n",
    "    acc += 1\n",
    "\n",
    "print(acc)\n",
    "print('accuracy is %f' % (100*acc/pre.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
