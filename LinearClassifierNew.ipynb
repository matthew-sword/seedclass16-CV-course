{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import print_function\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 文件读取\n",
    "def unpickle(file):\n",
    "\twith open(file,'rb') as fo:\n",
    "\t\tdict = pickle.load(fo)\n",
    "\treturn dict\n",
    "\n",
    "def load_file(file):\n",
    "\tdictTrain = unpickle(file + \"data_batch_1\")\n",
    "\tdataTrain = dictTrain['data']\n",
    "\tlabelTrain = dictTrain['labels']\n",
    "\n",
    "\tfor i in range(2,6):\n",
    "\t\tdictTrain = unpickle(file + \"data_batch_\" + str(i))\n",
    "\t\tdataTrain = np.vstack([dataTrain,dictTrain['data']])\n",
    "\t\tlabelTrain = np.hstack([labelTrain,dictTrain['labels']])\n",
    "\n",
    "\tdictTest = unpickle(file + \"test_batch\")\n",
    "\tdataTest = dictTest['data']\n",
    "\tlabelTest = dictTest['labels']\n",
    "\tlabelTest = np.array(labelTest)\n",
    "\n",
    "\treturn dataTrain, labelTrain, dataTest, labelTest\n",
    "\n",
    "\n",
    "#softmax loss 函数\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "\t'''\n",
    "\t\tW:权重矩阵\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t'''\n",
    "\t#初始化数据\n",
    "\tloss = 0.0\n",
    "\tdW = np.zeros_like(W)\n",
    "\tnum_train = X.shape[0]\t#样本数\n",
    "\tnum_class = W.shape[1]\t#样本类别数\n",
    "\n",
    "\tfor i in xrange(num_train):\n",
    "\t\tscore = X[i].dot(W)\n",
    "\t\tscore -= np.max(score)\t#提高样本稳定性\n",
    "\n",
    "\t\tcorrect_score = score[y[i]]\n",
    "\t\texp_sum = np.sum(np.exp(score))\n",
    "\t\tloss += np.log(exp_sum) - correct_score\n",
    "\n",
    "\t\tfor j in xrange(num_class):\n",
    "\t\t\tif (j == y[i]):\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i] - X[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i]\n",
    "\n",
    "\n",
    "\tloss /= num_train\n",
    "\tloss += 0.5 * reg * np.sum(W*W)\n",
    "\n",
    "\tdW /= num_train\n",
    "\tdW += reg * W\n",
    "\n",
    "\treturn loss, dW\n",
    "\n",
    "#线性分类器\n",
    "class LinearClassifier(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.W = None\n",
    "\n",
    "\tdef train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\tstep_size:学习步进速度\n",
    "\t\treg:正则化强度\n",
    "\t\tnum_iters:迭代次数\n",
    "\t\tbatch_size:每次迭代图片样本数\n",
    "\t\tverbose:是否打印信息\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss_history:每次训练loss值\n",
    "\t\t'''\n",
    "\t\tnum_train, dim = X.shape\n",
    "\t\tnum_classes = np.max(y) + 1\n",
    "\t\n",
    "\t\tif self.W is None:\n",
    "\t\t\tself.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "\t\tloss_history = []\n",
    "\n",
    "\t\tfor it in xrange(num_iters):\n",
    "\t\t\t#从样本中不重复随机采batch_size个样本\n",
    "\t\t\tsample_index = np.random.choice(num_train, batch_size, replace=False)\n",
    "\n",
    "\t\t\tX_batch = X[sample_index, :]\n",
    "\t\t\ty_batch = y[sample_index]\n",
    "\n",
    "\t\t\tloss, grad = self.loss(X_batch, y_batch, reg)\n",
    "\t\t\tloss_history.append(loss)\n",
    "\n",
    "\t\t\tself.W += -step_size * grad\n",
    "\n",
    "\t\t\tif (verbose and it %10 == 0):\n",
    "\t\t\t\tprint('iteration %d / %d, samples: %d, loss: %f' % (it, num_iters, batch_size, loss))\n",
    "\n",
    "\t\treturn loss_history\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\ty_pred:标签预测值\n",
    "\t\t'''\n",
    "\t\ty_pred = np.zeros(X.shape[1])\n",
    "\n",
    "\t\tscore = X.dot(self.W)\n",
    "\t\ty_pred = np.argmax(score, axis = 1)\n",
    "\n",
    "\t\treturn y_pred\n",
    "\n",
    "\n",
    "\tdef loss(self, X_batch, y_batch, reg):\n",
    "\t\t'''\n",
    "\t\tX_batch:图片训练集(矩阵)\n",
    "\t\ty_batch:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t\t'''\n",
    "\t\treturn softmax_loss_naive(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集加载完成\n",
      "测试集加载完成\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from PIL import Image\n",
    "#开始训练\n",
    "file_path = './'\n",
    "\n",
    "dataTrFirst, labelTrain, dataTsFirst, labelTest = load_file(file_path)\n",
    "\n",
    "dataTr = np.zeros((dataTrFirst.shape[0],32*32))\n",
    "dataTs = np.zeros((dataTsFirst.shape[0],32*32))\n",
    "\n",
    "\n",
    "for i in range(dataTrFirst.shape[0] -45000):\n",
    "    img = dataTrFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTr[i] = res.reshape((1,32*32))\n",
    "print(\"训练集加载完成\")\n",
    "\n",
    "for i in range(dataTsFirst.shape[0] -1):\n",
    "    img = dataTsFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTs[i] = res.reshape((1,32*32))\n",
    "print(\"测试集加载完成\")\n",
    "\n",
    "dataTrain = dataTrain - np.mean(dataTrain, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ...\n",
      "iteration 0 / 8000, samples: 200, loss: 5.496735\n",
      "iteration 10 / 8000, samples: 200, loss: 471.465223\n",
      "iteration 20 / 8000, samples: 200, loss: 288.024471\n",
      "iteration 30 / 8000, samples: 200, loss: 337.944751\n",
      "iteration 40 / 8000, samples: 200, loss: 292.491008\n",
      "iteration 50 / 8000, samples: 200, loss: 420.080957\n",
      "iteration 60 / 8000, samples: 200, loss: 389.946219\n",
      "iteration 70 / 8000, samples: 200, loss: 302.980857\n",
      "iteration 80 / 8000, samples: 200, loss: 316.832657\n",
      "iteration 90 / 8000, samples: 200, loss: 368.143984\n",
      "iteration 100 / 8000, samples: 200, loss: 333.789643\n",
      "iteration 110 / 8000, samples: 200, loss: 343.327407\n",
      "iteration 120 / 8000, samples: 200, loss: 310.722470\n",
      "iteration 130 / 8000, samples: 200, loss: 452.571276\n",
      "iteration 140 / 8000, samples: 200, loss: 184.883117\n",
      "iteration 150 / 8000, samples: 200, loss: 348.447355\n",
      "iteration 160 / 8000, samples: 200, loss: 202.414807\n",
      "iteration 170 / 8000, samples: 200, loss: 251.718882\n",
      "iteration 180 / 8000, samples: 200, loss: 308.974915\n",
      "iteration 190 / 8000, samples: 200, loss: 283.805727\n",
      "iteration 200 / 8000, samples: 200, loss: 258.679358\n",
      "iteration 210 / 8000, samples: 200, loss: 281.164160\n",
      "iteration 220 / 8000, samples: 200, loss: 186.263598\n",
      "iteration 230 / 8000, samples: 200, loss: 197.516327\n",
      "iteration 240 / 8000, samples: 200, loss: 323.162323\n",
      "iteration 250 / 8000, samples: 200, loss: 353.618035\n",
      "iteration 260 / 8000, samples: 200, loss: 265.170412\n",
      "iteration 270 / 8000, samples: 200, loss: 376.431061\n",
      "iteration 280 / 8000, samples: 200, loss: 281.250656\n",
      "iteration 290 / 8000, samples: 200, loss: 331.187030\n",
      "iteration 300 / 8000, samples: 200, loss: 270.747103\n",
      "iteration 310 / 8000, samples: 200, loss: 305.838669\n",
      "iteration 320 / 8000, samples: 200, loss: 291.658022\n",
      "iteration 330 / 8000, samples: 200, loss: 358.268076\n",
      "iteration 340 / 8000, samples: 200, loss: 273.355246\n",
      "iteration 350 / 8000, samples: 200, loss: 300.923516\n",
      "iteration 360 / 8000, samples: 200, loss: 353.387971\n",
      "iteration 370 / 8000, samples: 200, loss: 215.305772\n",
      "iteration 380 / 8000, samples: 200, loss: 209.442898\n",
      "iteration 390 / 8000, samples: 200, loss: 319.918163\n",
      "iteration 400 / 8000, samples: 200, loss: 435.426674\n",
      "iteration 410 / 8000, samples: 200, loss: 320.318624\n",
      "iteration 420 / 8000, samples: 200, loss: 326.485560\n",
      "iteration 430 / 8000, samples: 200, loss: 251.565081\n",
      "iteration 440 / 8000, samples: 200, loss: 200.871418\n",
      "iteration 450 / 8000, samples: 200, loss: 316.622211\n",
      "iteration 460 / 8000, samples: 200, loss: 261.217499\n",
      "iteration 470 / 8000, samples: 200, loss: 362.180338\n",
      "iteration 480 / 8000, samples: 200, loss: 267.915092\n",
      "iteration 490 / 8000, samples: 200, loss: 276.060732\n",
      "iteration 500 / 8000, samples: 200, loss: 290.609132\n",
      "iteration 510 / 8000, samples: 200, loss: 260.790143\n",
      "iteration 520 / 8000, samples: 200, loss: 238.101918\n",
      "iteration 530 / 8000, samples: 200, loss: 339.632525\n",
      "iteration 540 / 8000, samples: 200, loss: 307.079631\n",
      "iteration 550 / 8000, samples: 200, loss: 167.730724\n",
      "iteration 560 / 8000, samples: 200, loss: 220.363933\n",
      "iteration 570 / 8000, samples: 200, loss: 280.466867\n",
      "iteration 580 / 8000, samples: 200, loss: 323.810478\n",
      "iteration 590 / 8000, samples: 200, loss: 258.883422\n",
      "iteration 600 / 8000, samples: 200, loss: 248.791567\n",
      "iteration 610 / 8000, samples: 200, loss: 290.860119\n",
      "iteration 620 / 8000, samples: 200, loss: 206.223491\n",
      "iteration 630 / 8000, samples: 200, loss: 192.918125\n",
      "iteration 640 / 8000, samples: 200, loss: 202.954781\n",
      "iteration 650 / 8000, samples: 200, loss: 289.212964\n",
      "iteration 660 / 8000, samples: 200, loss: 255.033079\n",
      "iteration 670 / 8000, samples: 200, loss: 255.008923\n",
      "iteration 680 / 8000, samples: 200, loss: 209.487317\n",
      "iteration 690 / 8000, samples: 200, loss: 258.629653\n",
      "iteration 700 / 8000, samples: 200, loss: 210.745396\n",
      "iteration 710 / 8000, samples: 200, loss: 356.700757\n",
      "iteration 720 / 8000, samples: 200, loss: 218.001479\n",
      "iteration 730 / 8000, samples: 200, loss: 253.937745\n",
      "iteration 740 / 8000, samples: 200, loss: 228.595908\n",
      "iteration 750 / 8000, samples: 200, loss: 224.144926\n",
      "iteration 760 / 8000, samples: 200, loss: 308.256747\n",
      "iteration 770 / 8000, samples: 200, loss: 287.653865\n",
      "iteration 780 / 8000, samples: 200, loss: 312.667269\n",
      "iteration 790 / 8000, samples: 200, loss: 187.921596\n",
      "iteration 800 / 8000, samples: 200, loss: 293.418720\n",
      "iteration 810 / 8000, samples: 200, loss: 387.286678\n",
      "iteration 820 / 8000, samples: 200, loss: 251.137880\n",
      "iteration 830 / 8000, samples: 200, loss: 274.797346\n",
      "iteration 840 / 8000, samples: 200, loss: 288.483123\n",
      "iteration 850 / 8000, samples: 200, loss: 291.008863\n",
      "iteration 860 / 8000, samples: 200, loss: 341.995394\n",
      "iteration 870 / 8000, samples: 200, loss: 175.533331\n",
      "iteration 880 / 8000, samples: 200, loss: 370.765839\n",
      "iteration 890 / 8000, samples: 200, loss: 236.571893\n",
      "iteration 900 / 8000, samples: 200, loss: 335.951471\n",
      "iteration 910 / 8000, samples: 200, loss: 225.684347\n",
      "iteration 920 / 8000, samples: 200, loss: 239.820847\n",
      "iteration 930 / 8000, samples: 200, loss: 219.273615\n",
      "iteration 940 / 8000, samples: 200, loss: 206.335237\n",
      "iteration 950 / 8000, samples: 200, loss: 288.408019\n",
      "iteration 960 / 8000, samples: 200, loss: 244.424726\n",
      "iteration 970 / 8000, samples: 200, loss: 376.891706\n",
      "iteration 980 / 8000, samples: 200, loss: 194.026585\n",
      "iteration 990 / 8000, samples: 200, loss: 221.687736\n",
      "iteration 1000 / 8000, samples: 200, loss: 272.791468\n",
      "iteration 1010 / 8000, samples: 200, loss: 201.094845\n",
      "iteration 1020 / 8000, samples: 200, loss: 316.916621\n",
      "iteration 1030 / 8000, samples: 200, loss: 291.727035\n",
      "iteration 1040 / 8000, samples: 200, loss: 324.062237\n",
      "iteration 1050 / 8000, samples: 200, loss: 247.998478\n",
      "iteration 1060 / 8000, samples: 200, loss: 273.686121\n",
      "iteration 1070 / 8000, samples: 200, loss: 257.093739\n",
      "iteration 1080 / 8000, samples: 200, loss: 210.763238\n",
      "iteration 1090 / 8000, samples: 200, loss: 230.242488\n",
      "iteration 1100 / 8000, samples: 200, loss: 286.184599\n",
      "iteration 1110 / 8000, samples: 200, loss: 235.661141\n",
      "iteration 1120 / 8000, samples: 200, loss: 258.469355\n",
      "iteration 1130 / 8000, samples: 200, loss: 300.667352\n",
      "iteration 1140 / 8000, samples: 200, loss: 416.056558\n",
      "iteration 1150 / 8000, samples: 200, loss: 233.683368\n",
      "iteration 1160 / 8000, samples: 200, loss: 380.874511\n",
      "iteration 1170 / 8000, samples: 200, loss: 270.324998\n",
      "iteration 1180 / 8000, samples: 200, loss: 217.534610\n",
      "iteration 1190 / 8000, samples: 200, loss: 195.406202\n",
      "iteration 1200 / 8000, samples: 200, loss: 313.562503\n",
      "iteration 1210 / 8000, samples: 200, loss: 248.282876\n",
      "iteration 1220 / 8000, samples: 200, loss: 278.919480\n",
      "iteration 1230 / 8000, samples: 200, loss: 199.319689\n",
      "iteration 1240 / 8000, samples: 200, loss: 235.595720\n",
      "iteration 1250 / 8000, samples: 200, loss: 276.640707\n",
      "iteration 1260 / 8000, samples: 200, loss: 327.912331\n",
      "iteration 1270 / 8000, samples: 200, loss: 200.093718\n",
      "iteration 1280 / 8000, samples: 200, loss: 319.147586\n",
      "iteration 1290 / 8000, samples: 200, loss: 374.386406\n",
      "iteration 1300 / 8000, samples: 200, loss: 299.680796\n",
      "iteration 1310 / 8000, samples: 200, loss: 187.077206\n",
      "iteration 1320 / 8000, samples: 200, loss: 212.293504\n",
      "iteration 1330 / 8000, samples: 200, loss: 316.525155\n",
      "iteration 1340 / 8000, samples: 200, loss: 248.654551\n",
      "iteration 1350 / 8000, samples: 200, loss: 232.012816\n",
      "iteration 1360 / 8000, samples: 200, loss: 227.335824\n",
      "iteration 1370 / 8000, samples: 200, loss: 233.823703\n",
      "iteration 1380 / 8000, samples: 200, loss: 304.235306\n",
      "iteration 1390 / 8000, samples: 200, loss: 251.431493\n",
      "iteration 1400 / 8000, samples: 200, loss: 327.795276\n",
      "iteration 1410 / 8000, samples: 200, loss: 234.070624\n",
      "iteration 1420 / 8000, samples: 200, loss: 297.393484\n",
      "iteration 1430 / 8000, samples: 200, loss: 147.221381\n",
      "iteration 1440 / 8000, samples: 200, loss: 378.312830\n",
      "iteration 1450 / 8000, samples: 200, loss: 335.556329\n",
      "iteration 1460 / 8000, samples: 200, loss: 233.637700\n",
      "iteration 1470 / 8000, samples: 200, loss: 302.578834\n",
      "iteration 1480 / 8000, samples: 200, loss: 197.043533\n",
      "iteration 1490 / 8000, samples: 200, loss: 334.938670\n",
      "iteration 1500 / 8000, samples: 200, loss: 361.742571\n",
      "iteration 1510 / 8000, samples: 200, loss: 211.379568\n",
      "iteration 1520 / 8000, samples: 200, loss: 294.411477\n",
      "iteration 1530 / 8000, samples: 200, loss: 304.758034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1540 / 8000, samples: 200, loss: 351.737332\n",
      "iteration 1550 / 8000, samples: 200, loss: 190.675568\n",
      "iteration 1560 / 8000, samples: 200, loss: 273.168638\n",
      "iteration 1570 / 8000, samples: 200, loss: 217.379428\n",
      "iteration 1580 / 8000, samples: 200, loss: 191.557144\n",
      "iteration 1590 / 8000, samples: 200, loss: 221.470332\n",
      "iteration 1600 / 8000, samples: 200, loss: 281.891914\n",
      "iteration 1610 / 8000, samples: 200, loss: 223.161294\n",
      "iteration 1620 / 8000, samples: 200, loss: 268.786000\n",
      "iteration 1630 / 8000, samples: 200, loss: 345.365835\n",
      "iteration 1640 / 8000, samples: 200, loss: 188.578833\n",
      "iteration 1650 / 8000, samples: 200, loss: 182.954540\n",
      "iteration 1660 / 8000, samples: 200, loss: 187.634387\n",
      "iteration 1670 / 8000, samples: 200, loss: 416.217167\n",
      "iteration 1680 / 8000, samples: 200, loss: 338.110588\n",
      "iteration 1690 / 8000, samples: 200, loss: 195.725043\n",
      "iteration 1700 / 8000, samples: 200, loss: 228.374783\n",
      "iteration 1710 / 8000, samples: 200, loss: 213.012640\n",
      "iteration 1720 / 8000, samples: 200, loss: 311.201933\n",
      "iteration 1730 / 8000, samples: 200, loss: 230.661077\n",
      "iteration 1740 / 8000, samples: 200, loss: 426.409982\n",
      "iteration 1750 / 8000, samples: 200, loss: 248.669165\n",
      "iteration 1760 / 8000, samples: 200, loss: 256.393019\n",
      "iteration 1770 / 8000, samples: 200, loss: 174.787446\n",
      "iteration 1780 / 8000, samples: 200, loss: 488.187788\n",
      "iteration 1790 / 8000, samples: 200, loss: 180.275484\n",
      "iteration 1800 / 8000, samples: 200, loss: 229.843771\n",
      "iteration 1810 / 8000, samples: 200, loss: 349.183463\n",
      "iteration 1820 / 8000, samples: 200, loss: 251.331223\n",
      "iteration 1830 / 8000, samples: 200, loss: 149.391817\n",
      "iteration 1840 / 8000, samples: 200, loss: 285.119101\n",
      "iteration 1850 / 8000, samples: 200, loss: 311.363390\n",
      "iteration 1860 / 8000, samples: 200, loss: 207.771505\n",
      "iteration 1870 / 8000, samples: 200, loss: 259.814545\n",
      "iteration 1880 / 8000, samples: 200, loss: 270.925031\n",
      "iteration 1890 / 8000, samples: 200, loss: 305.897974\n",
      "iteration 1900 / 8000, samples: 200, loss: 478.481105\n",
      "iteration 1910 / 8000, samples: 200, loss: 313.015906\n",
      "iteration 1920 / 8000, samples: 200, loss: 139.971237\n",
      "iteration 1930 / 8000, samples: 200, loss: 249.371449\n",
      "iteration 1940 / 8000, samples: 200, loss: 279.933456\n",
      "iteration 1950 / 8000, samples: 200, loss: 275.638954\n",
      "iteration 1960 / 8000, samples: 200, loss: 217.181951\n",
      "iteration 1970 / 8000, samples: 200, loss: 235.570693\n",
      "iteration 1980 / 8000, samples: 200, loss: 171.204611\n",
      "iteration 1990 / 8000, samples: 200, loss: 178.023034\n",
      "iteration 2000 / 8000, samples: 200, loss: 344.473668\n",
      "iteration 2010 / 8000, samples: 200, loss: 194.138826\n",
      "iteration 2020 / 8000, samples: 200, loss: 207.202909\n",
      "iteration 2030 / 8000, samples: 200, loss: 236.601692\n",
      "iteration 2040 / 8000, samples: 200, loss: 231.079152\n",
      "iteration 2050 / 8000, samples: 200, loss: 262.435310\n",
      "iteration 2060 / 8000, samples: 200, loss: 208.485611\n",
      "iteration 2070 / 8000, samples: 200, loss: 158.378016\n",
      "iteration 2080 / 8000, samples: 200, loss: 288.589930\n",
      "iteration 2090 / 8000, samples: 200, loss: 310.675397\n",
      "iteration 2100 / 8000, samples: 200, loss: 207.692968\n",
      "iteration 2110 / 8000, samples: 200, loss: 217.974652\n",
      "iteration 2120 / 8000, samples: 200, loss: 322.425965\n",
      "iteration 2130 / 8000, samples: 200, loss: 117.008263\n",
      "iteration 2140 / 8000, samples: 200, loss: 349.098819\n",
      "iteration 2150 / 8000, samples: 200, loss: 292.139526\n",
      "iteration 2160 / 8000, samples: 200, loss: 306.093304\n",
      "iteration 2170 / 8000, samples: 200, loss: 238.505084\n",
      "iteration 2180 / 8000, samples: 200, loss: 246.255251\n",
      "iteration 2190 / 8000, samples: 200, loss: 256.884824\n",
      "iteration 2200 / 8000, samples: 200, loss: 187.454430\n",
      "iteration 2210 / 8000, samples: 200, loss: 287.977714\n",
      "iteration 2220 / 8000, samples: 200, loss: 165.208776\n",
      "iteration 2230 / 8000, samples: 200, loss: 183.914959\n",
      "iteration 2240 / 8000, samples: 200, loss: 338.300319\n",
      "iteration 2250 / 8000, samples: 200, loss: 210.198935\n",
      "iteration 2260 / 8000, samples: 200, loss: 399.803148\n",
      "iteration 2270 / 8000, samples: 200, loss: 238.398913\n",
      "iteration 2280 / 8000, samples: 200, loss: 247.952748\n",
      "iteration 2290 / 8000, samples: 200, loss: 333.417257\n",
      "iteration 2300 / 8000, samples: 200, loss: 333.741564\n",
      "iteration 2310 / 8000, samples: 200, loss: 279.223560\n",
      "iteration 2320 / 8000, samples: 200, loss: 335.647762\n",
      "iteration 2330 / 8000, samples: 200, loss: 204.376261\n",
      "iteration 2340 / 8000, samples: 200, loss: 441.804868\n",
      "iteration 2350 / 8000, samples: 200, loss: 245.990747\n",
      "iteration 2360 / 8000, samples: 200, loss: 361.595197\n",
      "iteration 2370 / 8000, samples: 200, loss: 201.983853\n",
      "iteration 2380 / 8000, samples: 200, loss: 210.727151\n",
      "iteration 2390 / 8000, samples: 200, loss: 179.156266\n",
      "iteration 2400 / 8000, samples: 200, loss: 331.469556\n",
      "iteration 2410 / 8000, samples: 200, loss: 278.207258\n",
      "iteration 2420 / 8000, samples: 200, loss: 278.568097\n",
      "iteration 2430 / 8000, samples: 200, loss: 216.979091\n",
      "iteration 2440 / 8000, samples: 200, loss: 225.167207\n",
      "iteration 2450 / 8000, samples: 200, loss: 353.521723\n",
      "iteration 2460 / 8000, samples: 200, loss: 217.474276\n",
      "iteration 2470 / 8000, samples: 200, loss: 201.106675\n",
      "iteration 2480 / 8000, samples: 200, loss: 317.350236\n",
      "iteration 2490 / 8000, samples: 200, loss: 289.509192\n",
      "iteration 2500 / 8000, samples: 200, loss: 311.165937\n",
      "iteration 2510 / 8000, samples: 200, loss: 180.646904\n",
      "iteration 2520 / 8000, samples: 200, loss: 241.895025\n",
      "iteration 2530 / 8000, samples: 200, loss: 181.215431\n",
      "iteration 2540 / 8000, samples: 200, loss: 327.168469\n",
      "iteration 2550 / 8000, samples: 200, loss: 164.452631\n",
      "iteration 2560 / 8000, samples: 200, loss: 275.377702\n",
      "iteration 2570 / 8000, samples: 200, loss: 172.379962\n",
      "iteration 2580 / 8000, samples: 200, loss: 190.669333\n",
      "iteration 2590 / 8000, samples: 200, loss: 295.647414\n",
      "iteration 2600 / 8000, samples: 200, loss: 251.190301\n",
      "iteration 2610 / 8000, samples: 200, loss: 284.482022\n",
      "iteration 2620 / 8000, samples: 200, loss: 217.008949\n",
      "iteration 2630 / 8000, samples: 200, loss: 168.113386\n",
      "iteration 2640 / 8000, samples: 200, loss: 264.964598\n",
      "iteration 2650 / 8000, samples: 200, loss: 196.220865\n",
      "iteration 2660 / 8000, samples: 200, loss: 397.308710\n",
      "iteration 2670 / 8000, samples: 200, loss: 362.010804\n",
      "iteration 2680 / 8000, samples: 200, loss: 229.137598\n",
      "iteration 2690 / 8000, samples: 200, loss: 329.464422\n",
      "iteration 2700 / 8000, samples: 200, loss: 258.807070\n",
      "iteration 2710 / 8000, samples: 200, loss: 208.650249\n",
      "iteration 2720 / 8000, samples: 200, loss: 174.332313\n",
      "iteration 2730 / 8000, samples: 200, loss: 244.883850\n",
      "iteration 2740 / 8000, samples: 200, loss: 175.558415\n",
      "iteration 2750 / 8000, samples: 200, loss: 289.900200\n",
      "iteration 2760 / 8000, samples: 200, loss: 370.618593\n",
      "iteration 2770 / 8000, samples: 200, loss: 198.080357\n",
      "iteration 2780 / 8000, samples: 200, loss: 367.820408\n",
      "iteration 2790 / 8000, samples: 200, loss: 224.033498\n",
      "iteration 2800 / 8000, samples: 200, loss: 279.099761\n",
      "iteration 2810 / 8000, samples: 200, loss: 263.527398\n",
      "iteration 2820 / 8000, samples: 200, loss: 285.653040\n",
      "iteration 2830 / 8000, samples: 200, loss: 179.010517\n",
      "iteration 2840 / 8000, samples: 200, loss: 259.443620\n",
      "iteration 2850 / 8000, samples: 200, loss: 358.487206\n",
      "iteration 2860 / 8000, samples: 200, loss: 310.416542\n",
      "iteration 2870 / 8000, samples: 200, loss: 267.341468\n",
      "iteration 2880 / 8000, samples: 200, loss: 209.941163\n",
      "iteration 2890 / 8000, samples: 200, loss: 239.912651\n",
      "iteration 2900 / 8000, samples: 200, loss: 249.467387\n",
      "iteration 2910 / 8000, samples: 200, loss: 262.305620\n",
      "iteration 2920 / 8000, samples: 200, loss: 368.462264\n",
      "iteration 2930 / 8000, samples: 200, loss: 471.739840\n",
      "iteration 2940 / 8000, samples: 200, loss: 304.563517\n",
      "iteration 2950 / 8000, samples: 200, loss: 240.354351\n",
      "iteration 2960 / 8000, samples: 200, loss: 198.119219\n",
      "iteration 2970 / 8000, samples: 200, loss: 250.749977\n",
      "iteration 2980 / 8000, samples: 200, loss: 366.564908\n",
      "iteration 2990 / 8000, samples: 200, loss: 529.588566\n",
      "iteration 3000 / 8000, samples: 200, loss: 220.458044\n",
      "iteration 3010 / 8000, samples: 200, loss: 290.091153\n",
      "iteration 3020 / 8000, samples: 200, loss: 457.865572\n",
      "iteration 3030 / 8000, samples: 200, loss: 275.709638\n",
      "iteration 3040 / 8000, samples: 200, loss: 290.545380\n",
      "iteration 3050 / 8000, samples: 200, loss: 249.447409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3060 / 8000, samples: 200, loss: 308.536802\n",
      "iteration 3070 / 8000, samples: 200, loss: 204.371859\n",
      "iteration 3080 / 8000, samples: 200, loss: 303.149349\n",
      "iteration 3090 / 8000, samples: 200, loss: 279.858284\n",
      "iteration 3100 / 8000, samples: 200, loss: 287.024707\n",
      "iteration 3110 / 8000, samples: 200, loss: 310.286069\n",
      "iteration 3120 / 8000, samples: 200, loss: 420.775584\n",
      "iteration 3130 / 8000, samples: 200, loss: 249.780889\n",
      "iteration 3140 / 8000, samples: 200, loss: 231.197362\n",
      "iteration 3150 / 8000, samples: 200, loss: 277.186362\n",
      "iteration 3160 / 8000, samples: 200, loss: 251.065287\n",
      "iteration 3170 / 8000, samples: 200, loss: 217.719264\n",
      "iteration 3180 / 8000, samples: 200, loss: 237.106431\n",
      "iteration 3190 / 8000, samples: 200, loss: 291.952422\n",
      "iteration 3200 / 8000, samples: 200, loss: 322.431915\n",
      "iteration 3210 / 8000, samples: 200, loss: 176.097607\n",
      "iteration 3220 / 8000, samples: 200, loss: 314.593985\n",
      "iteration 3230 / 8000, samples: 200, loss: 250.710160\n",
      "iteration 3240 / 8000, samples: 200, loss: 327.964715\n",
      "iteration 3250 / 8000, samples: 200, loss: 224.550060\n",
      "iteration 3260 / 8000, samples: 200, loss: 301.502819\n",
      "iteration 3270 / 8000, samples: 200, loss: 209.072566\n",
      "iteration 3280 / 8000, samples: 200, loss: 234.850025\n",
      "iteration 3290 / 8000, samples: 200, loss: 244.928359\n",
      "iteration 3300 / 8000, samples: 200, loss: 345.079559\n",
      "iteration 3310 / 8000, samples: 200, loss: 266.685009\n",
      "iteration 3320 / 8000, samples: 200, loss: 310.917960\n",
      "iteration 3330 / 8000, samples: 200, loss: 200.757961\n",
      "iteration 3340 / 8000, samples: 200, loss: 331.068720\n",
      "iteration 3350 / 8000, samples: 200, loss: 184.756577\n",
      "iteration 3360 / 8000, samples: 200, loss: 270.505766\n",
      "iteration 3370 / 8000, samples: 200, loss: 228.456765\n",
      "iteration 3380 / 8000, samples: 200, loss: 286.037080\n",
      "iteration 3390 / 8000, samples: 200, loss: 272.232080\n",
      "iteration 3400 / 8000, samples: 200, loss: 301.280460\n",
      "iteration 3410 / 8000, samples: 200, loss: 219.802924\n",
      "iteration 3420 / 8000, samples: 200, loss: 290.496344\n",
      "iteration 3430 / 8000, samples: 200, loss: 279.464728\n",
      "iteration 3440 / 8000, samples: 200, loss: 180.128481\n",
      "iteration 3450 / 8000, samples: 200, loss: 206.075465\n",
      "iteration 3460 / 8000, samples: 200, loss: 337.425855\n",
      "iteration 3470 / 8000, samples: 200, loss: 303.658770\n",
      "iteration 3480 / 8000, samples: 200, loss: 202.552718\n",
      "iteration 3490 / 8000, samples: 200, loss: 288.926816\n",
      "iteration 3500 / 8000, samples: 200, loss: 502.785454\n",
      "iteration 3510 / 8000, samples: 200, loss: 271.036379\n",
      "iteration 3520 / 8000, samples: 200, loss: 341.624408\n",
      "iteration 3530 / 8000, samples: 200, loss: 317.548642\n",
      "iteration 3540 / 8000, samples: 200, loss: 192.047778\n",
      "iteration 3550 / 8000, samples: 200, loss: 265.006564\n",
      "iteration 3560 / 8000, samples: 200, loss: 240.138417\n",
      "iteration 3570 / 8000, samples: 200, loss: 202.883855\n",
      "iteration 3580 / 8000, samples: 200, loss: 250.969272\n",
      "iteration 3590 / 8000, samples: 200, loss: 171.004101\n",
      "iteration 3600 / 8000, samples: 200, loss: 322.203905\n",
      "iteration 3610 / 8000, samples: 200, loss: 258.476339\n",
      "iteration 3620 / 8000, samples: 200, loss: 202.796123\n",
      "iteration 3630 / 8000, samples: 200, loss: 280.081108\n",
      "iteration 3640 / 8000, samples: 200, loss: 202.959631\n",
      "iteration 3650 / 8000, samples: 200, loss: 211.006237\n",
      "iteration 3660 / 8000, samples: 200, loss: 248.035172\n",
      "iteration 3670 / 8000, samples: 200, loss: 361.283274\n",
      "iteration 3680 / 8000, samples: 200, loss: 356.584662\n",
      "iteration 3690 / 8000, samples: 200, loss: 191.575763\n",
      "iteration 3700 / 8000, samples: 200, loss: 222.392152\n",
      "iteration 3710 / 8000, samples: 200, loss: 300.022519\n",
      "iteration 3720 / 8000, samples: 200, loss: 252.241255\n",
      "iteration 3730 / 8000, samples: 200, loss: 307.446556\n",
      "iteration 3740 / 8000, samples: 200, loss: 263.345273\n",
      "iteration 3750 / 8000, samples: 200, loss: 320.486064\n",
      "iteration 3760 / 8000, samples: 200, loss: 242.922342\n",
      "iteration 3770 / 8000, samples: 200, loss: 221.377312\n",
      "iteration 3780 / 8000, samples: 200, loss: 168.923186\n",
      "iteration 3790 / 8000, samples: 200, loss: 341.876831\n",
      "iteration 3800 / 8000, samples: 200, loss: 402.746445\n",
      "iteration 3810 / 8000, samples: 200, loss: 328.919054\n",
      "iteration 3820 / 8000, samples: 200, loss: 390.897289\n",
      "iteration 3830 / 8000, samples: 200, loss: 271.912155\n",
      "iteration 3840 / 8000, samples: 200, loss: 238.279025\n",
      "iteration 3850 / 8000, samples: 200, loss: 274.270863\n",
      "iteration 3860 / 8000, samples: 200, loss: 206.762280\n",
      "iteration 3870 / 8000, samples: 200, loss: 271.459465\n",
      "iteration 3880 / 8000, samples: 200, loss: 214.530813\n",
      "iteration 3890 / 8000, samples: 200, loss: 267.450375\n",
      "iteration 3900 / 8000, samples: 200, loss: 171.868827\n",
      "iteration 3910 / 8000, samples: 200, loss: 231.072075\n",
      "iteration 3920 / 8000, samples: 200, loss: 255.539480\n",
      "iteration 3930 / 8000, samples: 200, loss: 314.531629\n",
      "iteration 3940 / 8000, samples: 200, loss: 244.642614\n",
      "iteration 3950 / 8000, samples: 200, loss: 348.577575\n",
      "iteration 3960 / 8000, samples: 200, loss: 188.288620\n",
      "iteration 3970 / 8000, samples: 200, loss: 250.771791\n",
      "iteration 3980 / 8000, samples: 200, loss: 188.875911\n",
      "iteration 3990 / 8000, samples: 200, loss: 331.756386\n",
      "iteration 4000 / 8000, samples: 200, loss: 334.182849\n",
      "iteration 4010 / 8000, samples: 200, loss: 301.862372\n",
      "iteration 4020 / 8000, samples: 200, loss: 306.018040\n",
      "iteration 4030 / 8000, samples: 200, loss: 240.373773\n",
      "iteration 4040 / 8000, samples: 200, loss: 206.119614\n",
      "iteration 4050 / 8000, samples: 200, loss: 250.641946\n",
      "iteration 4060 / 8000, samples: 200, loss: 378.720144\n",
      "iteration 4070 / 8000, samples: 200, loss: 263.464865\n",
      "iteration 4080 / 8000, samples: 200, loss: 209.238922\n",
      "iteration 4090 / 8000, samples: 200, loss: 298.294695\n",
      "iteration 4100 / 8000, samples: 200, loss: 317.052738\n",
      "iteration 4110 / 8000, samples: 200, loss: 374.382089\n",
      "iteration 4120 / 8000, samples: 200, loss: 213.223585\n",
      "iteration 4130 / 8000, samples: 200, loss: 307.251247\n",
      "iteration 4140 / 8000, samples: 200, loss: 186.963793\n",
      "iteration 4150 / 8000, samples: 200, loss: 224.731003\n",
      "iteration 4160 / 8000, samples: 200, loss: 207.082217\n",
      "iteration 4170 / 8000, samples: 200, loss: 468.252501\n",
      "iteration 4180 / 8000, samples: 200, loss: 255.573949\n",
      "iteration 4190 / 8000, samples: 200, loss: 315.003574\n",
      "iteration 4200 / 8000, samples: 200, loss: 308.782999\n",
      "iteration 4210 / 8000, samples: 200, loss: 225.242749\n",
      "iteration 4220 / 8000, samples: 200, loss: 200.726434\n",
      "iteration 4230 / 8000, samples: 200, loss: 275.037654\n",
      "iteration 4240 / 8000, samples: 200, loss: 192.243532\n",
      "iteration 4250 / 8000, samples: 200, loss: 309.759614\n",
      "iteration 4260 / 8000, samples: 200, loss: 267.007252\n",
      "iteration 4270 / 8000, samples: 200, loss: 179.535825\n",
      "iteration 4280 / 8000, samples: 200, loss: 321.468697\n",
      "iteration 4290 / 8000, samples: 200, loss: 304.426027\n",
      "iteration 4300 / 8000, samples: 200, loss: 294.424645\n",
      "iteration 4310 / 8000, samples: 200, loss: 277.088772\n",
      "iteration 4320 / 8000, samples: 200, loss: 269.303248\n",
      "iteration 4330 / 8000, samples: 200, loss: 278.877400\n",
      "iteration 4340 / 8000, samples: 200, loss: 222.197784\n",
      "iteration 4350 / 8000, samples: 200, loss: 232.451314\n",
      "iteration 4360 / 8000, samples: 200, loss: 248.760935\n",
      "iteration 4370 / 8000, samples: 200, loss: 299.955660\n",
      "iteration 4380 / 8000, samples: 200, loss: 288.784697\n",
      "iteration 4390 / 8000, samples: 200, loss: 198.819629\n",
      "iteration 4400 / 8000, samples: 200, loss: 280.315845\n",
      "iteration 4410 / 8000, samples: 200, loss: 229.804828\n",
      "iteration 4420 / 8000, samples: 200, loss: 205.304845\n",
      "iteration 4430 / 8000, samples: 200, loss: 178.247817\n",
      "iteration 4440 / 8000, samples: 200, loss: 260.331377\n",
      "iteration 4450 / 8000, samples: 200, loss: 230.009724\n",
      "iteration 4460 / 8000, samples: 200, loss: 271.431547\n",
      "iteration 4470 / 8000, samples: 200, loss: 314.272297\n",
      "iteration 4480 / 8000, samples: 200, loss: 227.255166\n",
      "iteration 4490 / 8000, samples: 200, loss: 284.981644\n",
      "iteration 4500 / 8000, samples: 200, loss: 364.267997\n",
      "iteration 4510 / 8000, samples: 200, loss: 294.374150\n",
      "iteration 4520 / 8000, samples: 200, loss: 182.994893\n",
      "iteration 4530 / 8000, samples: 200, loss: 434.061900\n",
      "iteration 4540 / 8000, samples: 200, loss: 249.370131\n",
      "iteration 4550 / 8000, samples: 200, loss: 189.067599\n",
      "iteration 4560 / 8000, samples: 200, loss: 201.274990\n",
      "iteration 4570 / 8000, samples: 200, loss: 232.164242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4580 / 8000, samples: 200, loss: 245.443852\n",
      "iteration 4590 / 8000, samples: 200, loss: 203.343901\n",
      "iteration 4600 / 8000, samples: 200, loss: 481.042344\n",
      "iteration 4610 / 8000, samples: 200, loss: 185.785612\n",
      "iteration 4620 / 8000, samples: 200, loss: 237.512398\n",
      "iteration 4630 / 8000, samples: 200, loss: 255.188366\n",
      "iteration 4640 / 8000, samples: 200, loss: 224.720284\n",
      "iteration 4650 / 8000, samples: 200, loss: 307.417748\n",
      "iteration 4660 / 8000, samples: 200, loss: 203.255017\n",
      "iteration 4670 / 8000, samples: 200, loss: 283.054359\n",
      "iteration 4680 / 8000, samples: 200, loss: 181.731674\n",
      "iteration 4690 / 8000, samples: 200, loss: 349.253359\n",
      "iteration 4700 / 8000, samples: 200, loss: 259.364950\n",
      "iteration 4710 / 8000, samples: 200, loss: 152.842786\n",
      "iteration 4720 / 8000, samples: 200, loss: 471.297229\n",
      "iteration 4730 / 8000, samples: 200, loss: 251.912837\n",
      "iteration 4740 / 8000, samples: 200, loss: 196.433389\n",
      "iteration 4750 / 8000, samples: 200, loss: 369.348096\n",
      "iteration 4760 / 8000, samples: 200, loss: 275.868364\n",
      "iteration 4770 / 8000, samples: 200, loss: 196.642619\n",
      "iteration 4780 / 8000, samples: 200, loss: 235.187255\n",
      "iteration 4790 / 8000, samples: 200, loss: 205.174613\n",
      "iteration 4800 / 8000, samples: 200, loss: 314.627782\n",
      "iteration 4810 / 8000, samples: 200, loss: 284.885105\n",
      "iteration 4820 / 8000, samples: 200, loss: 253.832282\n",
      "iteration 4830 / 8000, samples: 200, loss: 263.335144\n",
      "iteration 4840 / 8000, samples: 200, loss: 360.464859\n",
      "iteration 4850 / 8000, samples: 200, loss: 225.719157\n",
      "iteration 4860 / 8000, samples: 200, loss: 208.240363\n",
      "iteration 4870 / 8000, samples: 200, loss: 212.203772\n",
      "iteration 4880 / 8000, samples: 200, loss: 228.406838\n",
      "iteration 4890 / 8000, samples: 200, loss: 329.182399\n",
      "iteration 4900 / 8000, samples: 200, loss: 278.630655\n",
      "iteration 4910 / 8000, samples: 200, loss: 278.368768\n",
      "iteration 4920 / 8000, samples: 200, loss: 310.114864\n",
      "iteration 4930 / 8000, samples: 200, loss: 352.723703\n",
      "iteration 4940 / 8000, samples: 200, loss: 195.401435\n",
      "iteration 4950 / 8000, samples: 200, loss: 274.553961\n",
      "iteration 4960 / 8000, samples: 200, loss: 351.561235\n",
      "iteration 4970 / 8000, samples: 200, loss: 253.797278\n",
      "iteration 4980 / 8000, samples: 200, loss: 325.509275\n",
      "iteration 4990 / 8000, samples: 200, loss: 208.729679\n",
      "iteration 5000 / 8000, samples: 200, loss: 244.508736\n",
      "iteration 5010 / 8000, samples: 200, loss: 260.479111\n",
      "iteration 5020 / 8000, samples: 200, loss: 255.154601\n",
      "iteration 5030 / 8000, samples: 200, loss: 167.716075\n",
      "iteration 5040 / 8000, samples: 200, loss: 287.983652\n",
      "iteration 5050 / 8000, samples: 200, loss: 308.730638\n",
      "iteration 5060 / 8000, samples: 200, loss: 418.536033\n",
      "iteration 5070 / 8000, samples: 200, loss: 232.609327\n",
      "iteration 5080 / 8000, samples: 200, loss: 386.544786\n",
      "iteration 5090 / 8000, samples: 200, loss: 299.237265\n",
      "iteration 5100 / 8000, samples: 200, loss: 199.152440\n",
      "iteration 5110 / 8000, samples: 200, loss: 284.096064\n",
      "iteration 5120 / 8000, samples: 200, loss: 235.725668\n",
      "iteration 5130 / 8000, samples: 200, loss: 314.308401\n",
      "iteration 5140 / 8000, samples: 200, loss: 339.708893\n",
      "iteration 5150 / 8000, samples: 200, loss: 263.284953\n",
      "iteration 5160 / 8000, samples: 200, loss: 241.689734\n",
      "iteration 5170 / 8000, samples: 200, loss: 196.529740\n",
      "iteration 5180 / 8000, samples: 200, loss: 233.084590\n",
      "iteration 5190 / 8000, samples: 200, loss: 146.486277\n",
      "iteration 5200 / 8000, samples: 200, loss: 212.194897\n",
      "iteration 5210 / 8000, samples: 200, loss: 212.413952\n",
      "iteration 5220 / 8000, samples: 200, loss: 332.417375\n",
      "iteration 5230 / 8000, samples: 200, loss: 273.232505\n",
      "iteration 5240 / 8000, samples: 200, loss: 220.674199\n",
      "iteration 5250 / 8000, samples: 200, loss: 206.573550\n",
      "iteration 5260 / 8000, samples: 200, loss: 339.953472\n",
      "iteration 5270 / 8000, samples: 200, loss: 289.556326\n",
      "iteration 5280 / 8000, samples: 200, loss: 317.747653\n",
      "iteration 5290 / 8000, samples: 200, loss: 333.608118\n",
      "iteration 5300 / 8000, samples: 200, loss: 309.002584\n",
      "iteration 5310 / 8000, samples: 200, loss: 198.004261\n",
      "iteration 5320 / 8000, samples: 200, loss: 286.909710\n",
      "iteration 5330 / 8000, samples: 200, loss: 367.121436\n",
      "iteration 5340 / 8000, samples: 200, loss: 215.732685\n",
      "iteration 5350 / 8000, samples: 200, loss: 274.150936\n",
      "iteration 5360 / 8000, samples: 200, loss: 183.080678\n",
      "iteration 5370 / 8000, samples: 200, loss: 215.490752\n",
      "iteration 5380 / 8000, samples: 200, loss: 293.717277\n",
      "iteration 5390 / 8000, samples: 200, loss: 413.634077\n",
      "iteration 5400 / 8000, samples: 200, loss: 225.122198\n",
      "iteration 5410 / 8000, samples: 200, loss: 270.995229\n",
      "iteration 5420 / 8000, samples: 200, loss: 306.963862\n",
      "iteration 5430 / 8000, samples: 200, loss: 204.658789\n",
      "iteration 5440 / 8000, samples: 200, loss: 260.501843\n",
      "iteration 5450 / 8000, samples: 200, loss: 308.090109\n",
      "iteration 5460 / 8000, samples: 200, loss: 318.844884\n",
      "iteration 5470 / 8000, samples: 200, loss: 255.969925\n",
      "iteration 5480 / 8000, samples: 200, loss: 320.695565\n",
      "iteration 5490 / 8000, samples: 200, loss: 343.911012\n",
      "iteration 5500 / 8000, samples: 200, loss: 266.176214\n",
      "iteration 5510 / 8000, samples: 200, loss: 285.640663\n",
      "iteration 5520 / 8000, samples: 200, loss: 158.283805\n",
      "iteration 5530 / 8000, samples: 200, loss: 360.640627\n",
      "iteration 5540 / 8000, samples: 200, loss: 230.757114\n",
      "iteration 5550 / 8000, samples: 200, loss: 262.191220\n",
      "iteration 5560 / 8000, samples: 200, loss: 227.826046\n",
      "iteration 5570 / 8000, samples: 200, loss: 373.910906\n",
      "iteration 5580 / 8000, samples: 200, loss: 208.340955\n",
      "iteration 5590 / 8000, samples: 200, loss: 243.662379\n",
      "iteration 5600 / 8000, samples: 200, loss: 318.636252\n",
      "iteration 5610 / 8000, samples: 200, loss: 223.755198\n",
      "iteration 5620 / 8000, samples: 200, loss: 300.817502\n",
      "iteration 5630 / 8000, samples: 200, loss: 298.632636\n",
      "iteration 5640 / 8000, samples: 200, loss: 230.517052\n",
      "iteration 5650 / 8000, samples: 200, loss: 258.782147\n",
      "iteration 5660 / 8000, samples: 200, loss: 245.513179\n",
      "iteration 5670 / 8000, samples: 200, loss: 329.100069\n",
      "iteration 5680 / 8000, samples: 200, loss: 340.428331\n",
      "iteration 5690 / 8000, samples: 200, loss: 276.543383\n",
      "iteration 5700 / 8000, samples: 200, loss: 292.052257\n",
      "iteration 5710 / 8000, samples: 200, loss: 270.055462\n",
      "iteration 5720 / 8000, samples: 200, loss: 176.367916\n",
      "iteration 5730 / 8000, samples: 200, loss: 307.142040\n",
      "iteration 5740 / 8000, samples: 200, loss: 205.859496\n",
      "iteration 5750 / 8000, samples: 200, loss: 277.499580\n",
      "iteration 5760 / 8000, samples: 200, loss: 289.087868\n",
      "iteration 5770 / 8000, samples: 200, loss: 400.570042\n",
      "iteration 5780 / 8000, samples: 200, loss: 181.309699\n",
      "iteration 5790 / 8000, samples: 200, loss: 188.062949\n",
      "iteration 5800 / 8000, samples: 200, loss: 258.250355\n",
      "iteration 5810 / 8000, samples: 200, loss: 268.555116\n",
      "iteration 5820 / 8000, samples: 200, loss: 189.420419\n",
      "iteration 5830 / 8000, samples: 200, loss: 188.787572\n",
      "iteration 5840 / 8000, samples: 200, loss: 289.676022\n",
      "iteration 5850 / 8000, samples: 200, loss: 377.124571\n",
      "iteration 5860 / 8000, samples: 200, loss: 182.593982\n",
      "iteration 5870 / 8000, samples: 200, loss: 179.474881\n",
      "iteration 5880 / 8000, samples: 200, loss: 269.819892\n",
      "iteration 5890 / 8000, samples: 200, loss: 348.693941\n",
      "iteration 5900 / 8000, samples: 200, loss: 260.392377\n",
      "iteration 5910 / 8000, samples: 200, loss: 215.213108\n",
      "iteration 5920 / 8000, samples: 200, loss: 135.706726\n",
      "iteration 5930 / 8000, samples: 200, loss: 333.031661\n",
      "iteration 5940 / 8000, samples: 200, loss: 270.339076\n",
      "iteration 5950 / 8000, samples: 200, loss: 204.197215\n",
      "iteration 5960 / 8000, samples: 200, loss: 262.702087\n",
      "iteration 5970 / 8000, samples: 200, loss: 267.515813\n",
      "iteration 5980 / 8000, samples: 200, loss: 136.181394\n",
      "iteration 5990 / 8000, samples: 200, loss: 226.013061\n",
      "iteration 6000 / 8000, samples: 200, loss: 283.963416\n",
      "iteration 6010 / 8000, samples: 200, loss: 167.174633\n",
      "iteration 6020 / 8000, samples: 200, loss: 354.772181\n",
      "iteration 6030 / 8000, samples: 200, loss: 196.243390\n",
      "iteration 6040 / 8000, samples: 200, loss: 242.307192\n",
      "iteration 6050 / 8000, samples: 200, loss: 331.191172\n",
      "iteration 6060 / 8000, samples: 200, loss: 251.369900\n",
      "iteration 6070 / 8000, samples: 200, loss: 376.962912\n",
      "iteration 6080 / 8000, samples: 200, loss: 247.018728\n",
      "iteration 6090 / 8000, samples: 200, loss: 223.724901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6100 / 8000, samples: 200, loss: 306.917746\n",
      "iteration 6110 / 8000, samples: 200, loss: 177.344601\n",
      "iteration 6120 / 8000, samples: 200, loss: 211.582154\n",
      "iteration 6130 / 8000, samples: 200, loss: 316.344489\n",
      "iteration 6140 / 8000, samples: 200, loss: 173.785442\n",
      "iteration 6150 / 8000, samples: 200, loss: 246.406852\n",
      "iteration 6160 / 8000, samples: 200, loss: 274.189443\n",
      "iteration 6170 / 8000, samples: 200, loss: 156.602525\n",
      "iteration 6180 / 8000, samples: 200, loss: 318.262304\n",
      "iteration 6190 / 8000, samples: 200, loss: 394.752588\n",
      "iteration 6200 / 8000, samples: 200, loss: 220.107852\n",
      "iteration 6210 / 8000, samples: 200, loss: 266.602921\n",
      "iteration 6220 / 8000, samples: 200, loss: 247.077121\n",
      "iteration 6230 / 8000, samples: 200, loss: 183.819263\n",
      "iteration 6240 / 8000, samples: 200, loss: 196.457712\n",
      "iteration 6250 / 8000, samples: 200, loss: 303.189219\n",
      "iteration 6260 / 8000, samples: 200, loss: 241.897647\n",
      "iteration 6270 / 8000, samples: 200, loss: 316.698624\n",
      "iteration 6280 / 8000, samples: 200, loss: 208.200299\n",
      "iteration 6290 / 8000, samples: 200, loss: 191.278439\n",
      "iteration 6300 / 8000, samples: 200, loss: 398.034864\n",
      "iteration 6310 / 8000, samples: 200, loss: 269.252925\n",
      "iteration 6320 / 8000, samples: 200, loss: 452.220230\n",
      "iteration 6330 / 8000, samples: 200, loss: 251.533894\n",
      "iteration 6340 / 8000, samples: 200, loss: 230.755452\n",
      "iteration 6350 / 8000, samples: 200, loss: 288.679973\n",
      "iteration 6360 / 8000, samples: 200, loss: 275.469557\n",
      "iteration 6370 / 8000, samples: 200, loss: 272.077397\n",
      "iteration 6380 / 8000, samples: 200, loss: 279.728566\n",
      "iteration 6390 / 8000, samples: 200, loss: 275.351372\n",
      "iteration 6400 / 8000, samples: 200, loss: 308.519774\n",
      "iteration 6410 / 8000, samples: 200, loss: 198.329060\n",
      "iteration 6420 / 8000, samples: 200, loss: 213.594398\n",
      "iteration 6430 / 8000, samples: 200, loss: 345.260848\n",
      "iteration 6440 / 8000, samples: 200, loss: 270.872990\n",
      "iteration 6450 / 8000, samples: 200, loss: 403.509075\n",
      "iteration 6460 / 8000, samples: 200, loss: 169.030532\n",
      "iteration 6470 / 8000, samples: 200, loss: 262.129491\n",
      "iteration 6480 / 8000, samples: 200, loss: 265.372055\n",
      "iteration 6490 / 8000, samples: 200, loss: 199.838156\n",
      "iteration 6500 / 8000, samples: 200, loss: 329.088567\n",
      "iteration 6510 / 8000, samples: 200, loss: 159.015972\n",
      "iteration 6520 / 8000, samples: 200, loss: 193.956385\n",
      "iteration 6530 / 8000, samples: 200, loss: 207.924953\n",
      "iteration 6540 / 8000, samples: 200, loss: 215.674988\n",
      "iteration 6550 / 8000, samples: 200, loss: 171.744786\n",
      "iteration 6560 / 8000, samples: 200, loss: 337.605325\n",
      "iteration 6570 / 8000, samples: 200, loss: 255.308461\n",
      "iteration 6580 / 8000, samples: 200, loss: 256.062639\n",
      "iteration 6590 / 8000, samples: 200, loss: 260.987960\n",
      "iteration 6600 / 8000, samples: 200, loss: 192.215039\n",
      "iteration 6610 / 8000, samples: 200, loss: 216.779744\n",
      "iteration 6620 / 8000, samples: 200, loss: 264.533093\n",
      "iteration 6630 / 8000, samples: 200, loss: 213.644822\n",
      "iteration 6640 / 8000, samples: 200, loss: 211.518239\n",
      "iteration 6650 / 8000, samples: 200, loss: 299.000909\n",
      "iteration 6660 / 8000, samples: 200, loss: 233.154490\n",
      "iteration 6670 / 8000, samples: 200, loss: 152.865142\n",
      "iteration 6680 / 8000, samples: 200, loss: 270.704366\n",
      "iteration 6690 / 8000, samples: 200, loss: 323.379069\n",
      "iteration 6700 / 8000, samples: 200, loss: 236.387360\n",
      "iteration 6710 / 8000, samples: 200, loss: 216.848511\n",
      "iteration 6720 / 8000, samples: 200, loss: 346.691055\n",
      "iteration 6730 / 8000, samples: 200, loss: 323.396221\n",
      "iteration 6740 / 8000, samples: 200, loss: 219.081662\n",
      "iteration 6750 / 8000, samples: 200, loss: 221.476757\n",
      "iteration 6760 / 8000, samples: 200, loss: 213.326739\n",
      "iteration 6770 / 8000, samples: 200, loss: 247.122103\n",
      "iteration 6780 / 8000, samples: 200, loss: 244.339249\n",
      "iteration 6790 / 8000, samples: 200, loss: 224.169146\n",
      "iteration 6800 / 8000, samples: 200, loss: 291.277532\n",
      "iteration 6810 / 8000, samples: 200, loss: 209.186230\n",
      "iteration 6820 / 8000, samples: 200, loss: 339.253851\n",
      "iteration 6830 / 8000, samples: 200, loss: 318.525573\n",
      "iteration 6840 / 8000, samples: 200, loss: 300.916452\n",
      "iteration 6850 / 8000, samples: 200, loss: 155.817183\n",
      "iteration 6860 / 8000, samples: 200, loss: 234.419113\n",
      "iteration 6870 / 8000, samples: 200, loss: 417.234323\n",
      "iteration 6880 / 8000, samples: 200, loss: 238.634848\n",
      "iteration 6890 / 8000, samples: 200, loss: 425.005360\n",
      "iteration 6900 / 8000, samples: 200, loss: 213.975916\n",
      "iteration 6910 / 8000, samples: 200, loss: 224.794471\n",
      "iteration 6920 / 8000, samples: 200, loss: 416.698542\n",
      "iteration 6930 / 8000, samples: 200, loss: 209.354652\n",
      "iteration 6940 / 8000, samples: 200, loss: 321.000753\n",
      "iteration 6950 / 8000, samples: 200, loss: 380.048898\n",
      "iteration 6960 / 8000, samples: 200, loss: 220.291665\n",
      "iteration 6970 / 8000, samples: 200, loss: 293.672725\n",
      "iteration 6980 / 8000, samples: 200, loss: 178.103081\n",
      "iteration 6990 / 8000, samples: 200, loss: 210.432412\n",
      "iteration 7000 / 8000, samples: 200, loss: 350.707605\n",
      "iteration 7010 / 8000, samples: 200, loss: 284.588953\n",
      "iteration 7020 / 8000, samples: 200, loss: 247.118335\n",
      "iteration 7030 / 8000, samples: 200, loss: 194.404120\n",
      "iteration 7040 / 8000, samples: 200, loss: 204.855442\n",
      "iteration 7050 / 8000, samples: 200, loss: 242.683596\n",
      "iteration 7060 / 8000, samples: 200, loss: 191.284143\n",
      "iteration 7070 / 8000, samples: 200, loss: 202.945732\n",
      "iteration 7080 / 8000, samples: 200, loss: 250.969204\n",
      "iteration 7090 / 8000, samples: 200, loss: 158.199020\n",
      "iteration 7100 / 8000, samples: 200, loss: 258.258920\n",
      "iteration 7110 / 8000, samples: 200, loss: 292.439991\n",
      "iteration 7120 / 8000, samples: 200, loss: 236.008477\n",
      "iteration 7130 / 8000, samples: 200, loss: 315.399618\n",
      "iteration 7140 / 8000, samples: 200, loss: 244.383065\n",
      "iteration 7150 / 8000, samples: 200, loss: 227.414154\n",
      "iteration 7160 / 8000, samples: 200, loss: 237.366841\n",
      "iteration 7170 / 8000, samples: 200, loss: 378.159599\n",
      "iteration 7180 / 8000, samples: 200, loss: 296.163680\n",
      "iteration 7190 / 8000, samples: 200, loss: 128.553296\n",
      "iteration 7200 / 8000, samples: 200, loss: 201.009536\n",
      "iteration 7210 / 8000, samples: 200, loss: 322.773068\n",
      "iteration 7220 / 8000, samples: 200, loss: 340.476857\n",
      "iteration 7230 / 8000, samples: 200, loss: 199.462096\n",
      "iteration 7240 / 8000, samples: 200, loss: 296.235050\n",
      "iteration 7250 / 8000, samples: 200, loss: 263.750708\n",
      "iteration 7260 / 8000, samples: 200, loss: 220.589761\n",
      "iteration 7270 / 8000, samples: 200, loss: 270.422114\n",
      "iteration 7280 / 8000, samples: 200, loss: 262.762117\n",
      "iteration 7290 / 8000, samples: 200, loss: 353.412222\n",
      "iteration 7300 / 8000, samples: 200, loss: 269.151068\n",
      "iteration 7310 / 8000, samples: 200, loss: 226.751164\n",
      "iteration 7320 / 8000, samples: 200, loss: 243.111703\n",
      "iteration 7330 / 8000, samples: 200, loss: 184.447555\n",
      "iteration 7340 / 8000, samples: 200, loss: 236.889160\n",
      "iteration 7350 / 8000, samples: 200, loss: 252.775924\n",
      "iteration 7360 / 8000, samples: 200, loss: 226.152583\n",
      "iteration 7370 / 8000, samples: 200, loss: 350.505422\n",
      "iteration 7380 / 8000, samples: 200, loss: 378.807112\n",
      "iteration 7390 / 8000, samples: 200, loss: 316.179788\n",
      "iteration 7400 / 8000, samples: 200, loss: 172.111708\n",
      "iteration 7410 / 8000, samples: 200, loss: 250.859421\n",
      "iteration 7420 / 8000, samples: 200, loss: 300.370813\n",
      "iteration 7430 / 8000, samples: 200, loss: 350.877704\n",
      "iteration 7440 / 8000, samples: 200, loss: 145.307939\n",
      "iteration 7450 / 8000, samples: 200, loss: 256.293633\n",
      "iteration 7460 / 8000, samples: 200, loss: 244.314115\n",
      "iteration 7470 / 8000, samples: 200, loss: 325.688867\n",
      "iteration 7480 / 8000, samples: 200, loss: 358.476504\n",
      "iteration 7490 / 8000, samples: 200, loss: 224.794825\n",
      "iteration 7500 / 8000, samples: 200, loss: 290.637238\n",
      "iteration 7510 / 8000, samples: 200, loss: 172.801853\n",
      "iteration 7520 / 8000, samples: 200, loss: 227.914682\n",
      "iteration 7530 / 8000, samples: 200, loss: 254.362662\n",
      "iteration 7540 / 8000, samples: 200, loss: 250.146548\n",
      "iteration 7550 / 8000, samples: 200, loss: 175.897566\n",
      "iteration 7560 / 8000, samples: 200, loss: 212.802643\n",
      "iteration 7570 / 8000, samples: 200, loss: 285.895202\n",
      "iteration 7580 / 8000, samples: 200, loss: 341.724810\n",
      "iteration 7590 / 8000, samples: 200, loss: 390.677821\n",
      "iteration 7600 / 8000, samples: 200, loss: 294.960862\n",
      "iteration 7610 / 8000, samples: 200, loss: 261.301258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7620 / 8000, samples: 200, loss: 165.070153\n",
      "iteration 7630 / 8000, samples: 200, loss: 378.972635\n",
      "iteration 7640 / 8000, samples: 200, loss: 233.682621\n",
      "iteration 7650 / 8000, samples: 200, loss: 287.490144\n",
      "iteration 7660 / 8000, samples: 200, loss: 450.900873\n",
      "iteration 7670 / 8000, samples: 200, loss: 337.004582\n",
      "iteration 7680 / 8000, samples: 200, loss: 240.288192\n",
      "iteration 7690 / 8000, samples: 200, loss: 261.344646\n",
      "iteration 7700 / 8000, samples: 200, loss: 257.352501\n",
      "iteration 7710 / 8000, samples: 200, loss: 334.690307\n",
      "iteration 7720 / 8000, samples: 200, loss: 218.509133\n",
      "iteration 7730 / 8000, samples: 200, loss: 268.834401\n",
      "iteration 7740 / 8000, samples: 200, loss: 306.773974\n",
      "iteration 7750 / 8000, samples: 200, loss: 182.625891\n",
      "iteration 7760 / 8000, samples: 200, loss: 172.489701\n",
      "iteration 7770 / 8000, samples: 200, loss: 363.285594\n",
      "iteration 7780 / 8000, samples: 200, loss: 285.584183\n",
      "iteration 7790 / 8000, samples: 200, loss: 228.144733\n",
      "iteration 7800 / 8000, samples: 200, loss: 220.672941\n",
      "iteration 7810 / 8000, samples: 200, loss: 302.405775\n",
      "iteration 7820 / 8000, samples: 200, loss: 290.286181\n",
      "iteration 7830 / 8000, samples: 200, loss: 323.750437\n",
      "iteration 7840 / 8000, samples: 200, loss: 282.079897\n",
      "iteration 7850 / 8000, samples: 200, loss: 233.906758\n",
      "iteration 7860 / 8000, samples: 200, loss: 242.006841\n",
      "iteration 7870 / 8000, samples: 200, loss: 374.862468\n",
      "iteration 7880 / 8000, samples: 200, loss: 254.954117\n",
      "iteration 7890 / 8000, samples: 200, loss: 217.306726\n",
      "iteration 7900 / 8000, samples: 200, loss: 262.841721\n",
      "iteration 7910 / 8000, samples: 200, loss: 339.723306\n",
      "iteration 7920 / 8000, samples: 200, loss: 303.541562\n",
      "iteration 7930 / 8000, samples: 200, loss: 281.079429\n",
      "iteration 7940 / 8000, samples: 200, loss: 371.769598\n",
      "iteration 7950 / 8000, samples: 200, loss: 262.693048\n",
      "iteration 7960 / 8000, samples: 200, loss: 191.722609\n",
      "iteration 7970 / 8000, samples: 200, loss: 217.412419\n",
      "iteration 7980 / 8000, samples: 200, loss: 254.097039\n",
      "iteration 7990 / 8000, samples: 200, loss: 247.376220\n"
     ]
    }
   ],
   "source": [
    "LC = LinearClassifier()\n",
    "\n",
    "print('start training ...')\n",
    "#train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True)\n",
    "#在dataTrain中不重复随机抽取batch_size个样本，迭代训练num_iters次\n",
    "loss_all = LC.train(dataTrain, labelTrain, num_iters = 8000, batch_size = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss is 235.595129\n",
      "start predicting ...\n",
      "the accuracy rate is 12.000000 \n"
     ]
    }
   ],
   "source": [
    "print('last loss is %f' %(loss_all[-1]))\n",
    "#开始预测\n",
    "print('start predicting ...')\n",
    "y_pred = LC.predict(dataTest)\n",
    "\n",
    "hit = 0\n",
    "for i in xrange(y_pred.size):\n",
    "\tif (y_pred[i] == labelTest[i]):\n",
    "\t\thit += 1\n",
    "\n",
    "print('the accuracy rate is %f ' % (hit/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
