{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import print_function\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 文件读取\n",
    "def unpickle(file):\n",
    "\twith open(file,'rb') as fo:\n",
    "\t\tdict = pickle.load(fo)\n",
    "\treturn dict\n",
    "\n",
    "def load_file(file):\n",
    "\tdictTrain = unpickle(file + \"data_batch_1\")\n",
    "\tdataTrain = dictTrain['data']\n",
    "\tlabelTrain = dictTrain['labels']\n",
    "\n",
    "\tfor i in range(2,6):\n",
    "\t\tdictTrain = unpickle(file + \"data_batch_\" + str(i))\n",
    "\t\tdataTrain = np.vstack([dataTrain,dictTrain['data']])\n",
    "\t\tlabelTrain = np.hstack([labelTrain,dictTrain['labels']])\n",
    "\n",
    "\tdictTest = unpickle(file + \"test_batch\")\n",
    "\tdataTest = dictTest['data']\n",
    "\tlabelTest = dictTest['labels']\n",
    "\tlabelTest = np.array(labelTest)\n",
    "\n",
    "\treturn dataTrain, labelTrain, dataTest, labelTest\n",
    "\n",
    "\n",
    "#softmax loss 函数\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "\t'''\n",
    "\t\tW:权重矩阵\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t'''\n",
    "\t#初始化数据\n",
    "\tloss = 0.0\n",
    "\tdW = np.zeros_like(W)\n",
    "\tnum_train = X.shape[0]\t#样本数\n",
    "\tnum_class = W.shape[1]\t#样本类别数\n",
    "\n",
    "\tfor i in xrange(num_train):\n",
    "\t\tscore = X[i].dot(W)\n",
    "\t\tscore -= np.max(score)\t#提高样本稳定性\n",
    "\n",
    "\t\tcorrect_score = score[y[i]]\n",
    "\t\texp_sum = np.sum(np.exp(score))\n",
    "\t\tloss += np.log(exp_sum) - correct_score\n",
    "\n",
    "\t\tfor j in xrange(num_class):\n",
    "\t\t\tif (j == y[i]):\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i] - X[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i]\n",
    "\n",
    "\n",
    "\tloss /= num_train\n",
    "\tloss += 0.5 * reg * np.sum(W*W)\n",
    "\n",
    "\tdW /= num_train\n",
    "\tdW += reg * W\n",
    "\n",
    "\treturn loss, dW\n",
    "\n",
    "#线性分类器\n",
    "class LinearClassifier(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.W = None\n",
    "\n",
    "\tdef train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\tstep_size:学习步进速度\n",
    "\t\treg:正则化强度\n",
    "\t\tnum_iters:迭代次数\n",
    "\t\tbatch_size:每次迭代图片样本数\n",
    "\t\tverbose:是否打印信息\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss_history:每次训练loss值\n",
    "\t\t'''\n",
    "\t\tnum_train, dim = X.shape\n",
    "\t\tnum_classes = np.max(y) + 1\n",
    "\t\n",
    "\t\tif self.W is None:\n",
    "\t\t\tself.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "\t\tloss_history = []\n",
    "\n",
    "\t\tfor it in xrange(num_iters):\n",
    "\t\t\t#从样本中不重复随机采batch_size个样本\n",
    "\t\t\tsample_index = np.random.choice(num_train, batch_size, replace=False)\n",
    "\n",
    "\t\t\tX_batch = X[sample_index, :]\n",
    "\t\t\ty_batch = y[sample_index]\n",
    "\n",
    "\t\t\tloss, grad = self.loss(X_batch, y_batch, reg)\n",
    "\t\t\tloss_history.append(loss)\n",
    "\n",
    "\t\t\tself.W += -step_size * grad\n",
    "\n",
    "\t\t\tif (verbose and it %10 == 0):\n",
    "\t\t\t\tprint('iteration %d / %d, samples: %d, loss: %f' % (it, num_iters, batch_size, loss))\n",
    "\n",
    "\t\treturn loss_history\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\ty_pred:标签预测值\n",
    "\t\t'''\n",
    "\t\ty_pred = np.zeros(X.shape[1])\n",
    "\n",
    "\t\tscore = X.dot(self.W)\n",
    "\t\ty_pred = np.argmax(score, axis = 1)\n",
    "\n",
    "\t\treturn y_pred\n",
    "\n",
    "\n",
    "\tdef loss(self, X_batch, y_batch, reg):\n",
    "\t\t'''\n",
    "\t\tX_batch:图片训练集(矩阵)\n",
    "\t\ty_batch:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t\t'''\n",
    "\t\treturn softmax_loss_naive(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集加载完成\n",
      "测试集加载完成\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from PIL import Image\n",
    "#开始训练\n",
    "file_path = './'\n",
    "\n",
    "dataTrFirst, labelTrain, dataTsFirst, labelTest = load_file(file_path)\n",
    "\n",
    "dataTr = np.zeros((dataTrFirst.shape[0],32*32))\n",
    "dataTs = np.zeros((dataTsFirst.shape[0],32*32))\n",
    "\n",
    "\n",
    "for i in range(dataTrFirst.shape[0] -45000):\n",
    "    img = dataTrFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTr[i] = res.reshape((1,32*32))\n",
    "print(\"训练集加载完成\")\n",
    "\n",
    "for i in range(dataTsFirst.shape[0] -1):\n",
    "    img = dataTsFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTs[i] = res.reshape((1,32*32))\n",
    "print(\"测试集加载完成\")\n",
    "\n",
    "dataTrain = dataTrain - np.mean(dataTrain, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ...\n",
      "iteration 0 / 10000, samples: 256, loss: 6.407522\n",
      "iteration 10 / 10000, samples: 256, loss: 356.586537\n",
      "iteration 20 / 10000, samples: 256, loss: 326.962195\n",
      "iteration 30 / 10000, samples: 256, loss: 426.494104\n",
      "iteration 40 / 10000, samples: 256, loss: 431.048330\n",
      "iteration 50 / 10000, samples: 256, loss: 332.071075\n",
      "iteration 60 / 10000, samples: 256, loss: 261.378764\n",
      "iteration 70 / 10000, samples: 256, loss: 269.441075\n",
      "iteration 80 / 10000, samples: 256, loss: 374.883951\n",
      "iteration 90 / 10000, samples: 256, loss: 388.917750\n",
      "iteration 100 / 10000, samples: 256, loss: 237.202494\n",
      "iteration 110 / 10000, samples: 256, loss: 363.410592\n",
      "iteration 120 / 10000, samples: 256, loss: 218.306516\n",
      "iteration 130 / 10000, samples: 256, loss: 282.444498\n",
      "iteration 140 / 10000, samples: 256, loss: 324.298460\n",
      "iteration 150 / 10000, samples: 256, loss: 227.434371\n",
      "iteration 160 / 10000, samples: 256, loss: 293.948717\n",
      "iteration 170 / 10000, samples: 256, loss: 383.530329\n",
      "iteration 180 / 10000, samples: 256, loss: 236.944569\n",
      "iteration 190 / 10000, samples: 256, loss: 315.083453\n",
      "iteration 200 / 10000, samples: 256, loss: 251.615216\n",
      "iteration 210 / 10000, samples: 256, loss: 458.630320\n",
      "iteration 220 / 10000, samples: 256, loss: 189.126442\n",
      "iteration 230 / 10000, samples: 256, loss: 489.590122\n",
      "iteration 240 / 10000, samples: 256, loss: 274.674933\n",
      "iteration 250 / 10000, samples: 256, loss: 262.307269\n",
      "iteration 260 / 10000, samples: 256, loss: 224.845148\n",
      "iteration 270 / 10000, samples: 256, loss: 253.587151\n",
      "iteration 280 / 10000, samples: 256, loss: 195.128812\n",
      "iteration 290 / 10000, samples: 256, loss: 254.300885\n",
      "iteration 300 / 10000, samples: 256, loss: 293.995719\n",
      "iteration 310 / 10000, samples: 256, loss: 249.368701\n",
      "iteration 320 / 10000, samples: 256, loss: 325.100182\n",
      "iteration 330 / 10000, samples: 256, loss: 282.779767\n",
      "iteration 340 / 10000, samples: 256, loss: 261.023724\n",
      "iteration 350 / 10000, samples: 256, loss: 276.600317\n",
      "iteration 360 / 10000, samples: 256, loss: 224.122867\n",
      "iteration 370 / 10000, samples: 256, loss: 183.425623\n",
      "iteration 380 / 10000, samples: 256, loss: 240.277651\n",
      "iteration 390 / 10000, samples: 256, loss: 306.913277\n",
      "iteration 400 / 10000, samples: 256, loss: 325.664310\n",
      "iteration 410 / 10000, samples: 256, loss: 257.030887\n",
      "iteration 420 / 10000, samples: 256, loss: 278.300315\n",
      "iteration 430 / 10000, samples: 256, loss: 264.341629\n",
      "iteration 440 / 10000, samples: 256, loss: 321.104445\n",
      "iteration 450 / 10000, samples: 256, loss: 206.026534\n",
      "iteration 460 / 10000, samples: 256, loss: 239.556735\n",
      "iteration 470 / 10000, samples: 256, loss: 247.585112\n",
      "iteration 480 / 10000, samples: 256, loss: 265.065076\n",
      "iteration 490 / 10000, samples: 256, loss: 322.894969\n",
      "iteration 500 / 10000, samples: 256, loss: 281.056728\n",
      "iteration 510 / 10000, samples: 256, loss: 238.781579\n",
      "iteration 520 / 10000, samples: 256, loss: 202.580659\n",
      "iteration 530 / 10000, samples: 256, loss: 301.088784\n",
      "iteration 540 / 10000, samples: 256, loss: 271.373237\n",
      "iteration 550 / 10000, samples: 256, loss: 360.528076\n",
      "iteration 560 / 10000, samples: 256, loss: 219.448202\n",
      "iteration 570 / 10000, samples: 256, loss: 277.223961\n",
      "iteration 580 / 10000, samples: 256, loss: 256.110156\n",
      "iteration 590 / 10000, samples: 256, loss: 292.339266\n",
      "iteration 600 / 10000, samples: 256, loss: 334.551821\n",
      "iteration 610 / 10000, samples: 256, loss: 202.927560\n",
      "iteration 620 / 10000, samples: 256, loss: 210.108481\n",
      "iteration 630 / 10000, samples: 256, loss: 273.903905\n",
      "iteration 640 / 10000, samples: 256, loss: 226.037934\n",
      "iteration 650 / 10000, samples: 256, loss: 291.395092\n",
      "iteration 660 / 10000, samples: 256, loss: 281.371924\n",
      "iteration 670 / 10000, samples: 256, loss: 268.588840\n",
      "iteration 680 / 10000, samples: 256, loss: 227.147691\n",
      "iteration 690 / 10000, samples: 256, loss: 196.234393\n",
      "iteration 700 / 10000, samples: 256, loss: 200.523966\n",
      "iteration 710 / 10000, samples: 256, loss: 472.208366\n",
      "iteration 720 / 10000, samples: 256, loss: 325.225616\n",
      "iteration 730 / 10000, samples: 256, loss: 362.115585\n",
      "iteration 740 / 10000, samples: 256, loss: 238.487464\n",
      "iteration 750 / 10000, samples: 256, loss: 222.470716\n",
      "iteration 760 / 10000, samples: 256, loss: 264.039896\n",
      "iteration 770 / 10000, samples: 256, loss: 257.232840\n",
      "iteration 780 / 10000, samples: 256, loss: 350.231172\n",
      "iteration 790 / 10000, samples: 256, loss: 199.083720\n",
      "iteration 800 / 10000, samples: 256, loss: 356.042376\n",
      "iteration 810 / 10000, samples: 256, loss: 241.183203\n",
      "iteration 820 / 10000, samples: 256, loss: 252.201821\n",
      "iteration 830 / 10000, samples: 256, loss: 232.243834\n",
      "iteration 840 / 10000, samples: 256, loss: 266.775598\n",
      "iteration 850 / 10000, samples: 256, loss: 334.497467\n",
      "iteration 860 / 10000, samples: 256, loss: 202.876513\n",
      "iteration 870 / 10000, samples: 256, loss: 348.527292\n",
      "iteration 880 / 10000, samples: 256, loss: 216.657955\n",
      "iteration 890 / 10000, samples: 256, loss: 202.957761\n",
      "iteration 900 / 10000, samples: 256, loss: 390.616585\n",
      "iteration 910 / 10000, samples: 256, loss: 232.641059\n",
      "iteration 920 / 10000, samples: 256, loss: 313.742406\n",
      "iteration 930 / 10000, samples: 256, loss: 250.755148\n",
      "iteration 940 / 10000, samples: 256, loss: 286.072205\n",
      "iteration 950 / 10000, samples: 256, loss: 248.770001\n",
      "iteration 960 / 10000, samples: 256, loss: 194.489191\n",
      "iteration 970 / 10000, samples: 256, loss: 364.131183\n",
      "iteration 980 / 10000, samples: 256, loss: 157.937148\n",
      "iteration 990 / 10000, samples: 256, loss: 236.258923\n",
      "iteration 1000 / 10000, samples: 256, loss: 240.149223\n",
      "iteration 1010 / 10000, samples: 256, loss: 308.821640\n",
      "iteration 1020 / 10000, samples: 256, loss: 345.190349\n",
      "iteration 1030 / 10000, samples: 256, loss: 194.159024\n",
      "iteration 1040 / 10000, samples: 256, loss: 291.016786\n",
      "iteration 1050 / 10000, samples: 256, loss: 349.430226\n",
      "iteration 1060 / 10000, samples: 256, loss: 245.376070\n",
      "iteration 1070 / 10000, samples: 256, loss: 201.412584\n",
      "iteration 1080 / 10000, samples: 256, loss: 262.377913\n",
      "iteration 1090 / 10000, samples: 256, loss: 319.891353\n",
      "iteration 1100 / 10000, samples: 256, loss: 348.850100\n",
      "iteration 1110 / 10000, samples: 256, loss: 300.879701\n",
      "iteration 1120 / 10000, samples: 256, loss: 223.279456\n",
      "iteration 1130 / 10000, samples: 256, loss: 345.727737\n",
      "iteration 1140 / 10000, samples: 256, loss: 184.308299\n",
      "iteration 1150 / 10000, samples: 256, loss: 256.913831\n",
      "iteration 1160 / 10000, samples: 256, loss: 161.283209\n",
      "iteration 1170 / 10000, samples: 256, loss: 233.819845\n",
      "iteration 1180 / 10000, samples: 256, loss: 337.872758\n",
      "iteration 1190 / 10000, samples: 256, loss: 192.154159\n",
      "iteration 1200 / 10000, samples: 256, loss: 369.592618\n",
      "iteration 1210 / 10000, samples: 256, loss: 234.504486\n",
      "iteration 1220 / 10000, samples: 256, loss: 309.520691\n",
      "iteration 1230 / 10000, samples: 256, loss: 230.003256\n",
      "iteration 1240 / 10000, samples: 256, loss: 219.678373\n",
      "iteration 1250 / 10000, samples: 256, loss: 188.539485\n",
      "iteration 1260 / 10000, samples: 256, loss: 272.641150\n",
      "iteration 1270 / 10000, samples: 256, loss: 326.679422\n",
      "iteration 1280 / 10000, samples: 256, loss: 230.794377\n",
      "iteration 1290 / 10000, samples: 256, loss: 317.546783\n",
      "iteration 1300 / 10000, samples: 256, loss: 207.285653\n",
      "iteration 1310 / 10000, samples: 256, loss: 254.760701\n",
      "iteration 1320 / 10000, samples: 256, loss: 214.434273\n",
      "iteration 1330 / 10000, samples: 256, loss: 300.891861\n",
      "iteration 1340 / 10000, samples: 256, loss: 256.595561\n",
      "iteration 1350 / 10000, samples: 256, loss: 322.110930\n",
      "iteration 1360 / 10000, samples: 256, loss: 277.020541\n",
      "iteration 1370 / 10000, samples: 256, loss: 236.435474\n",
      "iteration 1380 / 10000, samples: 256, loss: 339.595903\n",
      "iteration 1390 / 10000, samples: 256, loss: 315.138822\n",
      "iteration 1400 / 10000, samples: 256, loss: 223.512776\n",
      "iteration 1410 / 10000, samples: 256, loss: 266.120541\n",
      "iteration 1420 / 10000, samples: 256, loss: 231.393029\n",
      "iteration 1430 / 10000, samples: 256, loss: 341.514371\n",
      "iteration 1440 / 10000, samples: 256, loss: 177.212963\n",
      "iteration 1450 / 10000, samples: 256, loss: 213.433651\n",
      "iteration 1460 / 10000, samples: 256, loss: 275.874856\n",
      "iteration 1470 / 10000, samples: 256, loss: 270.448805\n",
      "iteration 1480 / 10000, samples: 256, loss: 227.362230\n",
      "iteration 1490 / 10000, samples: 256, loss: 279.696012\n",
      "iteration 1500 / 10000, samples: 256, loss: 160.004647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1510 / 10000, samples: 256, loss: 217.408580\n",
      "iteration 1520 / 10000, samples: 256, loss: 352.329090\n",
      "iteration 1530 / 10000, samples: 256, loss: 253.091046\n",
      "iteration 1540 / 10000, samples: 256, loss: 207.243529\n",
      "iteration 1550 / 10000, samples: 256, loss: 331.976200\n",
      "iteration 1560 / 10000, samples: 256, loss: 271.411037\n",
      "iteration 1570 / 10000, samples: 256, loss: 329.784995\n",
      "iteration 1580 / 10000, samples: 256, loss: 336.940116\n",
      "iteration 1590 / 10000, samples: 256, loss: 179.763583\n",
      "iteration 1600 / 10000, samples: 256, loss: 295.757840\n",
      "iteration 1610 / 10000, samples: 256, loss: 254.373743\n",
      "iteration 1620 / 10000, samples: 256, loss: 264.194423\n",
      "iteration 1630 / 10000, samples: 256, loss: 226.534805\n",
      "iteration 1640 / 10000, samples: 256, loss: 336.846412\n",
      "iteration 1650 / 10000, samples: 256, loss: 262.482532\n",
      "iteration 1660 / 10000, samples: 256, loss: 262.070352\n",
      "iteration 1670 / 10000, samples: 256, loss: 405.450126\n",
      "iteration 1680 / 10000, samples: 256, loss: 316.205814\n",
      "iteration 1690 / 10000, samples: 256, loss: 300.734862\n",
      "iteration 1700 / 10000, samples: 256, loss: 315.926169\n",
      "iteration 1710 / 10000, samples: 256, loss: 333.455834\n",
      "iteration 1720 / 10000, samples: 256, loss: 285.573334\n",
      "iteration 1730 / 10000, samples: 256, loss: 255.273767\n",
      "iteration 1740 / 10000, samples: 256, loss: 265.263389\n",
      "iteration 1750 / 10000, samples: 256, loss: 240.288068\n",
      "iteration 1760 / 10000, samples: 256, loss: 405.168174\n",
      "iteration 1770 / 10000, samples: 256, loss: 282.541685\n",
      "iteration 1780 / 10000, samples: 256, loss: 243.360472\n",
      "iteration 1790 / 10000, samples: 256, loss: 175.439539\n",
      "iteration 1800 / 10000, samples: 256, loss: 202.498861\n",
      "iteration 1810 / 10000, samples: 256, loss: 243.689205\n",
      "iteration 1820 / 10000, samples: 256, loss: 275.216378\n",
      "iteration 1830 / 10000, samples: 256, loss: 230.804025\n",
      "iteration 1840 / 10000, samples: 256, loss: 332.043734\n",
      "iteration 1850 / 10000, samples: 256, loss: 216.489910\n",
      "iteration 1860 / 10000, samples: 256, loss: 273.131569\n",
      "iteration 1870 / 10000, samples: 256, loss: 246.351345\n",
      "iteration 1880 / 10000, samples: 256, loss: 256.758705\n",
      "iteration 1890 / 10000, samples: 256, loss: 193.334667\n",
      "iteration 1900 / 10000, samples: 256, loss: 194.942229\n",
      "iteration 1910 / 10000, samples: 256, loss: 301.456391\n",
      "iteration 1920 / 10000, samples: 256, loss: 239.562637\n",
      "iteration 1930 / 10000, samples: 256, loss: 311.925406\n",
      "iteration 1940 / 10000, samples: 256, loss: 301.656499\n",
      "iteration 1950 / 10000, samples: 256, loss: 246.490067\n",
      "iteration 1960 / 10000, samples: 256, loss: 283.236440\n",
      "iteration 1970 / 10000, samples: 256, loss: 307.751841\n",
      "iteration 1980 / 10000, samples: 256, loss: 279.057918\n",
      "iteration 1990 / 10000, samples: 256, loss: 193.698766\n",
      "iteration 2000 / 10000, samples: 256, loss: 292.764152\n",
      "iteration 2010 / 10000, samples: 256, loss: 236.254115\n",
      "iteration 2020 / 10000, samples: 256, loss: 167.772571\n",
      "iteration 2030 / 10000, samples: 256, loss: 249.454024\n",
      "iteration 2040 / 10000, samples: 256, loss: 338.686892\n",
      "iteration 2050 / 10000, samples: 256, loss: 262.231192\n",
      "iteration 2060 / 10000, samples: 256, loss: 255.930045\n",
      "iteration 2070 / 10000, samples: 256, loss: 182.131576\n",
      "iteration 2080 / 10000, samples: 256, loss: 273.108948\n",
      "iteration 2090 / 10000, samples: 256, loss: 265.122914\n",
      "iteration 2100 / 10000, samples: 256, loss: 234.147406\n",
      "iteration 2110 / 10000, samples: 256, loss: 173.395507\n",
      "iteration 2120 / 10000, samples: 256, loss: 375.439545\n",
      "iteration 2130 / 10000, samples: 256, loss: 336.199641\n",
      "iteration 2140 / 10000, samples: 256, loss: 289.057089\n",
      "iteration 2150 / 10000, samples: 256, loss: 215.224889\n",
      "iteration 2160 / 10000, samples: 256, loss: 230.747072\n",
      "iteration 2170 / 10000, samples: 256, loss: 286.440006\n",
      "iteration 2180 / 10000, samples: 256, loss: 228.273709\n",
      "iteration 2190 / 10000, samples: 256, loss: 320.590024\n",
      "iteration 2200 / 10000, samples: 256, loss: 250.750504\n",
      "iteration 2210 / 10000, samples: 256, loss: 275.499386\n",
      "iteration 2220 / 10000, samples: 256, loss: 213.998817\n",
      "iteration 2230 / 10000, samples: 256, loss: 219.476716\n",
      "iteration 2240 / 10000, samples: 256, loss: 296.726018\n",
      "iteration 2250 / 10000, samples: 256, loss: 215.989032\n",
      "iteration 2260 / 10000, samples: 256, loss: 411.604053\n",
      "iteration 2270 / 10000, samples: 256, loss: 339.859316\n",
      "iteration 2280 / 10000, samples: 256, loss: 208.784606\n",
      "iteration 2290 / 10000, samples: 256, loss: 328.902220\n",
      "iteration 2300 / 10000, samples: 256, loss: 295.501389\n",
      "iteration 2310 / 10000, samples: 256, loss: 429.802160\n",
      "iteration 2320 / 10000, samples: 256, loss: 292.891568\n",
      "iteration 2330 / 10000, samples: 256, loss: 181.994434\n",
      "iteration 2340 / 10000, samples: 256, loss: 174.977134\n",
      "iteration 2350 / 10000, samples: 256, loss: 220.355105\n",
      "iteration 2360 / 10000, samples: 256, loss: 255.321414\n",
      "iteration 2370 / 10000, samples: 256, loss: 285.680943\n",
      "iteration 2380 / 10000, samples: 256, loss: 396.187708\n",
      "iteration 2390 / 10000, samples: 256, loss: 257.479147\n",
      "iteration 2400 / 10000, samples: 256, loss: 219.950424\n",
      "iteration 2410 / 10000, samples: 256, loss: 348.127362\n",
      "iteration 2420 / 10000, samples: 256, loss: 297.879498\n",
      "iteration 2430 / 10000, samples: 256, loss: 306.338031\n",
      "iteration 2440 / 10000, samples: 256, loss: 322.910192\n",
      "iteration 2450 / 10000, samples: 256, loss: 404.781669\n",
      "iteration 2460 / 10000, samples: 256, loss: 286.504844\n",
      "iteration 2470 / 10000, samples: 256, loss: 215.109041\n",
      "iteration 2480 / 10000, samples: 256, loss: 285.569954\n",
      "iteration 2490 / 10000, samples: 256, loss: 264.734079\n",
      "iteration 2500 / 10000, samples: 256, loss: 307.860702\n",
      "iteration 2510 / 10000, samples: 256, loss: 325.583650\n",
      "iteration 2520 / 10000, samples: 256, loss: 241.172672\n",
      "iteration 2530 / 10000, samples: 256, loss: 334.757088\n",
      "iteration 2540 / 10000, samples: 256, loss: 285.019216\n",
      "iteration 2550 / 10000, samples: 256, loss: 295.582065\n",
      "iteration 2560 / 10000, samples: 256, loss: 265.767550\n",
      "iteration 2570 / 10000, samples: 256, loss: 305.446470\n",
      "iteration 2580 / 10000, samples: 256, loss: 353.995767\n",
      "iteration 2590 / 10000, samples: 256, loss: 255.489422\n",
      "iteration 2600 / 10000, samples: 256, loss: 301.522021\n",
      "iteration 2610 / 10000, samples: 256, loss: 250.800787\n",
      "iteration 2620 / 10000, samples: 256, loss: 187.222383\n",
      "iteration 2630 / 10000, samples: 256, loss: 206.466379\n",
      "iteration 2640 / 10000, samples: 256, loss: 304.791075\n",
      "iteration 2650 / 10000, samples: 256, loss: 188.462960\n",
      "iteration 2660 / 10000, samples: 256, loss: 170.300203\n",
      "iteration 2670 / 10000, samples: 256, loss: 277.023179\n",
      "iteration 2680 / 10000, samples: 256, loss: 248.123246\n",
      "iteration 2690 / 10000, samples: 256, loss: 384.678029\n",
      "iteration 2700 / 10000, samples: 256, loss: 270.427268\n",
      "iteration 2710 / 10000, samples: 256, loss: 346.671589\n",
      "iteration 2720 / 10000, samples: 256, loss: 323.767985\n",
      "iteration 2730 / 10000, samples: 256, loss: 261.280365\n",
      "iteration 2740 / 10000, samples: 256, loss: 273.589513\n",
      "iteration 2750 / 10000, samples: 256, loss: 262.305425\n",
      "iteration 2760 / 10000, samples: 256, loss: 267.367067\n",
      "iteration 2770 / 10000, samples: 256, loss: 193.897790\n",
      "iteration 2780 / 10000, samples: 256, loss: 299.049467\n",
      "iteration 2790 / 10000, samples: 256, loss: 332.253866\n",
      "iteration 2800 / 10000, samples: 256, loss: 226.036349\n",
      "iteration 2810 / 10000, samples: 256, loss: 214.980683\n",
      "iteration 2820 / 10000, samples: 256, loss: 345.845064\n",
      "iteration 2830 / 10000, samples: 256, loss: 198.761062\n",
      "iteration 2840 / 10000, samples: 256, loss: 350.194002\n",
      "iteration 2850 / 10000, samples: 256, loss: 256.206415\n",
      "iteration 2860 / 10000, samples: 256, loss: 358.356965\n",
      "iteration 2870 / 10000, samples: 256, loss: 344.586059\n",
      "iteration 2880 / 10000, samples: 256, loss: 382.239326\n",
      "iteration 2890 / 10000, samples: 256, loss: 477.586527\n",
      "iteration 2900 / 10000, samples: 256, loss: 230.296537\n",
      "iteration 2910 / 10000, samples: 256, loss: 288.607970\n",
      "iteration 2920 / 10000, samples: 256, loss: 136.405716\n",
      "iteration 2930 / 10000, samples: 256, loss: 172.364651\n",
      "iteration 2940 / 10000, samples: 256, loss: 364.255015\n",
      "iteration 2950 / 10000, samples: 256, loss: 215.264766\n",
      "iteration 2960 / 10000, samples: 256, loss: 259.849192\n",
      "iteration 2970 / 10000, samples: 256, loss: 249.788824\n",
      "iteration 2980 / 10000, samples: 256, loss: 293.254366\n",
      "iteration 2990 / 10000, samples: 256, loss: 349.246195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000 / 10000, samples: 256, loss: 221.418613\n",
      "iteration 3010 / 10000, samples: 256, loss: 219.631591\n",
      "iteration 3020 / 10000, samples: 256, loss: 426.214376\n",
      "iteration 3030 / 10000, samples: 256, loss: 181.161484\n",
      "iteration 3040 / 10000, samples: 256, loss: 254.068958\n",
      "iteration 3050 / 10000, samples: 256, loss: 185.482487\n",
      "iteration 3060 / 10000, samples: 256, loss: 230.757724\n",
      "iteration 3070 / 10000, samples: 256, loss: 200.745903\n",
      "iteration 3080 / 10000, samples: 256, loss: 273.225695\n",
      "iteration 3090 / 10000, samples: 256, loss: 264.586224\n",
      "iteration 3100 / 10000, samples: 256, loss: 259.379901\n",
      "iteration 3110 / 10000, samples: 256, loss: 234.097825\n",
      "iteration 3120 / 10000, samples: 256, loss: 243.064261\n",
      "iteration 3130 / 10000, samples: 256, loss: 262.251208\n",
      "iteration 3140 / 10000, samples: 256, loss: 298.116732\n",
      "iteration 3150 / 10000, samples: 256, loss: 237.621564\n",
      "iteration 3160 / 10000, samples: 256, loss: 235.582854\n",
      "iteration 3170 / 10000, samples: 256, loss: 218.403517\n",
      "iteration 3180 / 10000, samples: 256, loss: 362.462042\n",
      "iteration 3190 / 10000, samples: 256, loss: 268.276729\n",
      "iteration 3200 / 10000, samples: 256, loss: 318.679850\n",
      "iteration 3210 / 10000, samples: 256, loss: 237.466526\n",
      "iteration 3220 / 10000, samples: 256, loss: 288.285561\n",
      "iteration 3230 / 10000, samples: 256, loss: 292.032826\n",
      "iteration 3240 / 10000, samples: 256, loss: 226.609064\n",
      "iteration 3250 / 10000, samples: 256, loss: 304.462264\n",
      "iteration 3260 / 10000, samples: 256, loss: 223.668835\n",
      "iteration 3270 / 10000, samples: 256, loss: 329.414100\n",
      "iteration 3280 / 10000, samples: 256, loss: 251.032601\n",
      "iteration 3290 / 10000, samples: 256, loss: 376.125182\n",
      "iteration 3300 / 10000, samples: 256, loss: 201.396069\n",
      "iteration 3310 / 10000, samples: 256, loss: 263.827899\n",
      "iteration 3320 / 10000, samples: 256, loss: 174.233301\n",
      "iteration 3330 / 10000, samples: 256, loss: 230.834732\n",
      "iteration 3340 / 10000, samples: 256, loss: 179.393754\n",
      "iteration 3350 / 10000, samples: 256, loss: 308.548587\n",
      "iteration 3360 / 10000, samples: 256, loss: 182.593404\n",
      "iteration 3370 / 10000, samples: 256, loss: 298.371677\n",
      "iteration 3380 / 10000, samples: 256, loss: 316.870530\n",
      "iteration 3390 / 10000, samples: 256, loss: 227.972005\n",
      "iteration 3400 / 10000, samples: 256, loss: 217.557565\n",
      "iteration 3410 / 10000, samples: 256, loss: 263.717099\n",
      "iteration 3420 / 10000, samples: 256, loss: 275.826270\n",
      "iteration 3430 / 10000, samples: 256, loss: 242.468926\n",
      "iteration 3440 / 10000, samples: 256, loss: 287.002262\n",
      "iteration 3450 / 10000, samples: 256, loss: 204.674123\n",
      "iteration 3460 / 10000, samples: 256, loss: 212.646550\n",
      "iteration 3470 / 10000, samples: 256, loss: 192.310245\n",
      "iteration 3480 / 10000, samples: 256, loss: 258.965332\n",
      "iteration 3490 / 10000, samples: 256, loss: 179.348482\n",
      "iteration 3500 / 10000, samples: 256, loss: 343.811453\n",
      "iteration 3510 / 10000, samples: 256, loss: 195.159406\n",
      "iteration 3520 / 10000, samples: 256, loss: 313.540681\n",
      "iteration 3530 / 10000, samples: 256, loss: 201.939228\n",
      "iteration 3540 / 10000, samples: 256, loss: 367.309029\n",
      "iteration 3550 / 10000, samples: 256, loss: 284.640094\n",
      "iteration 3560 / 10000, samples: 256, loss: 193.967233\n",
      "iteration 3570 / 10000, samples: 256, loss: 250.060612\n",
      "iteration 3580 / 10000, samples: 256, loss: 268.565172\n",
      "iteration 3590 / 10000, samples: 256, loss: 310.294792\n",
      "iteration 3600 / 10000, samples: 256, loss: 233.034902\n",
      "iteration 3610 / 10000, samples: 256, loss: 200.967004\n",
      "iteration 3620 / 10000, samples: 256, loss: 266.958643\n",
      "iteration 3630 / 10000, samples: 256, loss: 244.835165\n",
      "iteration 3640 / 10000, samples: 256, loss: 264.194709\n",
      "iteration 3650 / 10000, samples: 256, loss: 440.752229\n",
      "iteration 3660 / 10000, samples: 256, loss: 211.027683\n",
      "iteration 3670 / 10000, samples: 256, loss: 327.586122\n",
      "iteration 3680 / 10000, samples: 256, loss: 322.215572\n",
      "iteration 3690 / 10000, samples: 256, loss: 186.338710\n",
      "iteration 3700 / 10000, samples: 256, loss: 303.954603\n",
      "iteration 3710 / 10000, samples: 256, loss: 268.552349\n",
      "iteration 3720 / 10000, samples: 256, loss: 292.400141\n",
      "iteration 3730 / 10000, samples: 256, loss: 206.133136\n",
      "iteration 3740 / 10000, samples: 256, loss: 195.881185\n",
      "iteration 3750 / 10000, samples: 256, loss: 310.755054\n",
      "iteration 3760 / 10000, samples: 256, loss: 263.019555\n",
      "iteration 3770 / 10000, samples: 256, loss: 189.168169\n",
      "iteration 3780 / 10000, samples: 256, loss: 270.951241\n",
      "iteration 3790 / 10000, samples: 256, loss: 167.760466\n",
      "iteration 3800 / 10000, samples: 256, loss: 166.377848\n",
      "iteration 3810 / 10000, samples: 256, loss: 159.071481\n",
      "iteration 3820 / 10000, samples: 256, loss: 303.599796\n",
      "iteration 3830 / 10000, samples: 256, loss: 211.354628\n",
      "iteration 3840 / 10000, samples: 256, loss: 218.661457\n",
      "iteration 3850 / 10000, samples: 256, loss: 379.256005\n",
      "iteration 3860 / 10000, samples: 256, loss: 298.267240\n",
      "iteration 3870 / 10000, samples: 256, loss: 205.957809\n",
      "iteration 3880 / 10000, samples: 256, loss: 173.650144\n",
      "iteration 3890 / 10000, samples: 256, loss: 184.668660\n",
      "iteration 3900 / 10000, samples: 256, loss: 254.029368\n",
      "iteration 3910 / 10000, samples: 256, loss: 370.770169\n",
      "iteration 3920 / 10000, samples: 256, loss: 263.688760\n",
      "iteration 3930 / 10000, samples: 256, loss: 229.310305\n",
      "iteration 3940 / 10000, samples: 256, loss: 179.250906\n",
      "iteration 3950 / 10000, samples: 256, loss: 211.175054\n",
      "iteration 3960 / 10000, samples: 256, loss: 212.420693\n",
      "iteration 3970 / 10000, samples: 256, loss: 271.040070\n",
      "iteration 3980 / 10000, samples: 256, loss: 309.380922\n",
      "iteration 3990 / 10000, samples: 256, loss: 267.449557\n",
      "iteration 4000 / 10000, samples: 256, loss: 290.978161\n",
      "iteration 4010 / 10000, samples: 256, loss: 280.834766\n",
      "iteration 4020 / 10000, samples: 256, loss: 238.827326\n",
      "iteration 4030 / 10000, samples: 256, loss: 210.816961\n",
      "iteration 4040 / 10000, samples: 256, loss: 229.927436\n",
      "iteration 4050 / 10000, samples: 256, loss: 199.052857\n",
      "iteration 4060 / 10000, samples: 256, loss: 282.184132\n",
      "iteration 4070 / 10000, samples: 256, loss: 215.110933\n",
      "iteration 4080 / 10000, samples: 256, loss: 347.606776\n",
      "iteration 4090 / 10000, samples: 256, loss: 199.392722\n",
      "iteration 4100 / 10000, samples: 256, loss: 370.851028\n",
      "iteration 4110 / 10000, samples: 256, loss: 248.563840\n",
      "iteration 4120 / 10000, samples: 256, loss: 235.699544\n",
      "iteration 4130 / 10000, samples: 256, loss: 258.217523\n",
      "iteration 4140 / 10000, samples: 256, loss: 316.830048\n",
      "iteration 4150 / 10000, samples: 256, loss: 221.123929\n",
      "iteration 4160 / 10000, samples: 256, loss: 347.219156\n",
      "iteration 4170 / 10000, samples: 256, loss: 184.358892\n",
      "iteration 4180 / 10000, samples: 256, loss: 342.389623\n",
      "iteration 4190 / 10000, samples: 256, loss: 235.591311\n",
      "iteration 4200 / 10000, samples: 256, loss: 354.772908\n",
      "iteration 4210 / 10000, samples: 256, loss: 289.783369\n",
      "iteration 4220 / 10000, samples: 256, loss: 177.749373\n",
      "iteration 4230 / 10000, samples: 256, loss: 261.721613\n",
      "iteration 4240 / 10000, samples: 256, loss: 249.506392\n",
      "iteration 4250 / 10000, samples: 256, loss: 328.327441\n",
      "iteration 4260 / 10000, samples: 256, loss: 225.873384\n",
      "iteration 4270 / 10000, samples: 256, loss: 205.809178\n",
      "iteration 4280 / 10000, samples: 256, loss: 208.098941\n",
      "iteration 4290 / 10000, samples: 256, loss: 229.532234\n",
      "iteration 4300 / 10000, samples: 256, loss: 225.402018\n",
      "iteration 4310 / 10000, samples: 256, loss: 323.898500\n",
      "iteration 4320 / 10000, samples: 256, loss: 311.927886\n",
      "iteration 4330 / 10000, samples: 256, loss: 421.722348\n",
      "iteration 4340 / 10000, samples: 256, loss: 202.898712\n",
      "iteration 4350 / 10000, samples: 256, loss: 148.787741\n",
      "iteration 4360 / 10000, samples: 256, loss: 183.638207\n",
      "iteration 4370 / 10000, samples: 256, loss: 172.687103\n",
      "iteration 4380 / 10000, samples: 256, loss: 254.100978\n",
      "iteration 4390 / 10000, samples: 256, loss: 188.000243\n",
      "iteration 4400 / 10000, samples: 256, loss: 199.767109\n",
      "iteration 4410 / 10000, samples: 256, loss: 325.578917\n",
      "iteration 4420 / 10000, samples: 256, loss: 371.251116\n",
      "iteration 4430 / 10000, samples: 256, loss: 252.481448\n",
      "iteration 4440 / 10000, samples: 256, loss: 281.607025\n",
      "iteration 4450 / 10000, samples: 256, loss: 183.715952\n",
      "iteration 4460 / 10000, samples: 256, loss: 172.432491\n",
      "iteration 4470 / 10000, samples: 256, loss: 236.724880\n",
      "iteration 4480 / 10000, samples: 256, loss: 261.167441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4490 / 10000, samples: 256, loss: 221.871507\n",
      "iteration 4500 / 10000, samples: 256, loss: 237.375326\n",
      "iteration 4510 / 10000, samples: 256, loss: 302.382985\n",
      "iteration 4520 / 10000, samples: 256, loss: 249.441348\n",
      "iteration 4530 / 10000, samples: 256, loss: 207.299195\n",
      "iteration 4540 / 10000, samples: 256, loss: 345.781957\n",
      "iteration 4550 / 10000, samples: 256, loss: 188.993721\n",
      "iteration 4560 / 10000, samples: 256, loss: 221.534107\n",
      "iteration 4570 / 10000, samples: 256, loss: 203.777095\n",
      "iteration 4580 / 10000, samples: 256, loss: 366.108027\n",
      "iteration 4590 / 10000, samples: 256, loss: 403.658607\n",
      "iteration 4600 / 10000, samples: 256, loss: 263.544144\n",
      "iteration 4610 / 10000, samples: 256, loss: 184.565110\n",
      "iteration 4620 / 10000, samples: 256, loss: 158.585723\n",
      "iteration 4630 / 10000, samples: 256, loss: 329.599254\n",
      "iteration 4640 / 10000, samples: 256, loss: 245.024884\n",
      "iteration 4650 / 10000, samples: 256, loss: 236.015098\n",
      "iteration 4660 / 10000, samples: 256, loss: 200.065944\n",
      "iteration 4670 / 10000, samples: 256, loss: 215.688083\n",
      "iteration 4680 / 10000, samples: 256, loss: 404.068424\n",
      "iteration 4690 / 10000, samples: 256, loss: 316.709245\n",
      "iteration 4700 / 10000, samples: 256, loss: 267.563558\n",
      "iteration 4710 / 10000, samples: 256, loss: 218.672528\n",
      "iteration 4720 / 10000, samples: 256, loss: 303.860854\n",
      "iteration 4730 / 10000, samples: 256, loss: 237.892869\n",
      "iteration 4740 / 10000, samples: 256, loss: 245.511828\n",
      "iteration 4750 / 10000, samples: 256, loss: 265.357663\n",
      "iteration 4760 / 10000, samples: 256, loss: 251.098913\n",
      "iteration 4770 / 10000, samples: 256, loss: 196.918436\n",
      "iteration 4780 / 10000, samples: 256, loss: 346.613225\n",
      "iteration 4790 / 10000, samples: 256, loss: 219.522441\n",
      "iteration 4800 / 10000, samples: 256, loss: 246.258475\n",
      "iteration 4810 / 10000, samples: 256, loss: 269.827681\n",
      "iteration 4820 / 10000, samples: 256, loss: 282.922212\n",
      "iteration 4830 / 10000, samples: 256, loss: 312.585294\n",
      "iteration 4840 / 10000, samples: 256, loss: 280.632884\n",
      "iteration 4850 / 10000, samples: 256, loss: 261.476926\n",
      "iteration 4860 / 10000, samples: 256, loss: 416.355347\n",
      "iteration 4870 / 10000, samples: 256, loss: 432.693703\n",
      "iteration 4880 / 10000, samples: 256, loss: 173.654428\n",
      "iteration 4890 / 10000, samples: 256, loss: 160.829068\n",
      "iteration 4900 / 10000, samples: 256, loss: 238.737148\n",
      "iteration 4910 / 10000, samples: 256, loss: 228.375665\n",
      "iteration 4920 / 10000, samples: 256, loss: 331.248759\n",
      "iteration 4930 / 10000, samples: 256, loss: 260.234492\n",
      "iteration 4940 / 10000, samples: 256, loss: 330.866330\n",
      "iteration 4950 / 10000, samples: 256, loss: 179.763499\n",
      "iteration 4960 / 10000, samples: 256, loss: 218.595109\n",
      "iteration 4970 / 10000, samples: 256, loss: 249.058517\n",
      "iteration 4980 / 10000, samples: 256, loss: 233.700105\n",
      "iteration 4990 / 10000, samples: 256, loss: 204.555422\n",
      "iteration 5000 / 10000, samples: 256, loss: 266.264251\n",
      "iteration 5010 / 10000, samples: 256, loss: 263.663200\n",
      "iteration 5020 / 10000, samples: 256, loss: 215.481700\n",
      "iteration 5030 / 10000, samples: 256, loss: 174.961372\n",
      "iteration 5040 / 10000, samples: 256, loss: 352.256093\n",
      "iteration 5050 / 10000, samples: 256, loss: 204.813607\n",
      "iteration 5060 / 10000, samples: 256, loss: 253.148019\n",
      "iteration 5070 / 10000, samples: 256, loss: 275.370878\n",
      "iteration 5080 / 10000, samples: 256, loss: 232.629595\n",
      "iteration 5090 / 10000, samples: 256, loss: 376.548621\n",
      "iteration 5100 / 10000, samples: 256, loss: 140.619287\n",
      "iteration 5110 / 10000, samples: 256, loss: 254.154179\n",
      "iteration 5120 / 10000, samples: 256, loss: 275.638595\n",
      "iteration 5130 / 10000, samples: 256, loss: 229.332604\n",
      "iteration 5140 / 10000, samples: 256, loss: 220.253874\n",
      "iteration 5150 / 10000, samples: 256, loss: 221.192720\n",
      "iteration 5160 / 10000, samples: 256, loss: 217.276548\n",
      "iteration 5170 / 10000, samples: 256, loss: 214.918570\n",
      "iteration 5180 / 10000, samples: 256, loss: 333.604161\n",
      "iteration 5190 / 10000, samples: 256, loss: 174.040788\n",
      "iteration 5200 / 10000, samples: 256, loss: 267.252022\n",
      "iteration 5210 / 10000, samples: 256, loss: 168.721309\n",
      "iteration 5220 / 10000, samples: 256, loss: 276.629668\n",
      "iteration 5230 / 10000, samples: 256, loss: 249.586252\n",
      "iteration 5240 / 10000, samples: 256, loss: 290.043237\n",
      "iteration 5250 / 10000, samples: 256, loss: 290.482467\n",
      "iteration 5260 / 10000, samples: 256, loss: 304.118317\n",
      "iteration 5270 / 10000, samples: 256, loss: 302.386503\n",
      "iteration 5280 / 10000, samples: 256, loss: 235.523867\n",
      "iteration 5290 / 10000, samples: 256, loss: 205.556765\n",
      "iteration 5300 / 10000, samples: 256, loss: 165.873920\n",
      "iteration 5310 / 10000, samples: 256, loss: 295.317393\n",
      "iteration 5320 / 10000, samples: 256, loss: 314.736055\n",
      "iteration 5330 / 10000, samples: 256, loss: 281.409600\n",
      "iteration 5340 / 10000, samples: 256, loss: 209.690930\n",
      "iteration 5350 / 10000, samples: 256, loss: 240.419445\n",
      "iteration 5360 / 10000, samples: 256, loss: 276.151491\n",
      "iteration 5370 / 10000, samples: 256, loss: 412.228504\n",
      "iteration 5380 / 10000, samples: 256, loss: 240.581251\n",
      "iteration 5390 / 10000, samples: 256, loss: 297.875663\n",
      "iteration 5400 / 10000, samples: 256, loss: 304.220095\n",
      "iteration 5410 / 10000, samples: 256, loss: 166.054310\n",
      "iteration 5420 / 10000, samples: 256, loss: 372.843025\n",
      "iteration 5430 / 10000, samples: 256, loss: 301.459009\n",
      "iteration 5440 / 10000, samples: 256, loss: 289.460277\n",
      "iteration 5450 / 10000, samples: 256, loss: 359.384667\n",
      "iteration 5460 / 10000, samples: 256, loss: 315.911341\n",
      "iteration 5470 / 10000, samples: 256, loss: 273.283040\n",
      "iteration 5480 / 10000, samples: 256, loss: 348.505125\n",
      "iteration 5490 / 10000, samples: 256, loss: 373.388521\n",
      "iteration 5500 / 10000, samples: 256, loss: 326.762594\n",
      "iteration 5510 / 10000, samples: 256, loss: 286.485555\n",
      "iteration 5520 / 10000, samples: 256, loss: 255.144474\n",
      "iteration 5530 / 10000, samples: 256, loss: 146.448686\n",
      "iteration 5540 / 10000, samples: 256, loss: 277.965275\n",
      "iteration 5550 / 10000, samples: 256, loss: 322.334517\n",
      "iteration 5560 / 10000, samples: 256, loss: 241.799303\n",
      "iteration 5570 / 10000, samples: 256, loss: 356.751907\n",
      "iteration 5580 / 10000, samples: 256, loss: 245.265312\n",
      "iteration 5590 / 10000, samples: 256, loss: 253.566549\n",
      "iteration 5600 / 10000, samples: 256, loss: 246.086399\n",
      "iteration 5610 / 10000, samples: 256, loss: 222.055278\n",
      "iteration 5620 / 10000, samples: 256, loss: 283.318800\n",
      "iteration 5630 / 10000, samples: 256, loss: 290.703821\n",
      "iteration 5640 / 10000, samples: 256, loss: 225.523217\n",
      "iteration 5650 / 10000, samples: 256, loss: 267.085540\n",
      "iteration 5660 / 10000, samples: 256, loss: 291.964853\n",
      "iteration 5670 / 10000, samples: 256, loss: 317.381361\n",
      "iteration 5680 / 10000, samples: 256, loss: 187.191131\n",
      "iteration 5690 / 10000, samples: 256, loss: 253.099752\n",
      "iteration 5700 / 10000, samples: 256, loss: 413.948940\n",
      "iteration 5710 / 10000, samples: 256, loss: 236.908791\n",
      "iteration 5720 / 10000, samples: 256, loss: 244.518044\n",
      "iteration 5730 / 10000, samples: 256, loss: 155.850332\n",
      "iteration 5740 / 10000, samples: 256, loss: 277.832823\n",
      "iteration 5750 / 10000, samples: 256, loss: 265.918697\n",
      "iteration 5760 / 10000, samples: 256, loss: 204.723032\n",
      "iteration 5770 / 10000, samples: 256, loss: 345.680200\n",
      "iteration 5780 / 10000, samples: 256, loss: 283.153967\n",
      "iteration 5790 / 10000, samples: 256, loss: 244.060675\n",
      "iteration 5800 / 10000, samples: 256, loss: 330.709713\n",
      "iteration 5810 / 10000, samples: 256, loss: 236.931907\n",
      "iteration 5820 / 10000, samples: 256, loss: 216.766722\n",
      "iteration 5830 / 10000, samples: 256, loss: 260.698989\n",
      "iteration 5840 / 10000, samples: 256, loss: 271.734119\n",
      "iteration 5850 / 10000, samples: 256, loss: 213.029681\n",
      "iteration 5860 / 10000, samples: 256, loss: 285.573741\n",
      "iteration 5870 / 10000, samples: 256, loss: 208.600558\n",
      "iteration 5880 / 10000, samples: 256, loss: 201.970167\n",
      "iteration 5890 / 10000, samples: 256, loss: 254.974088\n",
      "iteration 5900 / 10000, samples: 256, loss: 231.737113\n",
      "iteration 5910 / 10000, samples: 256, loss: 303.349599\n",
      "iteration 5920 / 10000, samples: 256, loss: 262.865166\n",
      "iteration 5930 / 10000, samples: 256, loss: 324.406746\n",
      "iteration 5940 / 10000, samples: 256, loss: 287.607743\n",
      "iteration 5950 / 10000, samples: 256, loss: 177.737232\n",
      "iteration 5960 / 10000, samples: 256, loss: 288.266155\n",
      "iteration 5970 / 10000, samples: 256, loss: 284.195417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5980 / 10000, samples: 256, loss: 304.464527\n",
      "iteration 5990 / 10000, samples: 256, loss: 302.065643\n",
      "iteration 6000 / 10000, samples: 256, loss: 283.565742\n",
      "iteration 6010 / 10000, samples: 256, loss: 301.713069\n",
      "iteration 6020 / 10000, samples: 256, loss: 312.404497\n",
      "iteration 6030 / 10000, samples: 256, loss: 221.274389\n",
      "iteration 6040 / 10000, samples: 256, loss: 304.293862\n",
      "iteration 6050 / 10000, samples: 256, loss: 238.971203\n",
      "iteration 6060 / 10000, samples: 256, loss: 339.261861\n",
      "iteration 6070 / 10000, samples: 256, loss: 369.122495\n",
      "iteration 6080 / 10000, samples: 256, loss: 173.968194\n",
      "iteration 6090 / 10000, samples: 256, loss: 251.134506\n",
      "iteration 6100 / 10000, samples: 256, loss: 279.747576\n",
      "iteration 6110 / 10000, samples: 256, loss: 317.579612\n",
      "iteration 6120 / 10000, samples: 256, loss: 195.916359\n",
      "iteration 6130 / 10000, samples: 256, loss: 331.411336\n",
      "iteration 6140 / 10000, samples: 256, loss: 295.532579\n",
      "iteration 6150 / 10000, samples: 256, loss: 267.490035\n",
      "iteration 6160 / 10000, samples: 256, loss: 299.532296\n",
      "iteration 6170 / 10000, samples: 256, loss: 245.900384\n",
      "iteration 6180 / 10000, samples: 256, loss: 292.634426\n",
      "iteration 6190 / 10000, samples: 256, loss: 281.394077\n",
      "iteration 6200 / 10000, samples: 256, loss: 231.623217\n",
      "iteration 6210 / 10000, samples: 256, loss: 229.221513\n",
      "iteration 6220 / 10000, samples: 256, loss: 295.438638\n",
      "iteration 6230 / 10000, samples: 256, loss: 251.706318\n",
      "iteration 6240 / 10000, samples: 256, loss: 281.016870\n",
      "iteration 6250 / 10000, samples: 256, loss: 365.909498\n",
      "iteration 6260 / 10000, samples: 256, loss: 192.957228\n",
      "iteration 6270 / 10000, samples: 256, loss: 320.803954\n",
      "iteration 6280 / 10000, samples: 256, loss: 259.130903\n",
      "iteration 6290 / 10000, samples: 256, loss: 247.540944\n",
      "iteration 6300 / 10000, samples: 256, loss: 273.349675\n",
      "iteration 6310 / 10000, samples: 256, loss: 403.550471\n",
      "iteration 6320 / 10000, samples: 256, loss: 172.947260\n",
      "iteration 6330 / 10000, samples: 256, loss: 217.070990\n",
      "iteration 6340 / 10000, samples: 256, loss: 328.138474\n",
      "iteration 6350 / 10000, samples: 256, loss: 249.991099\n",
      "iteration 6360 / 10000, samples: 256, loss: 344.962049\n",
      "iteration 6370 / 10000, samples: 256, loss: 362.127366\n",
      "iteration 6380 / 10000, samples: 256, loss: 179.018931\n",
      "iteration 6390 / 10000, samples: 256, loss: 201.223188\n",
      "iteration 6400 / 10000, samples: 256, loss: 313.279278\n",
      "iteration 6410 / 10000, samples: 256, loss: 244.854422\n",
      "iteration 6420 / 10000, samples: 256, loss: 273.915832\n",
      "iteration 6430 / 10000, samples: 256, loss: 306.074024\n",
      "iteration 6440 / 10000, samples: 256, loss: 300.309281\n",
      "iteration 6450 / 10000, samples: 256, loss: 311.849980\n",
      "iteration 6460 / 10000, samples: 256, loss: 194.897130\n",
      "iteration 6470 / 10000, samples: 256, loss: 334.541915\n",
      "iteration 6480 / 10000, samples: 256, loss: 257.972601\n",
      "iteration 6490 / 10000, samples: 256, loss: 264.058736\n",
      "iteration 6500 / 10000, samples: 256, loss: 356.484319\n",
      "iteration 6510 / 10000, samples: 256, loss: 257.911890\n",
      "iteration 6520 / 10000, samples: 256, loss: 226.539812\n",
      "iteration 6530 / 10000, samples: 256, loss: 205.566098\n",
      "iteration 6540 / 10000, samples: 256, loss: 352.404728\n",
      "iteration 6550 / 10000, samples: 256, loss: 239.140774\n",
      "iteration 6560 / 10000, samples: 256, loss: 302.493539\n",
      "iteration 6570 / 10000, samples: 256, loss: 314.126837\n",
      "iteration 6580 / 10000, samples: 256, loss: 225.960257\n",
      "iteration 6590 / 10000, samples: 256, loss: 384.521971\n",
      "iteration 6600 / 10000, samples: 256, loss: 231.970985\n",
      "iteration 6610 / 10000, samples: 256, loss: 140.794438\n",
      "iteration 6620 / 10000, samples: 256, loss: 213.885928\n",
      "iteration 6630 / 10000, samples: 256, loss: 240.113047\n",
      "iteration 6640 / 10000, samples: 256, loss: 196.809268\n",
      "iteration 6650 / 10000, samples: 256, loss: 162.257352\n",
      "iteration 6660 / 10000, samples: 256, loss: 231.611857\n",
      "iteration 6670 / 10000, samples: 256, loss: 262.809935\n",
      "iteration 6680 / 10000, samples: 256, loss: 279.973667\n",
      "iteration 6690 / 10000, samples: 256, loss: 211.350998\n",
      "iteration 6700 / 10000, samples: 256, loss: 187.541253\n",
      "iteration 6710 / 10000, samples: 256, loss: 252.244329\n",
      "iteration 6720 / 10000, samples: 256, loss: 274.556424\n",
      "iteration 6730 / 10000, samples: 256, loss: 200.177601\n",
      "iteration 6740 / 10000, samples: 256, loss: 207.229859\n",
      "iteration 6750 / 10000, samples: 256, loss: 207.191292\n",
      "iteration 6760 / 10000, samples: 256, loss: 210.561005\n",
      "iteration 6770 / 10000, samples: 256, loss: 269.098489\n",
      "iteration 6780 / 10000, samples: 256, loss: 314.682566\n",
      "iteration 6790 / 10000, samples: 256, loss: 168.086445\n",
      "iteration 6800 / 10000, samples: 256, loss: 393.141950\n",
      "iteration 6810 / 10000, samples: 256, loss: 216.234577\n",
      "iteration 6820 / 10000, samples: 256, loss: 281.951789\n",
      "iteration 6830 / 10000, samples: 256, loss: 221.548390\n",
      "iteration 6840 / 10000, samples: 256, loss: 247.730195\n",
      "iteration 6850 / 10000, samples: 256, loss: 345.436196\n",
      "iteration 6860 / 10000, samples: 256, loss: 302.999808\n",
      "iteration 6870 / 10000, samples: 256, loss: 152.603801\n",
      "iteration 6880 / 10000, samples: 256, loss: 184.907060\n",
      "iteration 6890 / 10000, samples: 256, loss: 227.119966\n",
      "iteration 6900 / 10000, samples: 256, loss: 181.292761\n",
      "iteration 6910 / 10000, samples: 256, loss: 336.716149\n",
      "iteration 6920 / 10000, samples: 256, loss: 231.390826\n",
      "iteration 6930 / 10000, samples: 256, loss: 230.460344\n",
      "iteration 6940 / 10000, samples: 256, loss: 379.743456\n",
      "iteration 6950 / 10000, samples: 256, loss: 309.726331\n",
      "iteration 6960 / 10000, samples: 256, loss: 185.662714\n",
      "iteration 6970 / 10000, samples: 256, loss: 243.299560\n",
      "iteration 6980 / 10000, samples: 256, loss: 304.994739\n",
      "iteration 6990 / 10000, samples: 256, loss: 276.334939\n",
      "iteration 7000 / 10000, samples: 256, loss: 239.272608\n",
      "iteration 7010 / 10000, samples: 256, loss: 228.178603\n",
      "iteration 7020 / 10000, samples: 256, loss: 252.567341\n",
      "iteration 7030 / 10000, samples: 256, loss: 263.247879\n",
      "iteration 7040 / 10000, samples: 256, loss: 220.657233\n",
      "iteration 7050 / 10000, samples: 256, loss: 236.658128\n",
      "iteration 7060 / 10000, samples: 256, loss: 348.879713\n",
      "iteration 7070 / 10000, samples: 256, loss: 302.239438\n",
      "iteration 7080 / 10000, samples: 256, loss: 226.422428\n",
      "iteration 7090 / 10000, samples: 256, loss: 203.695460\n",
      "iteration 7100 / 10000, samples: 256, loss: 257.897311\n",
      "iteration 7110 / 10000, samples: 256, loss: 241.130808\n",
      "iteration 7120 / 10000, samples: 256, loss: 315.203959\n",
      "iteration 7130 / 10000, samples: 256, loss: 322.410404\n",
      "iteration 7140 / 10000, samples: 256, loss: 204.849082\n",
      "iteration 7150 / 10000, samples: 256, loss: 329.351333\n",
      "iteration 7160 / 10000, samples: 256, loss: 254.144010\n",
      "iteration 7170 / 10000, samples: 256, loss: 303.659613\n",
      "iteration 7180 / 10000, samples: 256, loss: 263.575342\n",
      "iteration 7190 / 10000, samples: 256, loss: 227.662198\n",
      "iteration 7200 / 10000, samples: 256, loss: 309.743598\n",
      "iteration 7210 / 10000, samples: 256, loss: 267.835483\n",
      "iteration 7220 / 10000, samples: 256, loss: 218.874953\n",
      "iteration 7230 / 10000, samples: 256, loss: 228.534849\n",
      "iteration 7240 / 10000, samples: 256, loss: 275.390066\n",
      "iteration 7250 / 10000, samples: 256, loss: 154.198929\n",
      "iteration 7260 / 10000, samples: 256, loss: 211.708223\n",
      "iteration 7270 / 10000, samples: 256, loss: 291.104174\n",
      "iteration 7280 / 10000, samples: 256, loss: 201.937952\n",
      "iteration 7290 / 10000, samples: 256, loss: 227.420491\n",
      "iteration 7300 / 10000, samples: 256, loss: 370.483620\n",
      "iteration 7310 / 10000, samples: 256, loss: 295.262967\n",
      "iteration 7320 / 10000, samples: 256, loss: 357.477347\n",
      "iteration 7330 / 10000, samples: 256, loss: 335.687848\n",
      "iteration 7340 / 10000, samples: 256, loss: 372.104531\n",
      "iteration 7350 / 10000, samples: 256, loss: 217.281920\n",
      "iteration 7360 / 10000, samples: 256, loss: 255.553049\n",
      "iteration 7370 / 10000, samples: 256, loss: 220.342279\n",
      "iteration 7380 / 10000, samples: 256, loss: 314.308152\n",
      "iteration 7390 / 10000, samples: 256, loss: 220.501806\n",
      "iteration 7400 / 10000, samples: 256, loss: 264.570715\n",
      "iteration 7410 / 10000, samples: 256, loss: 329.854804\n",
      "iteration 7420 / 10000, samples: 256, loss: 218.190426\n",
      "iteration 7430 / 10000, samples: 256, loss: 253.264482\n",
      "iteration 7440 / 10000, samples: 256, loss: 204.785765\n",
      "iteration 7450 / 10000, samples: 256, loss: 396.552284\n",
      "iteration 7460 / 10000, samples: 256, loss: 262.220559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7470 / 10000, samples: 256, loss: 228.709333\n",
      "iteration 7480 / 10000, samples: 256, loss: 292.707228\n",
      "iteration 7490 / 10000, samples: 256, loss: 236.966908\n",
      "iteration 7500 / 10000, samples: 256, loss: 172.262304\n",
      "iteration 7510 / 10000, samples: 256, loss: 241.949537\n",
      "iteration 7520 / 10000, samples: 256, loss: 268.403227\n",
      "iteration 7530 / 10000, samples: 256, loss: 200.336226\n",
      "iteration 7540 / 10000, samples: 256, loss: 343.009413\n",
      "iteration 7550 / 10000, samples: 256, loss: 239.601789\n",
      "iteration 7560 / 10000, samples: 256, loss: 232.766585\n",
      "iteration 7570 / 10000, samples: 256, loss: 248.773983\n",
      "iteration 7580 / 10000, samples: 256, loss: 146.455192\n",
      "iteration 7590 / 10000, samples: 256, loss: 340.285603\n",
      "iteration 7600 / 10000, samples: 256, loss: 230.112450\n",
      "iteration 7610 / 10000, samples: 256, loss: 365.723194\n",
      "iteration 7620 / 10000, samples: 256, loss: 173.102839\n",
      "iteration 7630 / 10000, samples: 256, loss: 293.393442\n",
      "iteration 7640 / 10000, samples: 256, loss: 321.058332\n",
      "iteration 7650 / 10000, samples: 256, loss: 271.697078\n",
      "iteration 7660 / 10000, samples: 256, loss: 251.735808\n",
      "iteration 7670 / 10000, samples: 256, loss: 292.674596\n",
      "iteration 7680 / 10000, samples: 256, loss: 299.294300\n",
      "iteration 7690 / 10000, samples: 256, loss: 161.884194\n",
      "iteration 7700 / 10000, samples: 256, loss: 207.393280\n",
      "iteration 7710 / 10000, samples: 256, loss: 301.692895\n",
      "iteration 7720 / 10000, samples: 256, loss: 318.878062\n",
      "iteration 7730 / 10000, samples: 256, loss: 340.056505\n",
      "iteration 7740 / 10000, samples: 256, loss: 217.632582\n",
      "iteration 7750 / 10000, samples: 256, loss: 209.638144\n",
      "iteration 7760 / 10000, samples: 256, loss: 258.598025\n",
      "iteration 7770 / 10000, samples: 256, loss: 252.935429\n",
      "iteration 7780 / 10000, samples: 256, loss: 210.653214\n",
      "iteration 7790 / 10000, samples: 256, loss: 206.654676\n",
      "iteration 7800 / 10000, samples: 256, loss: 226.308408\n",
      "iteration 7810 / 10000, samples: 256, loss: 168.414788\n",
      "iteration 7820 / 10000, samples: 256, loss: 246.321214\n",
      "iteration 7830 / 10000, samples: 256, loss: 303.769568\n",
      "iteration 7840 / 10000, samples: 256, loss: 391.170996\n",
      "iteration 7850 / 10000, samples: 256, loss: 317.281288\n",
      "iteration 7860 / 10000, samples: 256, loss: 275.932941\n",
      "iteration 7870 / 10000, samples: 256, loss: 278.471089\n",
      "iteration 7880 / 10000, samples: 256, loss: 215.813935\n",
      "iteration 7890 / 10000, samples: 256, loss: 194.886392\n",
      "iteration 7900 / 10000, samples: 256, loss: 154.281878\n",
      "iteration 7910 / 10000, samples: 256, loss: 240.153304\n",
      "iteration 7920 / 10000, samples: 256, loss: 244.100732\n",
      "iteration 7930 / 10000, samples: 256, loss: 223.483729\n",
      "iteration 7940 / 10000, samples: 256, loss: 234.905935\n",
      "iteration 7950 / 10000, samples: 256, loss: 350.657513\n",
      "iteration 7960 / 10000, samples: 256, loss: 238.559374\n",
      "iteration 7970 / 10000, samples: 256, loss: 219.698532\n",
      "iteration 7980 / 10000, samples: 256, loss: 297.033374\n",
      "iteration 7990 / 10000, samples: 256, loss: 202.930286\n",
      "iteration 8000 / 10000, samples: 256, loss: 220.693591\n",
      "iteration 8010 / 10000, samples: 256, loss: 288.020085\n",
      "iteration 8020 / 10000, samples: 256, loss: 262.485824\n",
      "iteration 8030 / 10000, samples: 256, loss: 229.401924\n",
      "iteration 8040 / 10000, samples: 256, loss: 255.787800\n",
      "iteration 8050 / 10000, samples: 256, loss: 211.097081\n",
      "iteration 8060 / 10000, samples: 256, loss: 271.068372\n",
      "iteration 8070 / 10000, samples: 256, loss: 161.239693\n",
      "iteration 8080 / 10000, samples: 256, loss: 213.866004\n",
      "iteration 8090 / 10000, samples: 256, loss: 370.872629\n",
      "iteration 8100 / 10000, samples: 256, loss: 304.723143\n",
      "iteration 8110 / 10000, samples: 256, loss: 206.573023\n",
      "iteration 8120 / 10000, samples: 256, loss: 338.012557\n",
      "iteration 8130 / 10000, samples: 256, loss: 243.092037\n",
      "iteration 8140 / 10000, samples: 256, loss: 228.135008\n",
      "iteration 8150 / 10000, samples: 256, loss: 187.938043\n",
      "iteration 8160 / 10000, samples: 256, loss: 242.595454\n",
      "iteration 8170 / 10000, samples: 256, loss: 164.308602\n",
      "iteration 8180 / 10000, samples: 256, loss: 251.261598\n",
      "iteration 8190 / 10000, samples: 256, loss: 168.931823\n",
      "iteration 8200 / 10000, samples: 256, loss: 307.674132\n",
      "iteration 8210 / 10000, samples: 256, loss: 313.656909\n",
      "iteration 8220 / 10000, samples: 256, loss: 167.588677\n",
      "iteration 8230 / 10000, samples: 256, loss: 293.728772\n",
      "iteration 8240 / 10000, samples: 256, loss: 270.222672\n",
      "iteration 8250 / 10000, samples: 256, loss: 236.876222\n",
      "iteration 8260 / 10000, samples: 256, loss: 173.670038\n",
      "iteration 8270 / 10000, samples: 256, loss: 291.286196\n",
      "iteration 8280 / 10000, samples: 256, loss: 263.289899\n",
      "iteration 8290 / 10000, samples: 256, loss: 299.032612\n",
      "iteration 8300 / 10000, samples: 256, loss: 198.277909\n",
      "iteration 8310 / 10000, samples: 256, loss: 213.457845\n",
      "iteration 8320 / 10000, samples: 256, loss: 241.176556\n",
      "iteration 8330 / 10000, samples: 256, loss: 287.278727\n",
      "iteration 8340 / 10000, samples: 256, loss: 265.869393\n",
      "iteration 8350 / 10000, samples: 256, loss: 167.343557\n",
      "iteration 8360 / 10000, samples: 256, loss: 264.754725\n",
      "iteration 8370 / 10000, samples: 256, loss: 219.228527\n",
      "iteration 8380 / 10000, samples: 256, loss: 169.795037\n",
      "iteration 8390 / 10000, samples: 256, loss: 231.696333\n",
      "iteration 8400 / 10000, samples: 256, loss: 328.101795\n",
      "iteration 8410 / 10000, samples: 256, loss: 164.701897\n",
      "iteration 8420 / 10000, samples: 256, loss: 245.446186\n",
      "iteration 8430 / 10000, samples: 256, loss: 178.406553\n",
      "iteration 8440 / 10000, samples: 256, loss: 272.800088\n",
      "iteration 8450 / 10000, samples: 256, loss: 227.217820\n",
      "iteration 8460 / 10000, samples: 256, loss: 299.436766\n",
      "iteration 8470 / 10000, samples: 256, loss: 289.086581\n",
      "iteration 8480 / 10000, samples: 256, loss: 250.004912\n",
      "iteration 8490 / 10000, samples: 256, loss: 351.544254\n",
      "iteration 8500 / 10000, samples: 256, loss: 277.742445\n",
      "iteration 8510 / 10000, samples: 256, loss: 232.860710\n",
      "iteration 8520 / 10000, samples: 256, loss: 212.282558\n",
      "iteration 8530 / 10000, samples: 256, loss: 247.551292\n",
      "iteration 8540 / 10000, samples: 256, loss: 301.955653\n",
      "iteration 8550 / 10000, samples: 256, loss: 318.903983\n",
      "iteration 8560 / 10000, samples: 256, loss: 183.478805\n",
      "iteration 8570 / 10000, samples: 256, loss: 205.247077\n",
      "iteration 8580 / 10000, samples: 256, loss: 195.017103\n",
      "iteration 8590 / 10000, samples: 256, loss: 142.246436\n",
      "iteration 8600 / 10000, samples: 256, loss: 263.071709\n",
      "iteration 8610 / 10000, samples: 256, loss: 242.799806\n",
      "iteration 8620 / 10000, samples: 256, loss: 343.588302\n",
      "iteration 8630 / 10000, samples: 256, loss: 193.549465\n",
      "iteration 8640 / 10000, samples: 256, loss: 227.567840\n",
      "iteration 8650 / 10000, samples: 256, loss: 160.355226\n",
      "iteration 8660 / 10000, samples: 256, loss: 151.054625\n",
      "iteration 8670 / 10000, samples: 256, loss: 239.542891\n",
      "iteration 8680 / 10000, samples: 256, loss: 261.048131\n",
      "iteration 8690 / 10000, samples: 256, loss: 253.957034\n",
      "iteration 8700 / 10000, samples: 256, loss: 291.492734\n",
      "iteration 8710 / 10000, samples: 256, loss: 257.626933\n",
      "iteration 8720 / 10000, samples: 256, loss: 275.012573\n",
      "iteration 8730 / 10000, samples: 256, loss: 330.950716\n",
      "iteration 8740 / 10000, samples: 256, loss: 309.843037\n",
      "iteration 8750 / 10000, samples: 256, loss: 186.119570\n",
      "iteration 8760 / 10000, samples: 256, loss: 285.462735\n",
      "iteration 8770 / 10000, samples: 256, loss: 355.382273\n",
      "iteration 8780 / 10000, samples: 256, loss: 274.992915\n",
      "iteration 8790 / 10000, samples: 256, loss: 245.680146\n",
      "iteration 8800 / 10000, samples: 256, loss: 175.904033\n",
      "iteration 8810 / 10000, samples: 256, loss: 261.949772\n",
      "iteration 8820 / 10000, samples: 256, loss: 242.064087\n",
      "iteration 8830 / 10000, samples: 256, loss: 280.906235\n",
      "iteration 8840 / 10000, samples: 256, loss: 222.096920\n",
      "iteration 8850 / 10000, samples: 256, loss: 278.404477\n",
      "iteration 8860 / 10000, samples: 256, loss: 433.373185\n",
      "iteration 8870 / 10000, samples: 256, loss: 268.068787\n",
      "iteration 8880 / 10000, samples: 256, loss: 328.618259\n",
      "iteration 8890 / 10000, samples: 256, loss: 254.033588\n",
      "iteration 8900 / 10000, samples: 256, loss: 248.783202\n",
      "iteration 8910 / 10000, samples: 256, loss: 217.298465\n",
      "iteration 8920 / 10000, samples: 256, loss: 174.312702\n",
      "iteration 8930 / 10000, samples: 256, loss: 177.994307\n",
      "iteration 8940 / 10000, samples: 256, loss: 276.528781\n",
      "iteration 8950 / 10000, samples: 256, loss: 268.229837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8960 / 10000, samples: 256, loss: 163.773669\n",
      "iteration 8970 / 10000, samples: 256, loss: 290.033563\n",
      "iteration 8980 / 10000, samples: 256, loss: 168.305381\n",
      "iteration 8990 / 10000, samples: 256, loss: 277.827997\n",
      "iteration 9000 / 10000, samples: 256, loss: 297.416457\n",
      "iteration 9010 / 10000, samples: 256, loss: 301.631342\n",
      "iteration 9020 / 10000, samples: 256, loss: 171.941602\n",
      "iteration 9030 / 10000, samples: 256, loss: 261.350655\n",
      "iteration 9040 / 10000, samples: 256, loss: 187.725387\n",
      "iteration 9050 / 10000, samples: 256, loss: 369.324298\n",
      "iteration 9060 / 10000, samples: 256, loss: 205.558509\n",
      "iteration 9070 / 10000, samples: 256, loss: 280.511130\n",
      "iteration 9080 / 10000, samples: 256, loss: 215.266383\n",
      "iteration 9090 / 10000, samples: 256, loss: 234.034801\n",
      "iteration 9100 / 10000, samples: 256, loss: 200.957194\n",
      "iteration 9110 / 10000, samples: 256, loss: 164.758399\n",
      "iteration 9120 / 10000, samples: 256, loss: 354.528871\n",
      "iteration 9130 / 10000, samples: 256, loss: 306.407794\n",
      "iteration 9140 / 10000, samples: 256, loss: 222.760150\n",
      "iteration 9150 / 10000, samples: 256, loss: 342.101137\n",
      "iteration 9160 / 10000, samples: 256, loss: 248.279108\n",
      "iteration 9170 / 10000, samples: 256, loss: 241.239154\n",
      "iteration 9180 / 10000, samples: 256, loss: 287.797183\n",
      "iteration 9190 / 10000, samples: 256, loss: 309.004852\n",
      "iteration 9200 / 10000, samples: 256, loss: 167.203131\n",
      "iteration 9210 / 10000, samples: 256, loss: 238.371991\n",
      "iteration 9220 / 10000, samples: 256, loss: 230.044342\n",
      "iteration 9230 / 10000, samples: 256, loss: 295.524650\n",
      "iteration 9240 / 10000, samples: 256, loss: 204.211418\n",
      "iteration 9250 / 10000, samples: 256, loss: 299.908609\n",
      "iteration 9260 / 10000, samples: 256, loss: 296.180914\n",
      "iteration 9270 / 10000, samples: 256, loss: 317.556821\n",
      "iteration 9280 / 10000, samples: 256, loss: 304.469891\n",
      "iteration 9290 / 10000, samples: 256, loss: 251.626383\n",
      "iteration 9300 / 10000, samples: 256, loss: 222.801201\n",
      "iteration 9310 / 10000, samples: 256, loss: 334.045240\n",
      "iteration 9320 / 10000, samples: 256, loss: 265.294799\n",
      "iteration 9330 / 10000, samples: 256, loss: 220.006436\n",
      "iteration 9340 / 10000, samples: 256, loss: 437.122502\n",
      "iteration 9350 / 10000, samples: 256, loss: 220.728127\n",
      "iteration 9360 / 10000, samples: 256, loss: 237.342131\n",
      "iteration 9370 / 10000, samples: 256, loss: 151.539114\n",
      "iteration 9380 / 10000, samples: 256, loss: 210.469058\n",
      "iteration 9390 / 10000, samples: 256, loss: 263.709869\n",
      "iteration 9400 / 10000, samples: 256, loss: 252.951857\n",
      "iteration 9410 / 10000, samples: 256, loss: 187.440887\n",
      "iteration 9420 / 10000, samples: 256, loss: 185.488714\n",
      "iteration 9430 / 10000, samples: 256, loss: 254.577138\n",
      "iteration 9440 / 10000, samples: 256, loss: 357.913896\n",
      "iteration 9450 / 10000, samples: 256, loss: 271.367570\n",
      "iteration 9460 / 10000, samples: 256, loss: 238.816451\n",
      "iteration 9470 / 10000, samples: 256, loss: 251.337242\n",
      "iteration 9480 / 10000, samples: 256, loss: 267.110711\n",
      "iteration 9490 / 10000, samples: 256, loss: 193.090034\n",
      "iteration 9500 / 10000, samples: 256, loss: 341.085888\n",
      "iteration 9510 / 10000, samples: 256, loss: 182.816108\n",
      "iteration 9520 / 10000, samples: 256, loss: 331.360465\n",
      "iteration 9530 / 10000, samples: 256, loss: 335.010795\n",
      "iteration 9540 / 10000, samples: 256, loss: 242.446006\n",
      "iteration 9550 / 10000, samples: 256, loss: 232.129907\n",
      "iteration 9560 / 10000, samples: 256, loss: 211.899069\n",
      "iteration 9570 / 10000, samples: 256, loss: 243.772674\n",
      "iteration 9580 / 10000, samples: 256, loss: 173.914367\n",
      "iteration 9590 / 10000, samples: 256, loss: 290.259984\n",
      "iteration 9600 / 10000, samples: 256, loss: 227.144781\n",
      "iteration 9610 / 10000, samples: 256, loss: 233.350725\n",
      "iteration 9620 / 10000, samples: 256, loss: 200.227595\n",
      "iteration 9630 / 10000, samples: 256, loss: 346.930089\n",
      "iteration 9640 / 10000, samples: 256, loss: 213.161961\n",
      "iteration 9650 / 10000, samples: 256, loss: 337.393694\n",
      "iteration 9660 / 10000, samples: 256, loss: 255.429006\n",
      "iteration 9670 / 10000, samples: 256, loss: 343.507986\n",
      "iteration 9680 / 10000, samples: 256, loss: 306.006605\n",
      "iteration 9690 / 10000, samples: 256, loss: 343.901502\n",
      "iteration 9700 / 10000, samples: 256, loss: 204.156297\n",
      "iteration 9710 / 10000, samples: 256, loss: 306.818737\n",
      "iteration 9720 / 10000, samples: 256, loss: 201.537289\n",
      "iteration 9730 / 10000, samples: 256, loss: 223.264718\n",
      "iteration 9740 / 10000, samples: 256, loss: 260.086292\n",
      "iteration 9750 / 10000, samples: 256, loss: 272.475618\n",
      "iteration 9760 / 10000, samples: 256, loss: 308.987425\n",
      "iteration 9770 / 10000, samples: 256, loss: 243.322866\n",
      "iteration 9780 / 10000, samples: 256, loss: 209.430449\n",
      "iteration 9790 / 10000, samples: 256, loss: 200.129743\n",
      "iteration 9800 / 10000, samples: 256, loss: 210.428445\n",
      "iteration 9810 / 10000, samples: 256, loss: 218.895201\n",
      "iteration 9820 / 10000, samples: 256, loss: 327.385585\n",
      "iteration 9830 / 10000, samples: 256, loss: 212.117662\n",
      "iteration 9840 / 10000, samples: 256, loss: 178.323145\n",
      "iteration 9850 / 10000, samples: 256, loss: 167.590954\n",
      "iteration 9860 / 10000, samples: 256, loss: 272.592876\n",
      "iteration 9870 / 10000, samples: 256, loss: 225.374735\n",
      "iteration 9880 / 10000, samples: 256, loss: 204.747675\n",
      "iteration 9890 / 10000, samples: 256, loss: 266.887711\n",
      "iteration 9900 / 10000, samples: 256, loss: 259.011378\n",
      "iteration 9910 / 10000, samples: 256, loss: 209.099819\n",
      "iteration 9920 / 10000, samples: 256, loss: 282.187655\n",
      "iteration 9930 / 10000, samples: 256, loss: 170.122471\n",
      "iteration 9940 / 10000, samples: 256, loss: 366.910554\n",
      "iteration 9950 / 10000, samples: 256, loss: 231.039112\n",
      "iteration 9960 / 10000, samples: 256, loss: 185.917158\n",
      "iteration 9970 / 10000, samples: 256, loss: 348.223711\n",
      "iteration 9980 / 10000, samples: 256, loss: 244.499571\n",
      "iteration 9990 / 10000, samples: 256, loss: 385.717202\n"
     ]
    }
   ],
   "source": [
    "LC = LinearClassifier()\n",
    "\n",
    "print('start training ...')\n",
    "#train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True)\n",
    "#在dataTrain中不重复随机抽取batch_size个样本，迭代训练num_iters次\n",
    "loss_all = LC.train(dataTrain, labelTrain, num_iters = 10000, batch_size = 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss is 258.961734\n",
      "start predicting ...\n",
      "the accuracy rate is 26.000000 \n"
     ]
    }
   ],
   "source": [
    "dataTest = dataTest - np.mean(dataTest, axis=0)\n",
    "print('last loss is %f' %(loss_all[-1]))\n",
    "#开始预测\n",
    "print('start predicting ...')\n",
    "y_pred = LC.predict(dataTest)\n",
    "\n",
    "hit = 0\n",
    "for i in xrange(y_pred.size):\n",
    "\tif (y_pred[i] == labelTest[i]):\n",
    "\t\thit += 1\n",
    "\n",
    "print('the accuracy rate is %f ' % (hit/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
