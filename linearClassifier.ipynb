{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "\n",
    "import pickle\n",
    "from past.builtins import xrange\n",
    "\n",
    "\n",
    "'''\n",
    "文件读取和数据获取\n",
    "'''\n",
    "def unpickle(file):\n",
    "  with open(file,'rb') as fo:\n",
    "    dict=pickle.load(fo)\n",
    "  return dict\n",
    "\n",
    "\n",
    "\n",
    "def load_cifar10(file):\n",
    "    dictTrain = unpickle(file + \"data_batch_1\")\n",
    "    dataTrain = dictTrain['data']\n",
    "    labelTrain = dictTrain['labels']\n",
    "\n",
    "    for i in range(2,6):\n",
    "        dictTrain = unpickle(file+\"data_batch_\"+str(i))\n",
    "        dataTrain = np.vstack([dataTrain, dictTrain['data']])\n",
    "        labelTrain = np.hstack([labelTrain, dictTrain['labels']])\n",
    "\n",
    "    dictTest = unpickle(file + \"test_batch\")\n",
    "    dataTest = dictTest['data']\n",
    "    labelTest = dictTest['labels']\n",
    "    labelTest = np.array(labelTest)\n",
    "\n",
    "    return dataTrain, labelTrain, dataTest, labelTest\n",
    "\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "    Inputs:\n",
    "    - W: 权重矩阵\n",
    "    - X: 图片样本数据集(矩阵)\n",
    "    - y: 训练图片的标签\n",
    "    - reg: (float) regularization strength\n",
    "    Returns a tuple of:\n",
    "    - loss 损失函数值\n",
    "    - dW矩阵\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W) #初始化dW矩阵\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "    num_train=X.shape[0]\n",
    "    num_class=W.shape[1]    #标签数\n",
    "\n",
    "    for i in xrange(num_train):\n",
    "        score = X[i].dot(W) #分类器预测结果\n",
    "        score-=np.max(score)    #提高计算中的数值稳定性\n",
    "\n",
    "        correct_score = score[y[i]]   #取分类正确的评分值\n",
    "        exp_sum=np.sum(np.exp(score))\n",
    "\n",
    "        loss+=np.log(exp_sum)-correct_score #计算一张图片的loss值\n",
    "\n",
    "        for j in xrange(num_class):\n",
    "            \n",
    "            if j==y[i]: #图片标签\n",
    "                dW[:,j]+=np.exp(score[j])/exp_sum*X[i]-X[i]\n",
    "            else:\n",
    "                dW[:,j]+=np.exp(score[j])/exp_sum*X[i]\n",
    "\n",
    "    #平均loss            \n",
    "    loss/=num_train #一个训练集合平均loss\n",
    "    loss+=0.5*reg*np.sum(W*W)   #对W中元素平方后求和，计算L\n",
    "    dW/=num_train\n",
    "    dW+=reg*W\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "    return loss, dW\n",
    "\n",
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "\n",
    "  #############################################################################\n",
    "    num_train=X.shape[0] \n",
    "\n",
    "    score = X.dot(W)\n",
    "    score -= np.max(score, axis = 1)[:, np.newaxis]    #axis = 1每一行的最大值，score仍为500*10\n",
    "\n",
    "    correct_score=score[range(num_train), y]    #correct_score变为500*1\n",
    "    exp_score = np.exp(score)\n",
    "    sum_exp_score = np.sum(exp_score, axis = 1)    #sum_exp_score为500*1\n",
    "\n",
    "    loss = np.sum(np.log(sum_exp_score)) - np.sum(correct_score)\n",
    "    exp_score /= sum_exp_score[:,np.newaxis]  #exp_score为500*10\n",
    "\n",
    "    for i in xrange(num_train):\n",
    "        dW += exp_score[i] * X[i][:,np.newaxis]   # X[i][:,np.newaxis]将X[i]增加一列纬度\n",
    "        dW[:, y[i]] -= X[i]\n",
    "\n",
    "    loss/=num_train\n",
    "    loss+=0.5*reg*np.sum(W*W)\n",
    "    dW/=num_train\n",
    "    dW+=reg*W\n",
    "    \n",
    "\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "    return loss, dW\n",
    "\n",
    "\n",
    "class LinearClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=200,batch_size=500, verbose=True):\n",
    "        ####随机梯度下降\n",
    "        \"\"\"\n",
    "        Train this linear classifier using stochastic gradient descent.\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "        training samples each of dimension D.\n",
    "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "        means that X[i] has label 0 <= c < C for C classes.\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - reg: (float) regularization strength.\n",
    "        - num_iters: (integer) number of steps to take when optimizing\n",
    "        - batch_size: (integer) number of training examples to use at each step.\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            # lazily initialize W\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)  #生成随机矩阵\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in xrange(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Sample batch_size elements from the training data and their           #\n",
    "        # corresponding labels to use in this round of gradient descent.        #\n",
    "        # Store the data in X_batch and their corresponding labels in           #\n",
    "        # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
    "        # and y_batch should have shape (batch_size,)                           #\n",
    "        #                                                                       #\n",
    "        # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
    "        # replacement is faster than sampling without replacement.              #\n",
    "        #########################################################################\n",
    "            sample_index = np.random.choice(num_train, batch_size, replace=False)\n",
    "            X_batch = X[sample_index, :]   # select the batch sample\n",
    "            y_batch = y[sample_index]      # select the batch label     \n",
    "        #########################################################################\n",
    "        #                       END OF YOUR CODE                                #\n",
    "        #########################################################################\n",
    "        # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg) #获取loss数值\n",
    "            loss_history.append(loss) #把loss添加到末尾\n",
    "        # perform parameter update\n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Update the weights using the gradient and the learning rate.          #\n",
    "        #########################################################################\n",
    "        # perform parameter update\n",
    "            self.W += -learning_rate * grad    \n",
    "        #########################################################################\n",
    "        #                       END OF YOUR CODE                                #\n",
    "        #########################################################################\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "          training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N, and each element is an integer giving the predicted\n",
    "          class.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[1])\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Implement this method. Store the predicted labels in y_pred.            #\n",
    "        ###########################################################################\n",
    "        score = X.dot(self.W)\n",
    "        y_pred = np.argmax(score,axis=1)\n",
    "        ###########################################################################\n",
    "        #                           END OF YOUR CODE                              #\n",
    "        ###########################################################################\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative. \n",
    "        Subclasses will override this.\n",
    "        \n",
    "        Inputs:\n",
    "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
    "        data points; each point has dimension D.\n",
    "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "        - reg: (float) regularization strength\n",
    "        \n",
    "        Returns: A tuple containing:\n",
    "        - loss as a single float\n",
    "        - gradient with respect to self.W; an array of the same shape as W\n",
    "        \"\"\"\n",
    "        return softmax_loss_naive(self.W, X_batch, y_batch, reg)\n",
    "\n",
    "class Softmax(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return softmax_loss_naive(self.W, X_batch, y_batch, reg)\n",
    "        #return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "训练开始\n",
    "'''\n",
    "\n",
    "file_path = \"./\"\n",
    "\n",
    "#获取数据集\n",
    "dataTrain1, labelTrain1, dataTest1, labelTest1 = load_cifar10(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "iteration 0 / 200: loss 13.064733\n",
      "iteration 100 / 200: loss 8557.244809\n",
      "trainning 1 times \n",
      "\n",
      "iteration 0 / 200: loss 4095.092160\n",
      "iteration 100 / 200: loss 3989.959973\n",
      "trainning 2 times \n",
      "\n",
      "iteration 0 / 200: loss 5610.378672\n",
      "iteration 100 / 200: loss 4660.253644\n",
      "trainning 3 times \n",
      "\n",
      "iteration 0 / 200: loss 4703.475387\n",
      "iteration 100 / 200: loss 4075.405712\n",
      "trainning 4 times \n",
      "\n",
      "iteration 0 / 200: loss 3135.915568\n",
      "iteration 100 / 200: loss 3137.449369\n",
      "trainning 5 times \n",
      "\n",
      "iteration 0 / 200: loss 4226.652157\n",
      "iteration 100 / 200: loss 4488.025291\n",
      "trainning 6 times \n",
      "\n",
      "iteration 0 / 200: loss 4371.073670\n",
      "iteration 100 / 200: loss 6482.591849\n",
      "trainning 7 times \n",
      "\n",
      "iteration 0 / 200: loss 3170.833611\n",
      "iteration 100 / 200: loss 4045.285160\n",
      "trainning 8 times \n",
      "\n",
      "iteration 0 / 200: loss 5182.441596\n",
      "iteration 100 / 200: loss 4447.470787\n",
      "trainning 9 times \n",
      "\n",
      "iteration 0 / 200: loss 4228.659167\n",
      "iteration 100 / 200: loss 4566.798899\n",
      "trainning 10 times \n",
      "\n",
      "iteration 0 / 200: loss 3236.605760\n",
      "iteration 100 / 200: loss 3868.020197\n",
      "trainning 11 times \n",
      "\n",
      "iteration 0 / 200: loss 4202.968656\n",
      "iteration 100 / 200: loss 3550.103940\n",
      "trainning 12 times \n",
      "\n",
      "iteration 0 / 200: loss 3333.024512\n",
      "iteration 100 / 200: loss 5507.287972\n",
      "trainning 13 times \n",
      "\n",
      "iteration 0 / 200: loss 5070.481004\n",
      "iteration 100 / 200: loss 5806.237974\n",
      "trainning 14 times \n",
      "\n",
      "iteration 0 / 200: loss 5737.542289\n",
      "iteration 100 / 200: loss 3896.923634\n",
      "trainning 15 times \n",
      "\n",
      "iteration 0 / 200: loss 4000.040383\n",
      "iteration 100 / 200: loss 3285.620522\n",
      "trainning 16 times \n",
      "\n",
      "iteration 0 / 200: loss 5183.229310\n",
      "iteration 100 / 200: loss 6756.151328\n",
      "trainning 17 times \n",
      "\n",
      "iteration 0 / 200: loss 4740.696818\n",
      "iteration 100 / 200: loss 2444.775984\n",
      "trainning 18 times \n",
      "\n",
      "iteration 0 / 200: loss 5493.663808\n",
      "iteration 100 / 200: loss 5341.510896\n",
      "trainning 19 times \n",
      "\n",
      "iteration 0 / 200: loss 4561.709441\n",
      "iteration 100 / 200: loss 4244.809367\n",
      "trainning 20 times \n",
      "\n",
      "iteration 0 / 200: loss 4902.268602\n",
      "iteration 100 / 200: loss 5661.571576\n",
      "trainning 21 times \n",
      "\n",
      "iteration 0 / 200: loss 4947.466805\n",
      "iteration 100 / 200: loss 6627.410393\n",
      "trainning 22 times \n",
      "\n",
      "iteration 0 / 200: loss 5588.368780\n",
      "iteration 100 / 200: loss 5289.371833\n",
      "trainning 23 times \n",
      "\n",
      "iteration 0 / 200: loss 6550.340028\n",
      "iteration 100 / 200: loss 5073.379830\n",
      "trainning 24 times \n",
      "\n",
      "iteration 0 / 200: loss 5440.552808\n",
      "iteration 100 / 200: loss 4005.470410\n",
      "trainning 25 times \n",
      "\n",
      "iteration 0 / 200: loss 4232.168465\n",
      "iteration 100 / 200: loss 7131.177043\n",
      "trainning 26 times \n",
      "\n",
      "iteration 0 / 200: loss 5318.143291\n",
      "iteration 100 / 200: loss 5458.248516\n",
      "trainning 27 times \n",
      "\n",
      "iteration 0 / 200: loss 4339.020743\n",
      "iteration 100 / 200: loss 6955.488095\n",
      "trainning 28 times \n",
      "\n",
      "iteration 0 / 200: loss 6012.274193\n",
      "iteration 100 / 200: loss 5146.429863\n",
      "trainning 29 times \n",
      "\n",
      "iteration 0 / 200: loss 4645.868909\n",
      "iteration 100 / 200: loss 4704.501095\n",
      "trainning 30 times \n",
      "\n",
      "iteration 0 / 200: loss 3775.999716\n",
      "iteration 100 / 200: loss 4472.543479\n",
      "trainning 31 times \n",
      "\n",
      "iteration 0 / 200: loss 5993.752982\n",
      "iteration 100 / 200: loss 5836.644151\n",
      "trainning 32 times \n",
      "\n",
      "iteration 0 / 200: loss 5349.369937\n",
      "iteration 100 / 200: loss 5364.271689\n",
      "trainning 33 times \n",
      "\n",
      "iteration 0 / 200: loss 4421.277799\n",
      "iteration 100 / 200: loss 6331.818539\n",
      "trainning 34 times \n",
      "\n",
      "iteration 0 / 200: loss 5524.882904\n",
      "iteration 100 / 200: loss 5353.749624\n",
      "trainning 35 times \n",
      "\n",
      "iteration 0 / 200: loss 3929.460534\n",
      "iteration 100 / 200: loss 4478.805362\n",
      "trainning 36 times \n",
      "\n",
      "iteration 0 / 200: loss 5144.684090\n",
      "iteration 100 / 200: loss 5425.977452\n",
      "trainning 37 times \n",
      "\n",
      "iteration 0 / 200: loss 4315.201580\n",
      "iteration 100 / 200: loss 4510.962327\n",
      "trainning 38 times \n",
      "\n",
      "iteration 0 / 200: loss 5896.230484\n",
      "iteration 100 / 200: loss 4391.759053\n",
      "trainning 39 times \n",
      "\n",
      "iteration 0 / 200: loss 2893.556202\n",
      "iteration 100 / 200: loss 4494.068231\n",
      "trainning 40 times \n",
      "\n",
      "iteration 0 / 200: loss 4421.144890\n",
      "iteration 100 / 200: loss 5494.940652\n",
      "trainning 41 times \n",
      "\n",
      "iteration 0 / 200: loss 4393.450455\n",
      "iteration 100 / 200: loss 6656.373168\n",
      "trainning 42 times \n",
      "\n",
      "iteration 0 / 200: loss 5287.174487\n",
      "iteration 100 / 200: loss 4963.695734\n",
      "trainning 43 times \n",
      "\n",
      "iteration 0 / 200: loss 6288.949733\n",
      "iteration 100 / 200: loss 5713.007553\n",
      "trainning 44 times \n",
      "\n",
      "iteration 0 / 200: loss 4571.006489\n",
      "iteration 100 / 200: loss 5807.093580\n",
      "trainning 45 times \n",
      "\n",
      "iteration 0 / 200: loss 4002.199944\n",
      "iteration 100 / 200: loss 3614.617764\n",
      "trainning 46 times \n",
      "\n",
      "iteration 0 / 200: loss 5634.903540\n",
      "iteration 100 / 200: loss 3850.367599\n",
      "trainning 47 times \n",
      "\n",
      "iteration 0 / 200: loss 4868.007862\n",
      "iteration 100 / 200: loss 5019.091177\n",
      "trainning 48 times \n",
      "\n",
      "iteration 0 / 200: loss 5127.261083\n",
      "iteration 100 / 200: loss 4261.646150\n",
      "trainning 49 times \n",
      "\n",
      "iteration 0 / 200: loss 6522.406223\n",
      "iteration 100 / 200: loss 4546.752240\n",
      "trainning 50 times \n",
      "\n"
     ]
    }
   ],
   "source": [
    "LC = LinearClassifier()\n",
    "SM = Softmax()\n",
    "\n",
    "print(dataTrain1.shape)\n",
    "\n",
    "for i in range(50):\n",
    "    LC.train(dataTrain1[:(1+i)*500,:], labelTrain1[:500*(i+1)])\n",
    "  \n",
    "    print('trainning %d times \\n' % (i+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30720000\n",
      "100\n",
      "21\n",
      "accuracy is 21.000000\n"
     ]
    }
   ],
   "source": [
    "print(dataTest1.size)\n",
    "pre = LC.predict(dataTest1[:100,:])\n",
    "print(pre.size)\n",
    "acc = 0\n",
    "\n",
    "# print(pre.size)\n",
    "# print(labelTest1.size)\n",
    "\n",
    "for i in range(pre.size):\n",
    "  if (pre[i] == labelTest1[i]):\n",
    "    acc += 1\n",
    "\n",
    "print(acc)\n",
    "print('accuracy is %f' % (100*acc/pre.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
