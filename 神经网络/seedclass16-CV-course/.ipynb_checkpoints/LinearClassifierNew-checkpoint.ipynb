{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from __future__ import print_function\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 文件读取\n",
    "def unpickle(file):\n",
    "\twith open(file,'rb') as fo:\n",
    "\t\tdict = pickle.load(fo)\n",
    "\treturn dict\n",
    "\n",
    "def load_file(file):\n",
    "\tdictTrain = unpickle(file + \"data_batch_1\")\n",
    "\tdataTrain = dictTrain['data']\n",
    "\tlabelTrain = dictTrain['labels']\n",
    "\n",
    "\tfor i in range(2,6):\n",
    "\t\tdictTrain = unpickle(file + \"data_batch_\" + str(i))\n",
    "\t\tdataTrain = np.vstack([dataTrain,dictTrain['data']])\n",
    "\t\tlabelTrain = np.hstack([labelTrain,dictTrain['labels']])\n",
    "\n",
    "\tdictTest = unpickle(file + \"test_batch\")\n",
    "\tdataTest = dictTest['data']\n",
    "\tlabelTest = dictTest['labels']\n",
    "\tlabelTest = np.array(labelTest)\n",
    "\n",
    "\treturn dataTrain, labelTrain, dataTest, labelTest\n",
    "\n",
    "\n",
    "#softmax loss 函数\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "\t'''\n",
    "\t\tW:权重矩阵\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t'''\n",
    "\t#初始化数据\n",
    "\tloss = 0.0\n",
    "\tdW = np.zeros_like(W)\n",
    "\tnum_train = X.shape[0]\t#样本数\n",
    "\tnum_class = W.shape[1]\t#样本类别数\n",
    "\n",
    "\tfor i in xrange(num_train):\n",
    "\t\tscore = X[i].dot(W)\n",
    "\t\tscore -= np.max(score)\t#提高样本稳定性\n",
    "\n",
    "\t\tcorrect_score = score[y[i]]\n",
    "\t\texp_sum = np.sum(np.exp(score))\n",
    "\t\tloss += np.log(exp_sum) - correct_score\n",
    "\n",
    "\t\tfor j in xrange(num_class):\n",
    "\t\t\tif (j == y[i]):\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i] - X[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\tdW[:, j] += np.exp(score[j]) / exp_sum * X[i]\n",
    "\n",
    "\n",
    "\tloss /= num_train\n",
    "\tloss += 0.5 * reg * np.sum(W*W)\n",
    "\n",
    "\tdW /= num_train\n",
    "\tdW += reg * W\n",
    "\n",
    "\treturn loss, dW\n",
    "\n",
    "#线性分类器\n",
    "class LinearClassifier(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.W = None\n",
    "\n",
    "\tdef train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\t\ty:图片训练集标签(数组)\n",
    "\t\tstep_size:学习步进速度\n",
    "\t\treg:正则化强度\n",
    "\t\tnum_iters:迭代次数\n",
    "\t\tbatch_size:每次迭代图片样本数\n",
    "\t\tverbose:是否打印信息\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss_history:每次训练loss值\n",
    "\t\t'''\n",
    "\t\tnum_train, dim = X.shape\n",
    "\t\tnum_classes = np.max(y) + 1\n",
    "\t\n",
    "\t\tif self.W is None:\n",
    "\t\t\tself.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "\t\tloss_history = []\n",
    "\n",
    "\t\tfor it in xrange(num_iters):\n",
    "\t\t\t#从样本中不重复随机采batch_size个样本\n",
    "\t\t\tsample_index = np.random.choice(num_train, batch_size, replace=False)\n",
    "\n",
    "\t\t\tX_batch = X[sample_index, :]\n",
    "\t\t\ty_batch = y[sample_index]\n",
    "\n",
    "\t\t\tloss, grad = self.loss(X_batch, y_batch, reg)\n",
    "\t\t\tloss_history.append(loss)\n",
    "\n",
    "\t\t\tself.W += -step_size * grad\n",
    "\n",
    "\t\t\tif (verbose and it %10 == 0):\n",
    "\t\t\t\tprint('iteration %d / %d, samples: %d, loss: %f' % (it, num_iters, batch_size, loss))\n",
    "\n",
    "\t\treturn loss_history\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\t'''\n",
    "\t\tX:图片训练集(矩阵)\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\ty_pred:标签预测值\n",
    "\t\t'''\n",
    "\t\ty_pred = np.zeros(X.shape[1])\n",
    "\n",
    "\t\tscore = X.dot(self.W)\n",
    "\t\ty_pred = np.argmax(score, axis = 1)\n",
    "\n",
    "\t\treturn y_pred\n",
    "\n",
    "\n",
    "\tdef loss(self, X_batch, y_batch, reg):\n",
    "\t\t'''\n",
    "\t\tX_batch:图片训练集(矩阵)\n",
    "\t\ty_batch:图片训练集标签(数组)\n",
    "\t\treg:正则化强度\n",
    "\n",
    "\t\treturn:\n",
    "\t\t\tloss:训练集平均loss值\n",
    "\t\t\tdW:梯度矩阵\n",
    "\t\t'''\n",
    "\t\treturn softmax_loss_naive(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集加载完成\n",
      "测试集加载完成\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from PIL import Image\n",
    "#开始训练\n",
    "file_path = './'\n",
    "\n",
    "dataTrFirst, labelTrain, dataTsFirst, labelTest = load_file(file_path)\n",
    "\n",
    "dataTr = np.zeros((dataTrFirst.shape[0],32*32))\n",
    "dataTs = np.zeros((dataTsFirst.shape[0],32*32))\n",
    "\n",
    "\n",
    "for i in range(dataTrFirst.shape[0] -45000):\n",
    "    img = dataTrFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTr[i] = res.reshape((1,32*32))\n",
    "print(\"训练集加载完成\")\n",
    "\n",
    "for i in range(dataTsFirst.shape[0] -1):\n",
    "    img = dataTsFirst[i].reshape((32,32,3))\n",
    "    img = Image.fromarray(img)\n",
    "    res = local_binary_pattern(img.convert('L'),8,2,method='uniform')\n",
    "    dataTs[i] = res.reshape((1,32*32))\n",
    "print(\"测试集加载完成\")\n",
    "\n",
    "dataTrain = dataTrain - np.mean(dataTrain, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ...\n",
      "iteration 0 / 8000, samples: 200, loss: 4.971027\n",
      "iteration 10 / 8000, samples: 200, loss: 384.378958\n",
      "iteration 20 / 8000, samples: 200, loss: 293.225315\n",
      "iteration 30 / 8000, samples: 200, loss: 215.186162\n",
      "iteration 40 / 8000, samples: 200, loss: 309.724507\n",
      "iteration 50 / 8000, samples: 200, loss: 431.009159\n",
      "iteration 60 / 8000, samples: 200, loss: 242.300164\n",
      "iteration 70 / 8000, samples: 200, loss: 265.746932\n",
      "iteration 80 / 8000, samples: 200, loss: 452.359751\n",
      "iteration 90 / 8000, samples: 200, loss: 333.418578\n",
      "iteration 100 / 8000, samples: 200, loss: 163.394712\n",
      "iteration 110 / 8000, samples: 200, loss: 230.101835\n",
      "iteration 120 / 8000, samples: 200, loss: 421.908613\n",
      "iteration 130 / 8000, samples: 200, loss: 304.555362\n",
      "iteration 140 / 8000, samples: 200, loss: 305.773754\n",
      "iteration 150 / 8000, samples: 200, loss: 213.665798\n",
      "iteration 160 / 8000, samples: 200, loss: 255.989628\n",
      "iteration 170 / 8000, samples: 200, loss: 203.012875\n",
      "iteration 180 / 8000, samples: 200, loss: 333.285244\n",
      "iteration 190 / 8000, samples: 200, loss: 294.294764\n",
      "iteration 200 / 8000, samples: 200, loss: 320.386805\n",
      "iteration 210 / 8000, samples: 200, loss: 168.086695\n",
      "iteration 220 / 8000, samples: 200, loss: 330.626955\n",
      "iteration 230 / 8000, samples: 200, loss: 189.360636\n",
      "iteration 240 / 8000, samples: 200, loss: 293.587067\n",
      "iteration 250 / 8000, samples: 200, loss: 333.025328\n",
      "iteration 260 / 8000, samples: 200, loss: 298.914631\n",
      "iteration 270 / 8000, samples: 200, loss: 404.412561\n",
      "iteration 280 / 8000, samples: 200, loss: 239.119213\n",
      "iteration 290 / 8000, samples: 200, loss: 238.665345\n",
      "iteration 300 / 8000, samples: 200, loss: 267.713494\n",
      "iteration 310 / 8000, samples: 200, loss: 304.355822\n",
      "iteration 320 / 8000, samples: 200, loss: 212.011180\n",
      "iteration 330 / 8000, samples: 200, loss: 223.456649\n",
      "iteration 340 / 8000, samples: 200, loss: 360.746039\n",
      "iteration 350 / 8000, samples: 200, loss: 386.409068\n",
      "iteration 360 / 8000, samples: 200, loss: 216.050601\n",
      "iteration 370 / 8000, samples: 200, loss: 234.170693\n",
      "iteration 380 / 8000, samples: 200, loss: 234.731987\n",
      "iteration 390 / 8000, samples: 200, loss: 322.286655\n",
      "iteration 400 / 8000, samples: 200, loss: 265.512422\n",
      "iteration 410 / 8000, samples: 200, loss: 276.706586\n",
      "iteration 420 / 8000, samples: 200, loss: 171.076985\n",
      "iteration 430 / 8000, samples: 200, loss: 211.386141\n",
      "iteration 440 / 8000, samples: 200, loss: 141.813980\n",
      "iteration 450 / 8000, samples: 200, loss: 278.010277\n",
      "iteration 460 / 8000, samples: 200, loss: 423.286650\n",
      "iteration 470 / 8000, samples: 200, loss: 243.564070\n",
      "iteration 480 / 8000, samples: 200, loss: 279.834695\n",
      "iteration 490 / 8000, samples: 200, loss: 239.924562\n",
      "iteration 500 / 8000, samples: 200, loss: 341.585261\n",
      "iteration 510 / 8000, samples: 200, loss: 297.336996\n",
      "iteration 520 / 8000, samples: 200, loss: 312.507598\n",
      "iteration 530 / 8000, samples: 200, loss: 259.663664\n",
      "iteration 540 / 8000, samples: 200, loss: 259.500411\n",
      "iteration 550 / 8000, samples: 200, loss: 228.797032\n",
      "iteration 560 / 8000, samples: 200, loss: 407.896197\n",
      "iteration 570 / 8000, samples: 200, loss: 400.269853\n",
      "iteration 580 / 8000, samples: 200, loss: 194.844410\n",
      "iteration 590 / 8000, samples: 200, loss: 324.175255\n",
      "iteration 600 / 8000, samples: 200, loss: 273.873135\n",
      "iteration 610 / 8000, samples: 200, loss: 309.441097\n",
      "iteration 620 / 8000, samples: 200, loss: 325.118800\n",
      "iteration 630 / 8000, samples: 200, loss: 366.306081\n",
      "iteration 640 / 8000, samples: 200, loss: 414.012903\n",
      "iteration 650 / 8000, samples: 200, loss: 311.730666\n",
      "iteration 660 / 8000, samples: 200, loss: 290.179888\n",
      "iteration 670 / 8000, samples: 200, loss: 364.015881\n",
      "iteration 680 / 8000, samples: 200, loss: 251.960241\n",
      "iteration 690 / 8000, samples: 200, loss: 249.815489\n",
      "iteration 700 / 8000, samples: 200, loss: 211.402460\n",
      "iteration 710 / 8000, samples: 200, loss: 333.867634\n",
      "iteration 720 / 8000, samples: 200, loss: 222.626447\n",
      "iteration 730 / 8000, samples: 200, loss: 295.310946\n",
      "iteration 740 / 8000, samples: 200, loss: 372.258556\n",
      "iteration 750 / 8000, samples: 200, loss: 312.935576\n",
      "iteration 760 / 8000, samples: 200, loss: 163.298047\n",
      "iteration 770 / 8000, samples: 200, loss: 240.824366\n",
      "iteration 780 / 8000, samples: 200, loss: 381.689621\n",
      "iteration 790 / 8000, samples: 200, loss: 275.932619\n",
      "iteration 800 / 8000, samples: 200, loss: 304.228943\n",
      "iteration 810 / 8000, samples: 200, loss: 342.043811\n",
      "iteration 820 / 8000, samples: 200, loss: 294.065747\n",
      "iteration 830 / 8000, samples: 200, loss: 229.398662\n",
      "iteration 840 / 8000, samples: 200, loss: 376.288241\n",
      "iteration 850 / 8000, samples: 200, loss: 310.015928\n",
      "iteration 860 / 8000, samples: 200, loss: 189.787039\n",
      "iteration 870 / 8000, samples: 200, loss: 319.049594\n",
      "iteration 880 / 8000, samples: 200, loss: 270.568151\n",
      "iteration 890 / 8000, samples: 200, loss: 299.355824\n",
      "iteration 900 / 8000, samples: 200, loss: 280.697954\n",
      "iteration 910 / 8000, samples: 200, loss: 227.234397\n",
      "iteration 920 / 8000, samples: 200, loss: 186.347870\n",
      "iteration 930 / 8000, samples: 200, loss: 286.852249\n",
      "iteration 940 / 8000, samples: 200, loss: 194.894592\n",
      "iteration 950 / 8000, samples: 200, loss: 155.281252\n",
      "iteration 960 / 8000, samples: 200, loss: 309.189550\n",
      "iteration 970 / 8000, samples: 200, loss: 379.412529\n",
      "iteration 980 / 8000, samples: 200, loss: 280.680235\n",
      "iteration 990 / 8000, samples: 200, loss: 336.438894\n",
      "iteration 1000 / 8000, samples: 200, loss: 519.556307\n",
      "iteration 1010 / 8000, samples: 200, loss: 509.687494\n",
      "iteration 1020 / 8000, samples: 200, loss: 324.210663\n",
      "iteration 1030 / 8000, samples: 200, loss: 224.777272\n",
      "iteration 1040 / 8000, samples: 200, loss: 319.737709\n",
      "iteration 1050 / 8000, samples: 200, loss: 214.584574\n",
      "iteration 1060 / 8000, samples: 200, loss: 256.991336\n",
      "iteration 1070 / 8000, samples: 200, loss: 228.096688\n",
      "iteration 1080 / 8000, samples: 200, loss: 243.786320\n",
      "iteration 1090 / 8000, samples: 200, loss: 214.732449\n",
      "iteration 1100 / 8000, samples: 200, loss: 204.123160\n",
      "iteration 1110 / 8000, samples: 200, loss: 235.294441\n",
      "iteration 1120 / 8000, samples: 200, loss: 320.214524\n",
      "iteration 1130 / 8000, samples: 200, loss: 144.890972\n",
      "iteration 1140 / 8000, samples: 200, loss: 236.363302\n",
      "iteration 1150 / 8000, samples: 200, loss: 219.131007\n",
      "iteration 1160 / 8000, samples: 200, loss: 208.677702\n",
      "iteration 1170 / 8000, samples: 200, loss: 208.513677\n",
      "iteration 1180 / 8000, samples: 200, loss: 224.665089\n",
      "iteration 1190 / 8000, samples: 200, loss: 206.257716\n",
      "iteration 1200 / 8000, samples: 200, loss: 154.486191\n",
      "iteration 1210 / 8000, samples: 200, loss: 265.434702\n",
      "iteration 1220 / 8000, samples: 200, loss: 239.792844\n",
      "iteration 1230 / 8000, samples: 200, loss: 163.943649\n",
      "iteration 1240 / 8000, samples: 200, loss: 237.406210\n",
      "iteration 1250 / 8000, samples: 200, loss: 307.278771\n",
      "iteration 1260 / 8000, samples: 200, loss: 326.786094\n",
      "iteration 1270 / 8000, samples: 200, loss: 250.034266\n",
      "iteration 1280 / 8000, samples: 200, loss: 267.178908\n",
      "iteration 1290 / 8000, samples: 200, loss: 215.461279\n",
      "iteration 1300 / 8000, samples: 200, loss: 178.584671\n",
      "iteration 1310 / 8000, samples: 200, loss: 437.929129\n",
      "iteration 1320 / 8000, samples: 200, loss: 266.079686\n",
      "iteration 1330 / 8000, samples: 200, loss: 360.921408\n",
      "iteration 1340 / 8000, samples: 200, loss: 244.858206\n",
      "iteration 1350 / 8000, samples: 200, loss: 352.400478\n",
      "iteration 1360 / 8000, samples: 200, loss: 310.332209\n",
      "iteration 1370 / 8000, samples: 200, loss: 347.643110\n",
      "iteration 1380 / 8000, samples: 200, loss: 202.465954\n",
      "iteration 1390 / 8000, samples: 200, loss: 333.502656\n",
      "iteration 1400 / 8000, samples: 200, loss: 229.619449\n",
      "iteration 1410 / 8000, samples: 200, loss: 209.626983\n",
      "iteration 1420 / 8000, samples: 200, loss: 304.085050\n",
      "iteration 1430 / 8000, samples: 200, loss: 280.692499\n",
      "iteration 1440 / 8000, samples: 200, loss: 236.421293\n",
      "iteration 1450 / 8000, samples: 200, loss: 215.793801\n",
      "iteration 1460 / 8000, samples: 200, loss: 272.427443\n",
      "iteration 1470 / 8000, samples: 200, loss: 205.895711\n",
      "iteration 1480 / 8000, samples: 200, loss: 429.506654\n",
      "iteration 1490 / 8000, samples: 200, loss: 207.279420\n",
      "iteration 1500 / 8000, samples: 200, loss: 352.946080\n",
      "iteration 1510 / 8000, samples: 200, loss: 182.014438\n",
      "iteration 1520 / 8000, samples: 200, loss: 208.579512\n",
      "iteration 1530 / 8000, samples: 200, loss: 260.012341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1540 / 8000, samples: 200, loss: 218.681627\n",
      "iteration 1550 / 8000, samples: 200, loss: 239.035256\n",
      "iteration 1560 / 8000, samples: 200, loss: 249.448818\n",
      "iteration 1570 / 8000, samples: 200, loss: 298.953567\n",
      "iteration 1580 / 8000, samples: 200, loss: 174.608600\n",
      "iteration 1590 / 8000, samples: 200, loss: 375.521898\n",
      "iteration 1600 / 8000, samples: 200, loss: 249.784439\n",
      "iteration 1610 / 8000, samples: 200, loss: 278.854722\n",
      "iteration 1620 / 8000, samples: 200, loss: 299.617413\n",
      "iteration 1630 / 8000, samples: 200, loss: 227.050258\n",
      "iteration 1640 / 8000, samples: 200, loss: 350.381114\n",
      "iteration 1650 / 8000, samples: 200, loss: 139.918941\n",
      "iteration 1660 / 8000, samples: 200, loss: 457.544565\n",
      "iteration 1670 / 8000, samples: 200, loss: 174.932525\n",
      "iteration 1680 / 8000, samples: 200, loss: 297.269129\n",
      "iteration 1690 / 8000, samples: 200, loss: 264.610141\n",
      "iteration 1700 / 8000, samples: 200, loss: 175.602068\n",
      "iteration 1710 / 8000, samples: 200, loss: 296.329645\n",
      "iteration 1720 / 8000, samples: 200, loss: 176.351172\n",
      "iteration 1730 / 8000, samples: 200, loss: 215.224632\n",
      "iteration 1740 / 8000, samples: 200, loss: 200.891368\n",
      "iteration 1750 / 8000, samples: 200, loss: 371.257064\n",
      "iteration 1760 / 8000, samples: 200, loss: 190.502076\n",
      "iteration 1770 / 8000, samples: 200, loss: 316.553062\n",
      "iteration 1780 / 8000, samples: 200, loss: 246.273474\n",
      "iteration 1790 / 8000, samples: 200, loss: 266.675184\n",
      "iteration 1800 / 8000, samples: 200, loss: 284.586697\n",
      "iteration 1810 / 8000, samples: 200, loss: 258.868973\n",
      "iteration 1820 / 8000, samples: 200, loss: 220.292231\n",
      "iteration 1830 / 8000, samples: 200, loss: 321.964590\n",
      "iteration 1840 / 8000, samples: 200, loss: 233.658776\n",
      "iteration 1850 / 8000, samples: 200, loss: 396.646380\n",
      "iteration 1860 / 8000, samples: 200, loss: 448.468530\n",
      "iteration 1870 / 8000, samples: 200, loss: 172.504665\n",
      "iteration 1880 / 8000, samples: 200, loss: 267.457970\n",
      "iteration 1890 / 8000, samples: 200, loss: 247.150210\n",
      "iteration 1900 / 8000, samples: 200, loss: 211.748691\n",
      "iteration 1910 / 8000, samples: 200, loss: 326.019689\n",
      "iteration 1920 / 8000, samples: 200, loss: 251.759670\n",
      "iteration 1930 / 8000, samples: 200, loss: 404.995158\n",
      "iteration 1940 / 8000, samples: 200, loss: 302.651573\n",
      "iteration 1950 / 8000, samples: 200, loss: 246.807768\n",
      "iteration 1960 / 8000, samples: 200, loss: 301.993661\n",
      "iteration 1970 / 8000, samples: 200, loss: 185.168507\n",
      "iteration 1980 / 8000, samples: 200, loss: 229.893216\n",
      "iteration 1990 / 8000, samples: 200, loss: 225.396847\n",
      "iteration 2000 / 8000, samples: 200, loss: 303.140056\n",
      "iteration 2010 / 8000, samples: 200, loss: 232.327719\n",
      "iteration 2020 / 8000, samples: 200, loss: 219.410908\n",
      "iteration 2030 / 8000, samples: 200, loss: 374.175733\n",
      "iteration 2040 / 8000, samples: 200, loss: 227.229189\n",
      "iteration 2050 / 8000, samples: 200, loss: 264.269524\n",
      "iteration 2060 / 8000, samples: 200, loss: 279.080609\n",
      "iteration 2070 / 8000, samples: 200, loss: 365.123890\n",
      "iteration 2080 / 8000, samples: 200, loss: 245.077985\n",
      "iteration 2090 / 8000, samples: 200, loss: 225.161368\n",
      "iteration 2100 / 8000, samples: 200, loss: 234.060720\n",
      "iteration 2110 / 8000, samples: 200, loss: 308.762981\n",
      "iteration 2120 / 8000, samples: 200, loss: 165.489522\n",
      "iteration 2130 / 8000, samples: 200, loss: 234.031492\n",
      "iteration 2140 / 8000, samples: 200, loss: 225.473089\n",
      "iteration 2150 / 8000, samples: 200, loss: 200.971175\n",
      "iteration 2160 / 8000, samples: 200, loss: 372.651116\n",
      "iteration 2170 / 8000, samples: 200, loss: 313.901710\n",
      "iteration 2180 / 8000, samples: 200, loss: 230.772487\n",
      "iteration 2190 / 8000, samples: 200, loss: 222.151011\n",
      "iteration 2200 / 8000, samples: 200, loss: 239.167307\n",
      "iteration 2210 / 8000, samples: 200, loss: 187.144950\n",
      "iteration 2220 / 8000, samples: 200, loss: 309.336751\n",
      "iteration 2230 / 8000, samples: 200, loss: 197.901578\n",
      "iteration 2240 / 8000, samples: 200, loss: 202.333809\n",
      "iteration 2250 / 8000, samples: 200, loss: 232.165350\n",
      "iteration 2260 / 8000, samples: 200, loss: 340.021387\n",
      "iteration 2270 / 8000, samples: 200, loss: 228.641954\n",
      "iteration 2280 / 8000, samples: 200, loss: 272.117129\n",
      "iteration 2290 / 8000, samples: 200, loss: 319.929605\n",
      "iteration 2300 / 8000, samples: 200, loss: 315.605073\n",
      "iteration 2310 / 8000, samples: 200, loss: 193.349766\n",
      "iteration 2320 / 8000, samples: 200, loss: 282.161309\n",
      "iteration 2330 / 8000, samples: 200, loss: 174.354721\n",
      "iteration 2340 / 8000, samples: 200, loss: 324.476999\n",
      "iteration 2350 / 8000, samples: 200, loss: 230.294280\n",
      "iteration 2360 / 8000, samples: 200, loss: 159.003381\n",
      "iteration 2370 / 8000, samples: 200, loss: 290.626454\n",
      "iteration 2380 / 8000, samples: 200, loss: 364.435026\n",
      "iteration 2390 / 8000, samples: 200, loss: 293.666343\n",
      "iteration 2400 / 8000, samples: 200, loss: 230.722753\n",
      "iteration 2410 / 8000, samples: 200, loss: 296.568279\n",
      "iteration 2420 / 8000, samples: 200, loss: 220.957474\n",
      "iteration 2430 / 8000, samples: 200, loss: 336.175386\n",
      "iteration 2440 / 8000, samples: 200, loss: 259.408548\n",
      "iteration 2450 / 8000, samples: 200, loss: 248.172514\n",
      "iteration 2460 / 8000, samples: 200, loss: 252.602122\n",
      "iteration 2470 / 8000, samples: 200, loss: 363.111374\n",
      "iteration 2480 / 8000, samples: 200, loss: 318.443271\n",
      "iteration 2490 / 8000, samples: 200, loss: 237.142733\n",
      "iteration 2500 / 8000, samples: 200, loss: 223.872433\n",
      "iteration 2510 / 8000, samples: 200, loss: 209.034419\n",
      "iteration 2520 / 8000, samples: 200, loss: 325.763501\n",
      "iteration 2530 / 8000, samples: 200, loss: 218.354894\n",
      "iteration 2540 / 8000, samples: 200, loss: 341.417540\n",
      "iteration 2550 / 8000, samples: 200, loss: 397.066642\n",
      "iteration 2560 / 8000, samples: 200, loss: 384.171181\n",
      "iteration 2570 / 8000, samples: 200, loss: 358.343645\n",
      "iteration 2580 / 8000, samples: 200, loss: 328.412752\n",
      "iteration 2590 / 8000, samples: 200, loss: 248.887937\n",
      "iteration 2600 / 8000, samples: 200, loss: 169.474413\n",
      "iteration 2610 / 8000, samples: 200, loss: 214.926213\n",
      "iteration 2620 / 8000, samples: 200, loss: 355.423151\n",
      "iteration 2630 / 8000, samples: 200, loss: 196.858381\n",
      "iteration 2640 / 8000, samples: 200, loss: 254.575461\n",
      "iteration 2650 / 8000, samples: 200, loss: 287.424244\n",
      "iteration 2660 / 8000, samples: 200, loss: 194.560591\n",
      "iteration 2670 / 8000, samples: 200, loss: 398.129458\n",
      "iteration 2680 / 8000, samples: 200, loss: 312.366359\n",
      "iteration 2690 / 8000, samples: 200, loss: 269.532455\n",
      "iteration 2700 / 8000, samples: 200, loss: 176.190666\n",
      "iteration 2710 / 8000, samples: 200, loss: 235.958087\n",
      "iteration 2720 / 8000, samples: 200, loss: 203.160781\n",
      "iteration 2730 / 8000, samples: 200, loss: 299.896107\n",
      "iteration 2740 / 8000, samples: 200, loss: 174.632307\n",
      "iteration 2750 / 8000, samples: 200, loss: 269.173008\n",
      "iteration 2760 / 8000, samples: 200, loss: 289.867282\n",
      "iteration 2770 / 8000, samples: 200, loss: 349.006819\n",
      "iteration 2780 / 8000, samples: 200, loss: 140.829416\n",
      "iteration 2790 / 8000, samples: 200, loss: 203.041730\n",
      "iteration 2800 / 8000, samples: 200, loss: 204.930267\n",
      "iteration 2810 / 8000, samples: 200, loss: 259.373921\n",
      "iteration 2820 / 8000, samples: 200, loss: 198.282276\n",
      "iteration 2830 / 8000, samples: 200, loss: 257.368390\n",
      "iteration 2840 / 8000, samples: 200, loss: 282.116079\n",
      "iteration 2850 / 8000, samples: 200, loss: 168.515932\n",
      "iteration 2860 / 8000, samples: 200, loss: 228.400306\n",
      "iteration 2870 / 8000, samples: 200, loss: 261.161067\n",
      "iteration 2880 / 8000, samples: 200, loss: 173.830721\n",
      "iteration 2890 / 8000, samples: 200, loss: 389.224160\n",
      "iteration 2900 / 8000, samples: 200, loss: 168.707560\n",
      "iteration 2910 / 8000, samples: 200, loss: 271.884286\n",
      "iteration 2920 / 8000, samples: 200, loss: 286.554720\n",
      "iteration 2930 / 8000, samples: 200, loss: 218.048718\n",
      "iteration 2940 / 8000, samples: 200, loss: 278.997647\n",
      "iteration 2950 / 8000, samples: 200, loss: 259.748845\n",
      "iteration 2960 / 8000, samples: 200, loss: 200.345013\n",
      "iteration 2970 / 8000, samples: 200, loss: 294.720512\n",
      "iteration 2980 / 8000, samples: 200, loss: 217.691590\n",
      "iteration 2990 / 8000, samples: 200, loss: 394.709333\n",
      "iteration 3000 / 8000, samples: 200, loss: 404.955147\n",
      "iteration 3010 / 8000, samples: 200, loss: 272.617284\n",
      "iteration 3020 / 8000, samples: 200, loss: 286.699847\n",
      "iteration 3030 / 8000, samples: 200, loss: 250.128128\n",
      "iteration 3040 / 8000, samples: 200, loss: 316.584584\n",
      "iteration 3050 / 8000, samples: 200, loss: 249.320794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3060 / 8000, samples: 200, loss: 270.662928\n",
      "iteration 3070 / 8000, samples: 200, loss: 186.969014\n",
      "iteration 3080 / 8000, samples: 200, loss: 203.377238\n",
      "iteration 3090 / 8000, samples: 200, loss: 340.780691\n",
      "iteration 3100 / 8000, samples: 200, loss: 195.289555\n",
      "iteration 3110 / 8000, samples: 200, loss: 303.140388\n",
      "iteration 3120 / 8000, samples: 200, loss: 318.293030\n",
      "iteration 3130 / 8000, samples: 200, loss: 204.679222\n",
      "iteration 3140 / 8000, samples: 200, loss: 301.844122\n",
      "iteration 3150 / 8000, samples: 200, loss: 166.949842\n",
      "iteration 3160 / 8000, samples: 200, loss: 278.239741\n",
      "iteration 3170 / 8000, samples: 200, loss: 311.591996\n",
      "iteration 3180 / 8000, samples: 200, loss: 264.091710\n",
      "iteration 3190 / 8000, samples: 200, loss: 288.262821\n",
      "iteration 3200 / 8000, samples: 200, loss: 226.270738\n",
      "iteration 3210 / 8000, samples: 200, loss: 271.707280\n",
      "iteration 3220 / 8000, samples: 200, loss: 235.208952\n",
      "iteration 3230 / 8000, samples: 200, loss: 304.182410\n",
      "iteration 3240 / 8000, samples: 200, loss: 212.402710\n",
      "iteration 3250 / 8000, samples: 200, loss: 276.545681\n",
      "iteration 3260 / 8000, samples: 200, loss: 319.773977\n",
      "iteration 3270 / 8000, samples: 200, loss: 287.033324\n",
      "iteration 3280 / 8000, samples: 200, loss: 219.564295\n",
      "iteration 3290 / 8000, samples: 200, loss: 280.617003\n",
      "iteration 3300 / 8000, samples: 200, loss: 158.157259\n",
      "iteration 3310 / 8000, samples: 200, loss: 258.460426\n",
      "iteration 3320 / 8000, samples: 200, loss: 221.663524\n",
      "iteration 3330 / 8000, samples: 200, loss: 308.601795\n",
      "iteration 3340 / 8000, samples: 200, loss: 314.724154\n",
      "iteration 3350 / 8000, samples: 200, loss: 344.744224\n",
      "iteration 3360 / 8000, samples: 200, loss: 303.437868\n",
      "iteration 3370 / 8000, samples: 200, loss: 197.498939\n",
      "iteration 3380 / 8000, samples: 200, loss: 230.190771\n",
      "iteration 3390 / 8000, samples: 200, loss: 158.579216\n",
      "iteration 3400 / 8000, samples: 200, loss: 172.518110\n",
      "iteration 3410 / 8000, samples: 200, loss: 295.325232\n",
      "iteration 3420 / 8000, samples: 200, loss: 235.004952\n",
      "iteration 3430 / 8000, samples: 200, loss: 235.745280\n",
      "iteration 3440 / 8000, samples: 200, loss: 326.806650\n",
      "iteration 3450 / 8000, samples: 200, loss: 431.223328\n",
      "iteration 3460 / 8000, samples: 200, loss: 282.872857\n",
      "iteration 3470 / 8000, samples: 200, loss: 192.444856\n",
      "iteration 3480 / 8000, samples: 200, loss: 309.371937\n",
      "iteration 3490 / 8000, samples: 200, loss: 270.738804\n",
      "iteration 3500 / 8000, samples: 200, loss: 293.016278\n",
      "iteration 3510 / 8000, samples: 200, loss: 204.018898\n",
      "iteration 3520 / 8000, samples: 200, loss: 266.497057\n",
      "iteration 3530 / 8000, samples: 200, loss: 426.012532\n",
      "iteration 3540 / 8000, samples: 200, loss: 212.152057\n",
      "iteration 3550 / 8000, samples: 200, loss: 232.715432\n",
      "iteration 3560 / 8000, samples: 200, loss: 251.636471\n",
      "iteration 3570 / 8000, samples: 200, loss: 363.599978\n",
      "iteration 3580 / 8000, samples: 200, loss: 311.392708\n",
      "iteration 3590 / 8000, samples: 200, loss: 284.408773\n",
      "iteration 3600 / 8000, samples: 200, loss: 272.255352\n",
      "iteration 3610 / 8000, samples: 200, loss: 313.870968\n",
      "iteration 3620 / 8000, samples: 200, loss: 292.569933\n",
      "iteration 3630 / 8000, samples: 200, loss: 266.215146\n",
      "iteration 3640 / 8000, samples: 200, loss: 319.461603\n",
      "iteration 3650 / 8000, samples: 200, loss: 382.656712\n",
      "iteration 3660 / 8000, samples: 200, loss: 298.024227\n",
      "iteration 3670 / 8000, samples: 200, loss: 348.223361\n",
      "iteration 3680 / 8000, samples: 200, loss: 414.422186\n",
      "iteration 3690 / 8000, samples: 200, loss: 180.466261\n",
      "iteration 3700 / 8000, samples: 200, loss: 327.953812\n",
      "iteration 3710 / 8000, samples: 200, loss: 255.839713\n",
      "iteration 3720 / 8000, samples: 200, loss: 258.619227\n",
      "iteration 3730 / 8000, samples: 200, loss: 253.998231\n",
      "iteration 3740 / 8000, samples: 200, loss: 282.539174\n",
      "iteration 3750 / 8000, samples: 200, loss: 246.234284\n",
      "iteration 3760 / 8000, samples: 200, loss: 355.955442\n",
      "iteration 3770 / 8000, samples: 200, loss: 246.350993\n",
      "iteration 3780 / 8000, samples: 200, loss: 270.342064\n",
      "iteration 3790 / 8000, samples: 200, loss: 178.092266\n",
      "iteration 3800 / 8000, samples: 200, loss: 448.269425\n",
      "iteration 3810 / 8000, samples: 200, loss: 284.626513\n",
      "iteration 3820 / 8000, samples: 200, loss: 223.591036\n",
      "iteration 3830 / 8000, samples: 200, loss: 211.234661\n",
      "iteration 3840 / 8000, samples: 200, loss: 174.654814\n",
      "iteration 3850 / 8000, samples: 200, loss: 331.906557\n",
      "iteration 3860 / 8000, samples: 200, loss: 297.372919\n",
      "iteration 3870 / 8000, samples: 200, loss: 223.248448\n",
      "iteration 3880 / 8000, samples: 200, loss: 271.555609\n",
      "iteration 3890 / 8000, samples: 200, loss: 330.073785\n",
      "iteration 3900 / 8000, samples: 200, loss: 256.562021\n",
      "iteration 3910 / 8000, samples: 200, loss: 296.754830\n",
      "iteration 3920 / 8000, samples: 200, loss: 262.206692\n",
      "iteration 3930 / 8000, samples: 200, loss: 144.001144\n",
      "iteration 3940 / 8000, samples: 200, loss: 453.714370\n",
      "iteration 3950 / 8000, samples: 200, loss: 218.542937\n",
      "iteration 3960 / 8000, samples: 200, loss: 216.698962\n",
      "iteration 3970 / 8000, samples: 200, loss: 256.704834\n",
      "iteration 3980 / 8000, samples: 200, loss: 274.564544\n",
      "iteration 3990 / 8000, samples: 200, loss: 352.816755\n",
      "iteration 4000 / 8000, samples: 200, loss: 147.627510\n",
      "iteration 4010 / 8000, samples: 200, loss: 401.170961\n",
      "iteration 4020 / 8000, samples: 200, loss: 272.326128\n",
      "iteration 4030 / 8000, samples: 200, loss: 228.650125\n",
      "iteration 4040 / 8000, samples: 200, loss: 230.319257\n",
      "iteration 4050 / 8000, samples: 200, loss: 253.564042\n",
      "iteration 4060 / 8000, samples: 200, loss: 231.749200\n",
      "iteration 4070 / 8000, samples: 200, loss: 268.565215\n",
      "iteration 4080 / 8000, samples: 200, loss: 220.773630\n",
      "iteration 4090 / 8000, samples: 200, loss: 212.820946\n",
      "iteration 4100 / 8000, samples: 200, loss: 432.920711\n",
      "iteration 4110 / 8000, samples: 200, loss: 201.617266\n",
      "iteration 4120 / 8000, samples: 200, loss: 213.090197\n",
      "iteration 4130 / 8000, samples: 200, loss: 294.664670\n",
      "iteration 4140 / 8000, samples: 200, loss: 200.651899\n",
      "iteration 4150 / 8000, samples: 200, loss: 365.299058\n",
      "iteration 4160 / 8000, samples: 200, loss: 189.230892\n",
      "iteration 4170 / 8000, samples: 200, loss: 223.708072\n",
      "iteration 4180 / 8000, samples: 200, loss: 395.119534\n",
      "iteration 4190 / 8000, samples: 200, loss: 257.586923\n",
      "iteration 4200 / 8000, samples: 200, loss: 254.438975\n",
      "iteration 4210 / 8000, samples: 200, loss: 218.716718\n",
      "iteration 4220 / 8000, samples: 200, loss: 261.010623\n",
      "iteration 4230 / 8000, samples: 200, loss: 333.844412\n",
      "iteration 4240 / 8000, samples: 200, loss: 241.493941\n",
      "iteration 4250 / 8000, samples: 200, loss: 199.861891\n",
      "iteration 4260 / 8000, samples: 200, loss: 213.502750\n",
      "iteration 4270 / 8000, samples: 200, loss: 293.351288\n",
      "iteration 4280 / 8000, samples: 200, loss: 323.008470\n",
      "iteration 4290 / 8000, samples: 200, loss: 317.947909\n",
      "iteration 4300 / 8000, samples: 200, loss: 301.967721\n",
      "iteration 4310 / 8000, samples: 200, loss: 213.874392\n",
      "iteration 4320 / 8000, samples: 200, loss: 265.667202\n",
      "iteration 4330 / 8000, samples: 200, loss: 267.671872\n",
      "iteration 4340 / 8000, samples: 200, loss: 320.377928\n",
      "iteration 4350 / 8000, samples: 200, loss: 221.926983\n",
      "iteration 4360 / 8000, samples: 200, loss: 298.751747\n",
      "iteration 4370 / 8000, samples: 200, loss: 213.667937\n",
      "iteration 4380 / 8000, samples: 200, loss: 281.939634\n",
      "iteration 4390 / 8000, samples: 200, loss: 262.400026\n",
      "iteration 4400 / 8000, samples: 200, loss: 393.909715\n",
      "iteration 4410 / 8000, samples: 200, loss: 252.219762\n",
      "iteration 4420 / 8000, samples: 200, loss: 319.894472\n",
      "iteration 4430 / 8000, samples: 200, loss: 210.816333\n",
      "iteration 4440 / 8000, samples: 200, loss: 224.763270\n",
      "iteration 4450 / 8000, samples: 200, loss: 236.911257\n",
      "iteration 4460 / 8000, samples: 200, loss: 266.050224\n",
      "iteration 4470 / 8000, samples: 200, loss: 191.960604\n",
      "iteration 4480 / 8000, samples: 200, loss: 363.555690\n",
      "iteration 4490 / 8000, samples: 200, loss: 250.330023\n",
      "iteration 4500 / 8000, samples: 200, loss: 214.385516\n",
      "iteration 4510 / 8000, samples: 200, loss: 354.144900\n",
      "iteration 4520 / 8000, samples: 200, loss: 301.057377\n",
      "iteration 4530 / 8000, samples: 200, loss: 170.798747\n",
      "iteration 4540 / 8000, samples: 200, loss: 264.927951\n",
      "iteration 4550 / 8000, samples: 200, loss: 266.292563\n",
      "iteration 4560 / 8000, samples: 200, loss: 165.466268\n",
      "iteration 4570 / 8000, samples: 200, loss: 176.428465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4580 / 8000, samples: 200, loss: 234.901068\n",
      "iteration 4590 / 8000, samples: 200, loss: 274.888867\n",
      "iteration 4600 / 8000, samples: 200, loss: 263.968712\n",
      "iteration 4610 / 8000, samples: 200, loss: 217.467367\n",
      "iteration 4620 / 8000, samples: 200, loss: 195.028329\n",
      "iteration 4630 / 8000, samples: 200, loss: 251.966592\n",
      "iteration 4640 / 8000, samples: 200, loss: 175.003822\n",
      "iteration 4650 / 8000, samples: 200, loss: 375.231832\n",
      "iteration 4660 / 8000, samples: 200, loss: 253.186012\n",
      "iteration 4670 / 8000, samples: 200, loss: 287.192606\n",
      "iteration 4680 / 8000, samples: 200, loss: 452.944437\n",
      "iteration 4690 / 8000, samples: 200, loss: 237.906459\n",
      "iteration 4700 / 8000, samples: 200, loss: 250.504308\n",
      "iteration 4710 / 8000, samples: 200, loss: 228.055683\n",
      "iteration 4720 / 8000, samples: 200, loss: 221.777141\n",
      "iteration 4730 / 8000, samples: 200, loss: 349.652011\n",
      "iteration 4740 / 8000, samples: 200, loss: 277.871908\n",
      "iteration 4750 / 8000, samples: 200, loss: 323.730156\n",
      "iteration 4760 / 8000, samples: 200, loss: 214.951739\n",
      "iteration 4770 / 8000, samples: 200, loss: 347.304829\n",
      "iteration 4780 / 8000, samples: 200, loss: 391.107783\n",
      "iteration 4790 / 8000, samples: 200, loss: 179.096085\n",
      "iteration 4800 / 8000, samples: 200, loss: 266.092712\n",
      "iteration 4810 / 8000, samples: 200, loss: 203.126023\n",
      "iteration 4820 / 8000, samples: 200, loss: 326.470950\n",
      "iteration 4830 / 8000, samples: 200, loss: 181.768007\n",
      "iteration 4840 / 8000, samples: 200, loss: 393.757793\n",
      "iteration 4850 / 8000, samples: 200, loss: 256.295932\n",
      "iteration 4860 / 8000, samples: 200, loss: 202.632954\n",
      "iteration 4870 / 8000, samples: 200, loss: 251.418223\n",
      "iteration 4880 / 8000, samples: 200, loss: 279.001650\n",
      "iteration 4890 / 8000, samples: 200, loss: 248.213735\n",
      "iteration 4900 / 8000, samples: 200, loss: 362.390065\n",
      "iteration 4910 / 8000, samples: 200, loss: 330.297913\n",
      "iteration 4920 / 8000, samples: 200, loss: 258.348510\n",
      "iteration 4930 / 8000, samples: 200, loss: 150.552076\n",
      "iteration 4940 / 8000, samples: 200, loss: 330.532813\n",
      "iteration 4950 / 8000, samples: 200, loss: 200.687213\n",
      "iteration 4960 / 8000, samples: 200, loss: 599.623949\n",
      "iteration 4970 / 8000, samples: 200, loss: 313.132169\n",
      "iteration 4980 / 8000, samples: 200, loss: 263.059265\n",
      "iteration 4990 / 8000, samples: 200, loss: 254.795537\n",
      "iteration 5000 / 8000, samples: 200, loss: 332.176644\n",
      "iteration 5010 / 8000, samples: 200, loss: 383.195392\n",
      "iteration 5020 / 8000, samples: 200, loss: 191.806617\n",
      "iteration 5030 / 8000, samples: 200, loss: 258.713963\n",
      "iteration 5040 / 8000, samples: 200, loss: 321.126565\n",
      "iteration 5050 / 8000, samples: 200, loss: 283.224009\n",
      "iteration 5060 / 8000, samples: 200, loss: 302.858126\n",
      "iteration 5070 / 8000, samples: 200, loss: 292.547047\n",
      "iteration 5080 / 8000, samples: 200, loss: 270.746264\n",
      "iteration 5090 / 8000, samples: 200, loss: 279.446795\n",
      "iteration 5100 / 8000, samples: 200, loss: 283.816904\n",
      "iteration 5110 / 8000, samples: 200, loss: 287.788337\n",
      "iteration 5120 / 8000, samples: 200, loss: 159.202211\n",
      "iteration 5130 / 8000, samples: 200, loss: 277.674822\n",
      "iteration 5140 / 8000, samples: 200, loss: 188.199271\n",
      "iteration 5150 / 8000, samples: 200, loss: 240.835702\n",
      "iteration 5160 / 8000, samples: 200, loss: 230.167153\n",
      "iteration 5170 / 8000, samples: 200, loss: 258.298804\n",
      "iteration 5180 / 8000, samples: 200, loss: 226.693457\n",
      "iteration 5190 / 8000, samples: 200, loss: 208.090984\n",
      "iteration 5200 / 8000, samples: 200, loss: 247.910327\n",
      "iteration 5210 / 8000, samples: 200, loss: 349.051172\n",
      "iteration 5220 / 8000, samples: 200, loss: 380.728616\n",
      "iteration 5230 / 8000, samples: 200, loss: 356.632166\n",
      "iteration 5240 / 8000, samples: 200, loss: 188.494374\n",
      "iteration 5250 / 8000, samples: 200, loss: 349.345529\n",
      "iteration 5260 / 8000, samples: 200, loss: 202.895603\n",
      "iteration 5270 / 8000, samples: 200, loss: 185.676996\n",
      "iteration 5280 / 8000, samples: 200, loss: 229.423003\n",
      "iteration 5290 / 8000, samples: 200, loss: 231.974456\n",
      "iteration 5300 / 8000, samples: 200, loss: 268.497768\n",
      "iteration 5310 / 8000, samples: 200, loss: 283.119950\n",
      "iteration 5320 / 8000, samples: 200, loss: 368.103589\n",
      "iteration 5330 / 8000, samples: 200, loss: 298.296891\n",
      "iteration 5340 / 8000, samples: 200, loss: 246.449698\n",
      "iteration 5350 / 8000, samples: 200, loss: 204.163213\n",
      "iteration 5360 / 8000, samples: 200, loss: 224.759457\n",
      "iteration 5370 / 8000, samples: 200, loss: 296.227066\n",
      "iteration 5380 / 8000, samples: 200, loss: 315.607544\n",
      "iteration 5390 / 8000, samples: 200, loss: 262.615702\n",
      "iteration 5400 / 8000, samples: 200, loss: 188.103331\n",
      "iteration 5410 / 8000, samples: 200, loss: 284.892049\n",
      "iteration 5420 / 8000, samples: 200, loss: 246.053850\n",
      "iteration 5430 / 8000, samples: 200, loss: 268.081732\n",
      "iteration 5440 / 8000, samples: 200, loss: 444.251972\n",
      "iteration 5450 / 8000, samples: 200, loss: 334.065517\n",
      "iteration 5460 / 8000, samples: 200, loss: 182.319820\n",
      "iteration 5470 / 8000, samples: 200, loss: 323.645937\n",
      "iteration 5480 / 8000, samples: 200, loss: 373.946542\n",
      "iteration 5490 / 8000, samples: 200, loss: 209.495183\n",
      "iteration 5500 / 8000, samples: 200, loss: 269.847726\n",
      "iteration 5510 / 8000, samples: 200, loss: 174.793538\n",
      "iteration 5520 / 8000, samples: 200, loss: 249.983825\n",
      "iteration 5530 / 8000, samples: 200, loss: 326.739692\n",
      "iteration 5540 / 8000, samples: 200, loss: 204.168074\n",
      "iteration 5550 / 8000, samples: 200, loss: 226.636146\n",
      "iteration 5560 / 8000, samples: 200, loss: 309.015760\n",
      "iteration 5570 / 8000, samples: 200, loss: 245.585793\n",
      "iteration 5580 / 8000, samples: 200, loss: 181.942637\n",
      "iteration 5590 / 8000, samples: 200, loss: 206.631728\n",
      "iteration 5600 / 8000, samples: 200, loss: 125.357594\n",
      "iteration 5610 / 8000, samples: 200, loss: 280.280699\n",
      "iteration 5620 / 8000, samples: 200, loss: 254.037475\n",
      "iteration 5630 / 8000, samples: 200, loss: 274.461256\n",
      "iteration 5640 / 8000, samples: 200, loss: 262.570111\n",
      "iteration 5650 / 8000, samples: 200, loss: 255.930600\n",
      "iteration 5660 / 8000, samples: 200, loss: 333.510549\n",
      "iteration 5670 / 8000, samples: 200, loss: 209.625111\n",
      "iteration 5680 / 8000, samples: 200, loss: 416.018198\n",
      "iteration 5690 / 8000, samples: 200, loss: 206.900301\n",
      "iteration 5700 / 8000, samples: 200, loss: 183.733108\n",
      "iteration 5710 / 8000, samples: 200, loss: 329.867032\n",
      "iteration 5720 / 8000, samples: 200, loss: 219.637200\n",
      "iteration 5730 / 8000, samples: 200, loss: 215.639424\n",
      "iteration 5740 / 8000, samples: 200, loss: 319.691421\n",
      "iteration 5750 / 8000, samples: 200, loss: 288.642862\n",
      "iteration 5760 / 8000, samples: 200, loss: 349.441333\n",
      "iteration 5770 / 8000, samples: 200, loss: 197.436340\n",
      "iteration 5780 / 8000, samples: 200, loss: 209.449412\n",
      "iteration 5790 / 8000, samples: 200, loss: 223.934496\n",
      "iteration 5800 / 8000, samples: 200, loss: 295.104505\n",
      "iteration 5810 / 8000, samples: 200, loss: 218.885027\n",
      "iteration 5820 / 8000, samples: 200, loss: 175.558476\n",
      "iteration 5830 / 8000, samples: 200, loss: 198.761871\n",
      "iteration 5840 / 8000, samples: 200, loss: 274.174350\n",
      "iteration 5850 / 8000, samples: 200, loss: 208.969865\n",
      "iteration 5860 / 8000, samples: 200, loss: 266.946524\n",
      "iteration 5870 / 8000, samples: 200, loss: 192.329112\n",
      "iteration 5880 / 8000, samples: 200, loss: 405.588648\n",
      "iteration 5890 / 8000, samples: 200, loss: 349.242622\n",
      "iteration 5900 / 8000, samples: 200, loss: 314.383665\n",
      "iteration 5910 / 8000, samples: 200, loss: 212.941379\n",
      "iteration 5920 / 8000, samples: 200, loss: 203.327369\n",
      "iteration 5930 / 8000, samples: 200, loss: 277.392684\n",
      "iteration 5940 / 8000, samples: 200, loss: 201.360898\n",
      "iteration 5950 / 8000, samples: 200, loss: 251.661910\n",
      "iteration 5960 / 8000, samples: 200, loss: 162.745376\n",
      "iteration 5970 / 8000, samples: 200, loss: 239.258041\n",
      "iteration 5980 / 8000, samples: 200, loss: 356.095085\n",
      "iteration 5990 / 8000, samples: 200, loss: 228.600448\n",
      "iteration 6000 / 8000, samples: 200, loss: 233.701118\n",
      "iteration 6010 / 8000, samples: 200, loss: 320.112530\n",
      "iteration 6020 / 8000, samples: 200, loss: 263.412361\n",
      "iteration 6030 / 8000, samples: 200, loss: 265.184633\n",
      "iteration 6040 / 8000, samples: 200, loss: 257.278955\n",
      "iteration 6050 / 8000, samples: 200, loss: 325.830307\n",
      "iteration 6060 / 8000, samples: 200, loss: 297.773854\n",
      "iteration 6070 / 8000, samples: 200, loss: 368.310797\n",
      "iteration 6080 / 8000, samples: 200, loss: 281.072459\n",
      "iteration 6090 / 8000, samples: 200, loss: 170.216926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6100 / 8000, samples: 200, loss: 212.525544\n",
      "iteration 6110 / 8000, samples: 200, loss: 218.346935\n",
      "iteration 6120 / 8000, samples: 200, loss: 267.664083\n",
      "iteration 6130 / 8000, samples: 200, loss: 171.299013\n",
      "iteration 6140 / 8000, samples: 200, loss: 294.860152\n",
      "iteration 6150 / 8000, samples: 200, loss: 210.471417\n",
      "iteration 6160 / 8000, samples: 200, loss: 249.203731\n",
      "iteration 6170 / 8000, samples: 200, loss: 243.805420\n",
      "iteration 6180 / 8000, samples: 200, loss: 335.455424\n",
      "iteration 6190 / 8000, samples: 200, loss: 276.318166\n",
      "iteration 6200 / 8000, samples: 200, loss: 294.882872\n",
      "iteration 6210 / 8000, samples: 200, loss: 178.112571\n",
      "iteration 6220 / 8000, samples: 200, loss: 164.200721\n",
      "iteration 6230 / 8000, samples: 200, loss: 400.979804\n",
      "iteration 6240 / 8000, samples: 200, loss: 265.005544\n",
      "iteration 6250 / 8000, samples: 200, loss: 227.078978\n",
      "iteration 6260 / 8000, samples: 200, loss: 250.541992\n",
      "iteration 6270 / 8000, samples: 200, loss: 276.402338\n",
      "iteration 6280 / 8000, samples: 200, loss: 220.473743\n",
      "iteration 6290 / 8000, samples: 200, loss: 357.905832\n",
      "iteration 6300 / 8000, samples: 200, loss: 195.398814\n",
      "iteration 6310 / 8000, samples: 200, loss: 197.322532\n",
      "iteration 6320 / 8000, samples: 200, loss: 216.218227\n",
      "iteration 6330 / 8000, samples: 200, loss: 251.865450\n",
      "iteration 6340 / 8000, samples: 200, loss: 179.147993\n",
      "iteration 6350 / 8000, samples: 200, loss: 185.690524\n",
      "iteration 6360 / 8000, samples: 200, loss: 289.478517\n",
      "iteration 6370 / 8000, samples: 200, loss: 267.760714\n",
      "iteration 6380 / 8000, samples: 200, loss: 278.395540\n",
      "iteration 6390 / 8000, samples: 200, loss: 274.584960\n",
      "iteration 6400 / 8000, samples: 200, loss: 300.199595\n",
      "iteration 6410 / 8000, samples: 200, loss: 209.836430\n",
      "iteration 6420 / 8000, samples: 200, loss: 353.188446\n",
      "iteration 6430 / 8000, samples: 200, loss: 196.927935\n",
      "iteration 6440 / 8000, samples: 200, loss: 216.049722\n",
      "iteration 6450 / 8000, samples: 200, loss: 303.039316\n",
      "iteration 6460 / 8000, samples: 200, loss: 326.213036\n",
      "iteration 6470 / 8000, samples: 200, loss: 237.640897\n",
      "iteration 6480 / 8000, samples: 200, loss: 281.304827\n",
      "iteration 6490 / 8000, samples: 200, loss: 364.470876\n",
      "iteration 6500 / 8000, samples: 200, loss: 244.570771\n",
      "iteration 6510 / 8000, samples: 200, loss: 221.549895\n",
      "iteration 6520 / 8000, samples: 200, loss: 185.122836\n",
      "iteration 6530 / 8000, samples: 200, loss: 318.323815\n",
      "iteration 6540 / 8000, samples: 200, loss: 293.251457\n",
      "iteration 6550 / 8000, samples: 200, loss: 126.700310\n",
      "iteration 6560 / 8000, samples: 200, loss: 212.949923\n",
      "iteration 6570 / 8000, samples: 200, loss: 243.009332\n",
      "iteration 6580 / 8000, samples: 200, loss: 235.850501\n",
      "iteration 6590 / 8000, samples: 200, loss: 371.815316\n",
      "iteration 6600 / 8000, samples: 200, loss: 320.416970\n",
      "iteration 6610 / 8000, samples: 200, loss: 231.417661\n",
      "iteration 6620 / 8000, samples: 200, loss: 327.116611\n",
      "iteration 6630 / 8000, samples: 200, loss: 280.189639\n",
      "iteration 6640 / 8000, samples: 200, loss: 325.320886\n",
      "iteration 6650 / 8000, samples: 200, loss: 212.468751\n",
      "iteration 6660 / 8000, samples: 200, loss: 384.417905\n",
      "iteration 6670 / 8000, samples: 200, loss: 232.860440\n",
      "iteration 6680 / 8000, samples: 200, loss: 435.734426\n",
      "iteration 6690 / 8000, samples: 200, loss: 182.025353\n",
      "iteration 6700 / 8000, samples: 200, loss: 215.091317\n",
      "iteration 6710 / 8000, samples: 200, loss: 228.732666\n",
      "iteration 6720 / 8000, samples: 200, loss: 188.354124\n",
      "iteration 6730 / 8000, samples: 200, loss: 188.243281\n",
      "iteration 6740 / 8000, samples: 200, loss: 213.762734\n",
      "iteration 6750 / 8000, samples: 200, loss: 205.246094\n",
      "iteration 6760 / 8000, samples: 200, loss: 324.937844\n",
      "iteration 6770 / 8000, samples: 200, loss: 249.511681\n",
      "iteration 6780 / 8000, samples: 200, loss: 318.562779\n",
      "iteration 6790 / 8000, samples: 200, loss: 233.234063\n",
      "iteration 6800 / 8000, samples: 200, loss: 277.159023\n",
      "iteration 6810 / 8000, samples: 200, loss: 231.957461\n",
      "iteration 6820 / 8000, samples: 200, loss: 230.418328\n",
      "iteration 6830 / 8000, samples: 200, loss: 246.145130\n",
      "iteration 6840 / 8000, samples: 200, loss: 258.428822\n",
      "iteration 6850 / 8000, samples: 200, loss: 212.050889\n",
      "iteration 6860 / 8000, samples: 200, loss: 296.465828\n",
      "iteration 6870 / 8000, samples: 200, loss: 331.972279\n",
      "iteration 6880 / 8000, samples: 200, loss: 333.760226\n",
      "iteration 6890 / 8000, samples: 200, loss: 182.539305\n",
      "iteration 6900 / 8000, samples: 200, loss: 301.334251\n",
      "iteration 6910 / 8000, samples: 200, loss: 271.780610\n",
      "iteration 6920 / 8000, samples: 200, loss: 377.561972\n",
      "iteration 6930 / 8000, samples: 200, loss: 206.700154\n",
      "iteration 6940 / 8000, samples: 200, loss: 225.830882\n",
      "iteration 6950 / 8000, samples: 200, loss: 299.319513\n",
      "iteration 6960 / 8000, samples: 200, loss: 212.101380\n",
      "iteration 6970 / 8000, samples: 200, loss: 287.775228\n",
      "iteration 6980 / 8000, samples: 200, loss: 382.410669\n",
      "iteration 6990 / 8000, samples: 200, loss: 499.033672\n",
      "iteration 7000 / 8000, samples: 200, loss: 266.328976\n",
      "iteration 7010 / 8000, samples: 200, loss: 264.490139\n",
      "iteration 7020 / 8000, samples: 200, loss: 223.232231\n",
      "iteration 7030 / 8000, samples: 200, loss: 314.077721\n",
      "iteration 7040 / 8000, samples: 200, loss: 183.597290\n",
      "iteration 7050 / 8000, samples: 200, loss: 346.477964\n",
      "iteration 7060 / 8000, samples: 200, loss: 348.853151\n",
      "iteration 7070 / 8000, samples: 200, loss: 324.663031\n",
      "iteration 7080 / 8000, samples: 200, loss: 272.588562\n",
      "iteration 7090 / 8000, samples: 200, loss: 156.796765\n",
      "iteration 7100 / 8000, samples: 200, loss: 205.549029\n",
      "iteration 7110 / 8000, samples: 200, loss: 243.904297\n",
      "iteration 7120 / 8000, samples: 200, loss: 218.940854\n",
      "iteration 7130 / 8000, samples: 200, loss: 211.737899\n",
      "iteration 7140 / 8000, samples: 200, loss: 199.716745\n"
     ]
    }
   ],
   "source": [
    "LC = LinearClassifier()\n",
    "\n",
    "print('start training ...')\n",
    "#train(self, X, y, step_size = 1e-3, reg = 1e-5, num_iters = 100, batch_size = 200, verbose = True)\n",
    "#在dataTrain中不重复随机抽取batch_size个样本，迭代训练num_iters次\n",
    "loss_all = LC.train(dataTrain, labelTrain, num_iters = 8000, batch_size = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTest = dataTest - np.mean(dataTest, axis=0)\n",
    "print('last loss is %f' %(loss_all[-1]))\n",
    "#开始预测\n",
    "print('start predicting ...')\n",
    "y_pred = LC.predict(dataTest)\n",
    "\n",
    "hit = 0\n",
    "for i in xrange(y_pred.size):\n",
    "\tif (y_pred[i] == labelTest[i]):\n",
    "\t\thit += 1\n",
    "\n",
    "print('the accuracy rate is %f ' % (hit/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
