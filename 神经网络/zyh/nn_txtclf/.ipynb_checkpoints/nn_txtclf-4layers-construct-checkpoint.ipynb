{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*-coding:utf8-*-\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reg_lambda = 0.001\n",
    "epsilon = 0.00001\n",
    "epsilon_base = 0.00001\n",
    "num_examples = 0\n",
    "num_passes = 7000\n",
    "gamma = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_data():\n",
    "    categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "    # pprint(newsgroups_train.data[0])\n",
    "\n",
    "    num_train = len(newsgroups_train.data)\n",
    "    num_test = len(newsgroups_test.data)\n",
    "    print(num_train, num_test)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=512)\n",
    "\n",
    "    x = vectorizer.fit_transform(newsgroups_train.data + newsgroups_test.data)\n",
    "    x_train = x[0:num_train, :]\n",
    "    x_test = x[num_train:num_train + num_test, :]\n",
    "\n",
    "    y_train = newsgroups_train.target\n",
    "    y_test = newsgroups_test.target\n",
    "\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    print(x_test.shape, y_test.shape)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 1353\n",
      "(2034, 256) (2034,)\n",
      "(1353, 256) (1353,)\n",
      "Loss after iteration 0: 1.384826\n",
      "Loss after iteration 1000: 0.816608\n",
      "Loss after iteration 2000: 0.609392\n",
      "Loss after iteration 3000: 0.427327\n",
      "Loss after iteration 4000: 0.335092\n",
      "Loss after iteration 5000: 0.326990\n",
      "Loss after iteration 6000: 0.329748\n",
      "Accuracy 0.707317 = 957 / 1353\n",
      "Loss after iteration 0: 1.380547\n",
      "Loss after iteration 1000: 0.705351\n",
      "Loss after iteration 2000: 0.496502\n",
      "Loss after iteration 3000: 0.528081\n",
      "Loss after iteration 4000: 0.298731\n",
      "Loss after iteration 5000: 0.273968\n",
      "Loss after iteration 6000: 0.305380\n",
      "Accuracy 0.713230 = 965 / 1353\n",
      "Loss after iteration 0: 1.388320\n",
      "Loss after iteration 1000: 0.545475\n",
      "Loss after iteration 2000: 0.343494\n",
      "Loss after iteration 3000: 0.271660\n",
      "Loss after iteration 4000: 0.256242\n",
      "Loss after iteration 5000: 0.234811\n",
      "Loss after iteration 6000: 0.227245\n",
      "Accuracy 0.705839 = 955 / 1353\n",
      "Loss after iteration 0: 1.388918\n",
      "Loss after iteration 1000: 0.458917\n",
      "Loss after iteration 2000: 0.326404\n",
      "Loss after iteration 3000: 0.270583\n",
      "Loss after iteration 4000: 0.240502\n",
      "Loss after iteration 5000: 0.242550\n",
      "Loss after iteration 6000: 0.127233\n",
      "Accuracy 0.722838 = 978 / 1353\n",
      "Loss after iteration 0: 1.387608\n",
      "Loss after iteration 1000: 0.393181\n",
      "Loss after iteration 2000: 0.204850\n",
      "Loss after iteration 3000: 0.154921\n",
      "Loss after iteration 4000: 0.218174\n",
      "Loss after iteration 5000: 0.118629\n",
      "Loss after iteration 6000: 0.105607\n",
      "Accuracy 0.730968 = 989 / 1353\n",
      "Loss after iteration 0: 1.385892\n",
      "Loss after iteration 1000: 0.355396\n",
      "Loss after iteration 2000: 0.220711\n",
      "Loss after iteration 3000: 0.141311\n",
      "Loss after iteration 4000: 0.127715\n",
      "Loss after iteration 5000: 0.116001\n",
      "Loss after iteration 6000: 0.105245\n",
      "Accuracy 0.716925 = 970 / 1353\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_result(loss_arr, acc_arr, layer_arr, name):\n",
    "    loss_result_reshape = np.array(loss_arr).T\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(len(layer_arr)):\n",
    "        ax.plot(loss_result_reshape[:, i], label='number of %i layer' % i)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)  # 增加格点\n",
    "    plt.axis('tight')  # 坐标轴适应数据量 axis 设置坐标轴\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('The loss of time')\n",
    "    plt.savefig('The loss %s' % name)\n",
    "\n",
    "    ax = plt.subplot(211)\n",
    "    colors = ['r', 'y', 'g', 'b', 'r', 'y', 'g', 'b', 'orange']\n",
    "    for i in range(len(layer_arr)):\n",
    "        #     ax.plot(label='number of layer %i' %input_dim[i])\n",
    "        plt.scatter(i, acc_arr[i], c=colors[i % 4], cmap='brg', s=40, marker='x')\n",
    "        #     plt.legend('number of layer %i' %input_dim[i])\n",
    "\n",
    "    plt.grid(True)  # 增加格点\n",
    "    plt.axis('tight')  # 坐标轴适应数据量 axis 设置坐标轴\n",
    "    plt.xlabel('number of run')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('The accuracy of time')\n",
    "    plt.savefig('The accuracy %s' % name)\n",
    "\n",
    "\n",
    "def update_learn_rate(rate, method=\"fixed\", iters=0):\n",
    "    if method == \"step\":\n",
    "        rate *= np.power(gamma, (iters/num_passes))\n",
    "    elif method == \"exp\":\n",
    "        rate *= np.power(gamma, iters)\n",
    "    elif method == \"inv\":\n",
    "        pass\n",
    "        # rate *= np.power((1 + gamma * iters), (-power))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "class init_m(object):\n",
    "    def __init__(self, m_x):\n",
    "        self.a = m_x\n",
    "\n",
    "\n",
    "class end_m(object):\n",
    "    def __init__(self, e_x):\n",
    "        self.a = e_x\n",
    "\n",
    "\n",
    "class softmax(object):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.w = np.random.randn(in_dim, out_dim) / np.sqrt(in_dim)\n",
    "        self.b = np.zeros((1, out_dim))\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.delta = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "    def forward(self, f_x):\n",
    "        self.z = f_x.dot(self.w) + self.b\n",
    "        exp_scores = np.exp(self.z)\n",
    "        self.a = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return self.a\n",
    "\n",
    "    def backprop(self, font_layer, back_layer):\n",
    "        self.delta = self.a\n",
    "        self.delta[range(num_examples), back_layer.a] -= 1\n",
    "        self.dw = font_layer.a.T.dot(self.delta)\n",
    "        self.db = np.sum(self.delta, axis=0, keepdims=True)\n",
    "\n",
    "        self.dw += reg_lambda * self.w\n",
    "        self.w += -epsilon * self.dw\n",
    "        self.b += -epsilon * self.db\n",
    "\n",
    "\n",
    "class relu(object):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.w = np.random.randn(in_dim, out_dim) / np.sqrt(in_dim)\n",
    "        self.b = np.zeros((1, out_dim))\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.delta = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.grad = None\n",
    "\n",
    "    def forward(self, f_x):\n",
    "        self.z = f_x.dot(self.w) + self.b\n",
    "        self.a = np.where(self.z < 0, 0, self.z)\n",
    "        self.grad = np.where(self.a < 0, 0, 1)\n",
    "        return self.a\n",
    "\n",
    "    def backprop(self, font_layer, back_layer):\n",
    "        self.delta = back_layer.delta.dot(back_layer.w.T) * self.grad\n",
    "        self.dw = font_layer.a.T.dot(self.delta)\n",
    "        self.db = np.sum(self.delta, axis=0, keepdims=True)\n",
    "\n",
    "        self.dw += reg_lambda * self.w\n",
    "        self.w += -epsilon * self.dw\n",
    "        self.b += -epsilon * self.db\n",
    "\n",
    "\n",
    "class l_tanh(object):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.w = np.random.randn(in_dim, out_dim) / np.sqrt(in_dim)\n",
    "        self.b = np.zeros((1, out_dim))\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.delta = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.grad = None\n",
    "\n",
    "    def forward(self, f_x):\n",
    "        self.z = f_x.dot(self.w) + self.b\n",
    "        self.a = np.tanh(self.z)\n",
    "        self.grad = 1 - np.power(self.a, 2)\n",
    "        return self.a\n",
    "\n",
    "    def backprop(self, font_layer, back_layer):\n",
    "        self.delta = back_layer.delta.dot(back_layer.w.T) * self.grad\n",
    "        self.dw = font_layer.a.T.dot(self.delta)\n",
    "        self.db = np.sum(self.delta, axis=0, keepdims=True)\n",
    "\n",
    "        self.dw += reg_lambda * self.w\n",
    "        self.w += -epsilon * self.dw\n",
    "        self.b += -epsilon * self.db\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = [relu(20, 4),\n",
    "                       relu(4, 4),\n",
    "                       relu(4, 4),\n",
    "                       relu(4, 4),\n",
    "                       softmax(4, 20)]\n",
    "        self._loss = 1000\n",
    "        self.loss_result = []\n",
    "        self.accuracy = 0.0\n",
    "        self.learn_rate_method = \"fixed\"\n",
    "\n",
    "    def forward(self, f_x):\n",
    "        tmp = f_x\n",
    "        for layer in self.layers:\n",
    "            tmp = layer.forward(tmp)\n",
    "        return tmp\n",
    "\n",
    "    def loss(self, l_x, y):\n",
    "        l_x = self.forward(l_x)\n",
    "\n",
    "        # Calculating the loss\n",
    "        corect_logprobs = -np.log(l_x[range(num_examples), y])\n",
    "        data_loss = np.sum(corect_logprobs)\n",
    "        # Add regulatization term to loss (optional)\n",
    "        w_sum = 0\n",
    "        for layer in self.layers:\n",
    "            w_sum += np.sum(np.square(layer.w))\n",
    "\n",
    "        data_loss += reg_lambda / 2 * w_sum\n",
    "        self._loss = 1. / num_examples * data_loss\n",
    "\n",
    "    def train(self, t_x, y):\n",
    "\n",
    "        self.loss_result = []\n",
    "        for passes in range(0, num_passes):\n",
    "            self.loss(t_x, y)\n",
    "            if passes % 10 == 0:\n",
    "                self.loss_result.append(self._loss)\n",
    "                if passes % 1000 == 0:\n",
    "                    print(\"Loss after iteration %i: %f\" % (passes, self._loss))\n",
    "\n",
    "            for i in range(len(self.layers)):\n",
    "                if i == 0:\n",
    "                    self.layers[-i - 1].backprop(self.layers[-i - 2], end_m(y))\n",
    "                elif i == len(self.layers) - 1:\n",
    "                    self.layers[-i - 1].backprop(init_m(t_x), self.layers[-i])\n",
    "                else:\n",
    "                    self.layers[-i - 1].backprop(self.layers[-i - 2], self.layers[-i])\n",
    "            update_learn_rate(epsilon, self.learn_rate_method, passes)\n",
    "\n",
    "    def predict(self, _x, _y):\n",
    "        n_correct = 0\n",
    "        n_test = _x.shape[0]\n",
    "        for n in range(n_test):\n",
    "            xp = _x[n, :]\n",
    "            yp = np.argmax(self.forward(xp), axis=1)\n",
    "            if yp == _y[n]:\n",
    "                n_correct += 1.0\n",
    "\n",
    "        self.accuracy = n_correct / n_test\n",
    "        print('Accuracy %f = %d / %d' % (self.accuracy, int(n_correct), n_test))\n",
    "\n",
    "\n",
    "def save_result():\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train, Y_train, X_test, Y_test = fetch_data()\n",
    "    num_examples, input_dim = X_train.shape\n",
    "    Loss = []\n",
    "    Acc = []\n",
    "    Layer = []\n",
    "    np.random.seed(0)\n",
    "    layers = [[l_tanh(512, 4), l_tanh(4, 4), l_tanh(4, 4), l_tanh(4, 4), l_tanh(4, 4), softmax(4, 4)],\n",
    "              [l_tanh(512, 8), l_tanh(8, 8), l_tanh(8, 8), l_tanh(8, 8), l_tanh(8, 8), softmax(8, 4)],\n",
    "              [l_tanh(512, 16), l_tanh(16, 16), l_tanh(16, 16), l_tanh(16, 16), l_tanh(16, 16), softmax(16, 4)],\n",
    "              [l_tanh(512, 32), l_tanh(32, 32), l_tanh(32, 32), l_tanh(32, 32), l_tanh(32, 32), softmax(32, 4)],\n",
    "              [l_tanh(512, 64), l_tanh(64, 64), l_tanh(64, 64), l_tanh(64, 64), l_tanh(64, 64), softmax(64, 4)],\n",
    "              [l_tanh(512, 128), l_tanh(128, 128), l_tanh(128, 128), l_tanh(128, 128), l_tanh(128, 128), softmax(128, 4)],\n",
    "               [l_tanh(512, 256), l_tanh(256, 256), l_tanh(256, 256), l_tanh(256, 256), l_tanh(256, 256), softmax(256, 4)],\n",
    "               [l_tanh(512, 512), l_tanh(512, 512), l_tanh(512, 512), l_tanh(512, 512), l_tanh(512, 512), softmax(512, 4)],\n",
    "#               [l_tanh(256, 1024), l_tanh(1024, 1024), l_tanh(1024, 1024), l_tanh(1024, 1024), l_tanh(1024, 1024), softmax(1024, 4)],\n",
    "# #               [l_tanh(20, 8), l_tanh(8, 16), l_tanh(16, 32), l_tanh(32, 64), l_tanh(64, 128), softmax(128, 4)],\n",
    "#               [l_tanh(20, 128), l_tanh(128, 64), l_tanh(64, 32), l_tanh(32, 16), l_tanh(16, 4), softmax(4, 4)],\n",
    "#               [l_tanh(20, 8), l_tanh(8, 4), l_tanh(4, 8), l_tanh(8, 4), l_tanh(4, 8), softmax(8, 4)],\n",
    "#               [l_tanh(20, 16), l_tanh(16, 4), l_tanh(4, 16), l_tanh(16, 4), l_tanh(4, 16), softmax(16, 4)],\n",
    "#               [l_tanh(20, 32), l_tanh(32, 16), l_tanh(16, 32), l_tanh(32, 16), l_tanh(16, 32), softmax(32, 4)],\n",
    "#               [l_tanh(20, 64), l_tanh(64, 32), l_tanh(32, 4), l_tanh(4, 16), l_tanh(16, 4), softmax(4, 4)],\n",
    "#               [l_tanh(20, 16), l_tanh(16, 128), l_tanh(128, 4), l_tanh(4, 32), l_tanh(32, 4), softmax(4, 4)],\n",
    "#               [l_tanh(20, 64), l_tanh(64, 4), l_tanh(4, 128), l_tanh(128, 4), l_tanh(4, 32), softmax(32, 4)],\n",
    "#               [l_tanh(20, 64), l_tanh(64, 32), l_tanh(32, 128), l_tanh(128, 4), l_tanh(4, 16), softmax(16, 4)]\n",
    "              ]\n",
    "    \n",
    "    # 调整学习速率的起始大小\n",
    "    epsilon_base = 0.00001\n",
    "    \n",
    "    Model = MLP()\n",
    "\n",
    "    # 每次运行之前修改一下要保存的图片名称\n",
    "    pic_name = \"Tanh and feat 512 fixed 5 layers.png\"\n",
    "    for la in layers:\n",
    "        epsilon = epsilon_base\n",
    "        Model.layers = la\n",
    "        \n",
    "        # 修改学习速率的改变方式 \"fixed\" “step”  “exp\"\n",
    "        Model.learn_rate_method = \"step\"\n",
    "\n",
    "        Model.train(X_train, Y_train)\n",
    "        Loss.append(Model.loss_result)\n",
    "        Layer.append(len(la) - 1)\n",
    "\n",
    "        Model.predict(X_test, Y_test)\n",
    "        Acc.append(Model.accuracy)\n",
    "\n",
    "    plot_result(Loss, Acc, Layer, pic_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
